{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 2.03MB/s]                    \n",
      "2021-10-04 20:19:00 INFO: Downloading default packages for language: en (English)...\n",
      "2021-10-04 20:19:02 INFO: File exists: /Users/bl4z3/stanza_resources/en/default.zip.\n",
      "2021-10-04 20:19:11 INFO: Finished downloading models and saved to /Users/bl4z3/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import stanza # Stanford's stanza package\n",
    "stanza.download('en') # run this once\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Lexical Complexity\n",
    "\n",
    "Code taken from:\n",
    "\n",
    "This code is the lexical complexity analyzer described in\n",
    "\n",
    "Lu, Xiaofei (2012). The relationship of lexical richnes to the quality \n",
    "of ESL speakers' oral narratives. The Modern Language Journal, 96(2), 190-208. \n",
    "\n",
    "Version 1.1 Released on February 12, 2013\n",
    "\n",
    "Which can be found at:\n",
    "\n",
    "http://www.personal.psu.edu/xxl13/download.html\n",
    "\n",
    "It has been modified to work with search queries, as it was initially designed for sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string,re,sys,os,random\n",
    "from math import sqrt,log\n",
    "\n",
    "# adjust minimum sample size here\n",
    "standard=50\n",
    "\n",
    "# Returns the keys of dictionary d sorted by their values\n",
    "def sort_by_value(d):\n",
    "    items=d.items()\n",
    "    backitems=[ [v[1],v[0]] for v in items]\n",
    "    backitems.sort()\n",
    "    return [ backitems[i][1] for i in range(0,len(backitems))]\n",
    "\n",
    "# NDW for first z words in a sample\n",
    "def getndwfirstz(z,lemmalist):\n",
    "    ndwfirstztype={}\n",
    "    for lemma in lemmalist[:z]:\n",
    "        ndwfirstztype[lemma]=1\n",
    "    return len(ndwfirstztype.keys())\n",
    "\n",
    "# NDW expected random z words, 10 trials\n",
    "def getndwerz(z,lemmalist):\n",
    "    ndwerz=0\n",
    "    for i in range(10):\n",
    "        ndwerztype={}\n",
    "        erzlemmalist=random.sample(lemmalist,z)\n",
    "        for lemma in erzlemmalist:\n",
    "            ndwerztype[lemma]=1\n",
    "        ndwerz+=len(ndwerztype.keys())\n",
    "    return ndwerz/10.0\n",
    "\n",
    "# NDW expected random sequences of z words, 10 trials\n",
    "def getndwesz(z,lemmalist):\n",
    "    ndwesz=0\n",
    "    for i in range(10):\n",
    "        ndwesztype={}\n",
    "        startword=random.randint(0,len(lemmalist)-z)\n",
    "        eszlemmalist=lemmalist[startword:startword+z]\n",
    "        for lemma in eszlemmalist:\n",
    "            ndwesztype[lemma]=1\n",
    "        ndwesz+=len(ndwesztype.keys())\n",
    "    return ndwesz/10.0\n",
    "\n",
    "# MSTTR\n",
    "def getmsttr(z,lemmalist):\n",
    "    samples=0\n",
    "    msttr=0.0\n",
    "    while len(lemmalist)>=z:\n",
    "        samples+=1\n",
    "        msttrtype={}\n",
    "        for lemma in lemmalist[:z]:\n",
    "            msttrtype[lemma]=1\n",
    "        msttr+=len(msttrtype.keys())/float(z)\n",
    "        lemmalist=lemmalist[z:]    \n",
    "    return msttr/samples\n",
    "\n",
    "def isLetterNumber(character):\n",
    "    if character in string.printable and not character in string.punctuation:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def isSentence(line):\n",
    "    for character in line:\n",
    "        if isLetterNumber(character):\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLex(queries):\n",
    "    processor_dict = {\n",
    "    'tokenize': 'gsd', \n",
    "    'pos': 'bnc', \n",
    "    'lemma': 'default'\n",
    "    }\n",
    "\n",
    "    nlp = stanza.Pipeline('en', processors=processor_dict)\n",
    "    # reads information from bnc wordlist\n",
    "    lexFeat = []\n",
    "    adjdict={}\n",
    "    verbdict={}\n",
    "    noundict={}\n",
    "    worddict={}\n",
    "    wordlistfile=open(\"DataSets/bnc_all_filtered.txt\",\"r\")\n",
    "    wordlist=wordlistfile.readlines()\n",
    "    wordlistfile.close()\n",
    "    for word in wordlist:\n",
    "        wordinfo=word.strip()\n",
    "        if not wordinfo or \"Total words\" in wordinfo:\n",
    "            continue\n",
    "        infolist=wordinfo.split()\n",
    "        lemma=infolist[0]\n",
    "        pos=infolist[1]\n",
    "        frequency=int(infolist[2])\n",
    "        worddict[lemma]=worddict.get(lemma,0)+frequency\n",
    "        if pos==\"Adj\":\n",
    "            adjdict[lemma]=adjdict.get(lemma,0)+frequency\n",
    "        elif pos==\"Verb\":\n",
    "            verbdict[lemma]=verbdict.get(lemma,0)+frequency\n",
    "        elif pos==\"NoC\" or pos==\"NoP\":\n",
    "            noundict[lemma]=noundict.get(lemma,0)+frequency\n",
    "    wordranks=sort_by_value(worddict)\n",
    "    verbranks=sort_by_value(verbdict)\n",
    "    length = len(queries)\n",
    "    with tqdm(total = length) as pbar:\n",
    "        for query in queries:\n",
    "            filename=query\n",
    "            doc = nlp(query)\n",
    "            for sentence in doc.sentences:\n",
    "                s = ''\n",
    "                for word in sentence.words:\n",
    "                    s+='{}_{}'.format(word.lemma, word.xpos) + ' '\n",
    "            lemlines= s\n",
    "            #print(lemlines)\n",
    "            # process input file\n",
    "            wordtypes={}\n",
    "            wordtokens=0\n",
    "            swordtypes={}\n",
    "            swordtokens=0\n",
    "            lextypes={}\n",
    "            lextokens=0\n",
    "            slextypes={}\n",
    "            slextokens=0\n",
    "            verbtypes={}\n",
    "            verbtokens=0\n",
    "            sverbtypes={}\n",
    "            adjtypes={}\n",
    "            adjtokens=0\n",
    "            advtypes={}\n",
    "            advtokens=0\n",
    "            nountypes={}\n",
    "            nountokens=0\n",
    "            lemmaposlist=[]\n",
    "            lemmalist=[]\n",
    "\n",
    "            for lemline in lemlines:\n",
    "                lemline=lemline.strip()\n",
    "                lemline=lemline.lower()\n",
    "                if not isSentence(lemline):\n",
    "                    continue\n",
    "                lemmas=lemline.split()\n",
    "                for lemma in lemmas:\n",
    "                    word=lemma.split(\"_\")[0]\n",
    "                    pos=lemma.split(\"_\")[-1]\n",
    "                    if (not pos in string.punctuation) and pos!=\"sent\" and pos!=\"sym\":\n",
    "                        lemmaposlist.append(lemma)\n",
    "                        lemmalist.append(word)  \n",
    "                        wordtokens+=1\n",
    "                        wordtypes[word]=1\n",
    "                        try:\n",
    "\n",
    "                            if (not word in wordranks[-2000:]) and pos != \"cd\":\n",
    "                                swordtypes[word]=1\n",
    "                                swordtokens+=1\n",
    "                            if pos[0]==\"n\":\n",
    "                                lextypes[word]=1\n",
    "                                nountypes[word]=1\n",
    "                                lextokens+=1\n",
    "                                nountokens+=1\n",
    "                                if not word in wordranks[-2000:]:\n",
    "                                    slextypes[word]=1\n",
    "                                    slextokens+=1\n",
    "                            elif pos[0]==\"j\":\n",
    "                                lextypes[word]=1\n",
    "                                adjtypes[word]=1\n",
    "                                lextokens+=1\n",
    "                                adjtokens+=1\n",
    "                                if not word in wordranks[-2000:]:\n",
    "                                    slextypes[word]=1\n",
    "                                    slextokens+=1\n",
    "                            elif pos[0]==\"r\" and (adjdict.has_key(word) or (word[-2:]==\"ly\" and adjdict.has_key(word[:-2]))):\n",
    "                                lextypes[word]=1\n",
    "                                advtypes[word]=1\n",
    "                                lextokens+=1\n",
    "                                advtokens+=1\n",
    "                                if not word in wordranks[-2000:]:\n",
    "                                    slextypes[word]=1\n",
    "                                    slextokens+=1\n",
    "                            elif pos[0]==\"v\" and not word in [\"be\",\"have\"]:\n",
    "                                verbtypes[word]=1\n",
    "                                verbtokens+=1\n",
    "                                lextypes[word]=1\n",
    "                                lextokens+=1\n",
    "                                if not word in wordranks[-2000:]:\n",
    "                                    sverbtypes[word]=1\n",
    "                                    slextypes[word]=1\n",
    "                                    slextokens+=1\n",
    "                        except(AttributeError):\n",
    "                            pass\n",
    "\n",
    "            # 1. lexical density\n",
    "            if wordtokens > 0:\n",
    "                ld=float(lextokens)/wordtokens\n",
    "            else:\n",
    "                ld=0\n",
    "            # 2. lexical sophistication\n",
    "            # 2.1 lexical sophistication\n",
    "            if lextokens != 0:\n",
    "                ls1=slextokens/float(lextokens)\n",
    "            else:\n",
    "                ls1 = 0\n",
    "            if len(wordtypes.keys()) > 0:\n",
    "                ls2=len(swordtypes.keys())/float(len(wordtypes.keys()))\n",
    "            else:\n",
    "                ls2 = 0\n",
    "\n",
    "            # 2.2 verb sophistication\n",
    "            vs1 = 0\n",
    "            vs2=0\n",
    "            cvs1=0\n",
    "            if verbtokens > 0:\n",
    "                vs1=len(sverbtypes.keys())/float(verbtokens)\n",
    "                vs2=(len(sverbtypes.keys())*len(sverbtypes.keys()))/float(verbtokens)\n",
    "                cvs1=len(sverbtypes.keys())/sqrt(2*verbtokens)\n",
    "\n",
    "            # 3 lexical diversity or variation\n",
    "\n",
    "            # 3.1 NDW, may adjust the values of \"standard\"\n",
    "            ndw=len(wordtypes.keys())\n",
    "\n",
    "            # 3.2 TTR\n",
    "            \n",
    "            if wordtokens > 0:\n",
    "                ttr=len(wordtypes.keys())/float(wordtokens)\n",
    "                if len(lemmalist)>=standard:\n",
    "                    msttr=getmsttr(standard,lemmalist)\n",
    "                cttr=len(wordtypes.keys())/sqrt(2*wordtokens)\n",
    "                rttr=len(wordtypes.keys())/sqrt(wordtokens)\n",
    "            else:\n",
    "                ttr = 0\n",
    "                cttr = 0\n",
    "                rttr = 0\n",
    "            if wordtokens == 0 or len(wordtypes.keys()) == 0:\n",
    "                logttr = 0\n",
    "            else:\n",
    "                logttr=log(len(wordtypes.keys()))/log(wordtokens)\n",
    "            # 3.3 verb diversity\n",
    "            vv1, svv1, cvv1 = 0, 0, 0\n",
    "            if verbtokens > 0:\n",
    "                vv1=len(verbtypes.keys())/float(verbtokens)\n",
    "                svv1=len(verbtypes.keys())*len(verbtypes.keys())/float(verbtokens)\n",
    "                cvv1=len(verbtypes.keys())/sqrt(2*verbtokens)\n",
    "\n",
    "            # 3.4 lexical diversity\n",
    "            if lextokens != 0:\n",
    "                lv=len(lextypes.keys())/float(lextokens)\n",
    "                vv2=len(verbtypes.keys())/float(lextokens)\n",
    "                adjv=len(adjtypes.keys())/float(lextokens)\n",
    "\n",
    "            else:\n",
    "                lv=0\n",
    "                vv2=0\n",
    "                adjv=0\n",
    "\n",
    "            if nountokens != 0:\n",
    "                nv=len(nountypes.keys())/float(nountokens)\n",
    "            else:\n",
    "                nv=0\n",
    "\n",
    "\n",
    "\n",
    "          \n",
    "            lexFeat.append([query, ld, ls1, ls2, vs1, vs2, cvs1, ndw, ttr,\n",
    "                           cttr, rttr, logttr, lv, vv1, svv1, cvv1, vv2, nv, adjv])\n",
    "            pbar.update()\n",
    "    lexical = pd.DataFrame(data = lexFeat, columns = [\"query\", \"ld\", \"ls1\", \"ls2\", \"vs1\", \"vs2\", \"cvs1\", \"ndw\", \"ttr\",\n",
    "                                                      \"cttr\", \"rttr\", \"logttr\", \"lv\", \"vv1\", \"svv1\", \"cvv1\", \"vv2\", \"nv\", \"adjv\"])\n",
    "    return lexical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230362\n",
      "231867\n",
      "70485\n"
     ]
    }
   ],
   "source": [
    "#allSessions = pickle.load( open( \"../../thesis/Data/Session/allSessionsProc.p\", \"rb\" ) )\n",
    "allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allSessionsSQS = list(pickle.load( open( \"../Data/DataSets/SQS/SQS.p\", \"rb\" ) ))\n",
    "allQueries = allSessions['query'].tolist()\n",
    "print(len(allQueries))\n",
    "allQueries = allQueries + list(allSessionsSQS)\n",
    "print(len(allQueries))\n",
    "setQueries = set(allQueries)\n",
    "print(len(setQueries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70196"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(allSessions['query'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/70485 [00:00<?, ?it/s]/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '9'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '1'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '2'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '0'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '7'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '3'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '5'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '6'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '8'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '4'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "  1%|          | 571/70485 [00:00<00:12, 5708.51it/s]/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ö'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "  1%|▏         | 883/70485 [00:00<00:15, 4569.76it/s]/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ê'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ç'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'é'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      " 16%|█▌        | 11164/70485 [00:21<03:04, 321.68it/s]/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'á'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      " 23%|██▎       | 16121/70485 [00:42<04:37, 195.60it/s]/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ú'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      " 34%|███▎      | 23773/70485 [01:34<06:07, 127.13it/s]/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ñ'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      " 36%|███▌      | 25461/70485 [01:47<06:05, 123.12it/s]/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ó'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      " 69%|██████▉   | 48602/70485 [06:59<11:40, 31.26it/s] /Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'è'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      " 84%|████████▍ | 59538/70485 [11:03<02:01, 89.77it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-22fee675c6a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mleast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSSP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mrunning\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0msyllable_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyllable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_syllables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyllable_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py\u001b[0m in \u001b[0;36mvalidate_syllables\u001b[0;34m(self, syllable_list)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mvalid_syllables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyllable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvowels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyllable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_syllables\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mfront\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msyllable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/re.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \"\"\"Scan through string looking for a match to the pattern, returning\n\u001b[1;32m    184\u001b[0m     a Match object, or None if no match was found.\"\"\"\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/re.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/sre_compile.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/sre_parse.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(str, flags, pattern)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mSRE_FLAG_VERBOSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mVerbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;31m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         itemsappend(_parse(source, state, verbose, nested + 1,\n\u001b[0;32m--> 420\u001b[0;31m                            not nested and not items))\n\u001b[0m\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;31m# precompute constants into local variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0msubpatternappend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m     \u001b[0msourceget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m     \u001b[0msourcematch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0m_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Lexical Characteristics\n",
    "\n",
    "totalSyl = []\n",
    "avgSyl = []\n",
    "simWords = []\n",
    "comWords = []\n",
    "simWordsAvg = []\n",
    "comWordsAvg = []\n",
    "mostSyl = []\n",
    "leastSyl = []\n",
    "SSP = SyllableTokenizer()\n",
    "\n",
    "with tqdm(total = len(setQueries) ) as pbar:\n",
    "    for text in setQueries:\n",
    "        running = 0\n",
    "        count = 0\n",
    "        simpleWords = 0\n",
    "        complexWords = 0\n",
    "        most = 0\n",
    "        least = 19\n",
    "        for word in text.split(\" \"):\n",
    "            current = len(SSP.tokenize(word))\n",
    "            running += current\n",
    "            count +=1\n",
    "            if current < 3:\n",
    "                simpleWords += 1\n",
    "            else:\n",
    "                complexWords +=1\n",
    "            if most < current:\n",
    "                most = current\n",
    "            if least > current:\n",
    "                least = current\n",
    "                \n",
    "        totalSyl.append(running)\n",
    "        avgSyl.append(running/count)\n",
    "        simWords.append(simpleWords)\n",
    "        comWords.append(complexWords)\n",
    "        mostSyl.append(most)\n",
    "        leastSyl.append(least)\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textComplex = pd.DataFrame(setQueries)\n",
    "textComplex = textComplex.set_index(0, drop=True)\n",
    "textComplex = textComplex.reset_index().rename(columns={0:'query'})\n",
    "textComplex['totalSyl'] = totalSyl\n",
    "textComplex['avgSyl'] = avgSyl\n",
    "textComplex['simWords'] = simWords\n",
    "textComplex['comWords'] = comWords\n",
    "textComplex['greatestSyl'] = mostSyl\n",
    "textComplex['leastSyl'] = leastSyl\n",
    "textComplex['numChars'] = textComplex['query'].str.len()\n",
    "textComplex['numWords'] = textComplex['query'].str.split().str.len()\n",
    "textComplex['avgLenWord'] = textComplex['numChars']/textComplex['numWords']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lexical Complexity\n",
    "\n",
    "lexFeats = getLex(setQueries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicalFeatures = textComplex.merge(lexFeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lexicalFeatures, open( \"Pickles/LexFeat.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
