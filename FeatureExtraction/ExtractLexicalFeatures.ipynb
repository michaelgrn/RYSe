{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook extracts the lexical features from each query, which will then be joined to sessions on queries in feature extraction main. Extracting features relating to lexical complexity can be time consuming, taking approximately 4 hours on a 2.2 i7 processor with 16 gig RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 10.2MB/s]                    \n",
      "2021-10-12 17:31:33 INFO: Downloading default packages for language: en (English)...\n",
      "2021-10-12 17:31:34 INFO: File exists: /Users/bl4z3/stanza_resources/en/default.zip.\n",
      "2021-10-12 17:31:40 INFO: Finished downloading models and saved to /Users/bl4z3/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import stanza  \n",
    "import string\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from math import sqrt\n",
    "from math import log\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "stanza.download('en') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Lexical Complexity\n",
    "\n",
    "Code taken from:\n",
    "\n",
    "This code is the lexical complexity analyzer described in\n",
    "\n",
    "Lu, Xiaofei (2012). The relationship of lexical richnes to the quality \n",
    "of ESL speakers' oral narratives. The Modern Language Journal, 96(2), 190-208. \n",
    "\n",
    "Version 1.1 Released on February 12, 2013\n",
    "\n",
    "Which can be found at:\n",
    "\n",
    "http://www.personal.psu.edu/xxl13/download.html\n",
    "\n",
    "It has been modified to work with search queries, as it was initially designed for sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust minimum sample size here\n",
    "standard=50\n",
    "\n",
    "# Returns the keys of dictionary d sorted by their values\n",
    "def sort_by_value(d):\n",
    "    items=d.items()\n",
    "    backitems=[ [v[1],v[0]] for v in items]\n",
    "    backitems.sort()\n",
    "    return [ backitems[i][1] for i in range(0,len(backitems))]\n",
    "\n",
    "# NDW for first z words in a sample\n",
    "def getndwfirstz(z,lemmalist):\n",
    "    ndwfirstztype={}\n",
    "    for lemma in lemmalist[:z]:\n",
    "        ndwfirstztype[lemma]=1\n",
    "    return len(ndwfirstztype.keys())\n",
    "\n",
    "# NDW expected random z words, 10 trials\n",
    "def getndwerz(z,lemmalist):\n",
    "    ndwerz=0\n",
    "    for i in range(10):\n",
    "        ndwerztype={}\n",
    "        erzlemmalist=random.sample(lemmalist,z)\n",
    "        for lemma in erzlemmalist:\n",
    "            ndwerztype[lemma]=1\n",
    "        ndwerz+=len(ndwerztype.keys())\n",
    "    return ndwerz/10.0\n",
    "\n",
    "# NDW expected random sequences of z words, 10 trials\n",
    "def getndwesz(z,lemmalist):\n",
    "    ndwesz=0\n",
    "    for i in range(10):\n",
    "        ndwesztype={}\n",
    "        startword=random.randint(0,len(lemmalist)-z)\n",
    "        eszlemmalist=lemmalist[startword:startword+z]\n",
    "        for lemma in eszlemmalist:\n",
    "            ndwesztype[lemma]=1\n",
    "        ndwesz+=len(ndwesztype.keys())\n",
    "    return ndwesz/10.0\n",
    "\n",
    "# MSTTR\n",
    "def getmsttr(z,lemmalist):\n",
    "    samples=0\n",
    "    msttr=0.0\n",
    "    while len(lemmalist)>=z:\n",
    "        samples+=1\n",
    "        msttrtype={}\n",
    "        for lemma in lemmalist[:z]:\n",
    "            msttrtype[lemma]=1\n",
    "        msttr+=len(msttrtype.keys())/float(z)\n",
    "        lemmalist=lemmalist[z:]    \n",
    "    return msttr/samples\n",
    "\n",
    "def isLetterNumber(character):\n",
    "    if character in string.printable and not character in string.punctuation:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def isSentence(line):\n",
    "    for character in line:\n",
    "        if isLetterNumber(character):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# Accepts a list of queries, returns a dataframe of extracted lexical features \n",
    "# that correspond to each query\n",
    "def getLex(queries):\n",
    "    processor_dict = {\n",
    "    'tokenize': 'gsd', \n",
    "    'pos': 'bnc', \n",
    "    'lemma': 'default'\n",
    "    }\n",
    "\n",
    "    nlp = stanza.Pipeline('en', processors=processor_dict)\n",
    "    # reads information from bnc wordlist\n",
    "    lexFeat = []\n",
    "    adjdict={}\n",
    "    verbdict={}\n",
    "    noundict={}\n",
    "    worddict={}\n",
    "    wordlistfile=open(\"DataSets/BNC/bnc_all_filtered.txt\",\"r\")\n",
    "    wordlist=wordlistfile.readlines()\n",
    "    wordlistfile.close()\n",
    "    for word in wordlist:\n",
    "        wordinfo=word.strip()\n",
    "        if not wordinfo or \"Total words\" in wordinfo:\n",
    "            continue\n",
    "        infolist=wordinfo.split()\n",
    "        lemma=infolist[0]\n",
    "        pos=infolist[1]\n",
    "        frequency=int(infolist[2])\n",
    "        worddict[lemma]=worddict.get(lemma,0)+frequency\n",
    "        if pos==\"Adj\":\n",
    "            adjdict[lemma]=adjdict.get(lemma,0)+frequency\n",
    "        elif pos==\"Verb\":\n",
    "            verbdict[lemma]=verbdict.get(lemma,0)+frequency\n",
    "        elif pos==\"NoC\" or pos==\"NoP\":\n",
    "            noundict[lemma]=noundict.get(lemma,0)+frequency\n",
    "    wordranks=sort_by_value(worddict)\n",
    "    verbranks=sort_by_value(verbdict)\n",
    "    length = len(queries)\n",
    "    with tqdm(total = length) as pbar:\n",
    "        for query in queries:\n",
    "            filename=query\n",
    "            doc = nlp(query)\n",
    "            for sentence in doc.sentences:\n",
    "                s = ''\n",
    "                for word in sentence.words:\n",
    "                    s+='{}_{}'.format(word.lemma, word.xpos) + ' '\n",
    "            lemlines= s\n",
    "            #print(lemlines)\n",
    "            # process input file\n",
    "            wordtypes={}\n",
    "            wordtokens=0\n",
    "            swordtypes={}\n",
    "            swordtokens=0\n",
    "            lextypes={}\n",
    "            lextokens=0\n",
    "            slextypes={}\n",
    "            slextokens=0\n",
    "            verbtypes={}\n",
    "            verbtokens=0\n",
    "            sverbtypes={}\n",
    "            adjtypes={}\n",
    "            adjtokens=0\n",
    "            advtypes={}\n",
    "            advtokens=0\n",
    "            nountypes={}\n",
    "            nountokens=0\n",
    "            lemmaposlist=[]\n",
    "            lemmalist=[]\n",
    "\n",
    "            for lemline in lemlines:\n",
    "                lemline=lemline.strip()\n",
    "                lemline=lemline.lower()\n",
    "                if not isSentence(lemline):\n",
    "                    continue\n",
    "                lemmas=lemline.split()\n",
    "                for lemma in lemmas:\n",
    "                    word=lemma.split(\"_\")[0]\n",
    "                    pos=lemma.split(\"_\")[-1]\n",
    "                    if (not pos in string.punctuation) and pos!=\"sent\" and pos!=\"sym\":\n",
    "                        lemmaposlist.append(lemma)\n",
    "                        lemmalist.append(word)  \n",
    "                        wordtokens+=1\n",
    "                        wordtypes[word]=1\n",
    "                        try:\n",
    "\n",
    "                            if (not word in wordranks[-2000:]) and pos != \"cd\":\n",
    "                                swordtypes[word]=1\n",
    "                                swordtokens+=1\n",
    "                            if pos[0]==\"n\":\n",
    "                                lextypes[word]=1\n",
    "                                nountypes[word]=1\n",
    "                                lextokens+=1\n",
    "                                nountokens+=1\n",
    "                                if not word in wordranks[-2000:]:\n",
    "                                    slextypes[word]=1\n",
    "                                    slextokens+=1\n",
    "                            elif pos[0]==\"j\":\n",
    "                                lextypes[word]=1\n",
    "                                adjtypes[word]=1\n",
    "                                lextokens+=1\n",
    "                                adjtokens+=1\n",
    "                                if not word in wordranks[-2000:]:\n",
    "                                    slextypes[word]=1\n",
    "                                    slextokens+=1\n",
    "                            elif pos[0]==\"r\" and (adjdict.has_key(word) or (word[-2:]==\"ly\" and adjdict.has_key(word[:-2]))):\n",
    "                                lextypes[word]=1\n",
    "                                advtypes[word]=1\n",
    "                                lextokens+=1\n",
    "                                advtokens+=1\n",
    "                                if not word in wordranks[-2000:]:\n",
    "                                    slextypes[word]=1\n",
    "                                    slextokens+=1\n",
    "                            elif pos[0]==\"v\" and not word in [\"be\",\"have\"]:\n",
    "                                verbtypes[word]=1\n",
    "                                verbtokens+=1\n",
    "                                lextypes[word]=1\n",
    "                                lextokens+=1\n",
    "                                if not word in wordranks[-2000:]:\n",
    "                                    sverbtypes[word]=1\n",
    "                                    slextypes[word]=1\n",
    "                                    slextokens+=1\n",
    "                        except(AttributeError):\n",
    "                            pass\n",
    "\n",
    "            # 1. lexical density\n",
    "            if wordtokens > 0:\n",
    "                ld=float(lextokens)/wordtokens\n",
    "            else:\n",
    "                ld=0\n",
    "            # 2. lexical sophistication\n",
    "            # 2.1 lexical sophistication\n",
    "            if lextokens != 0:\n",
    "                ls1=slextokens/float(lextokens)\n",
    "            else:\n",
    "                ls1 = 0\n",
    "            if len(wordtypes.keys()) > 0:\n",
    "                ls2=len(swordtypes.keys())/float(len(wordtypes.keys()))\n",
    "            else:\n",
    "                ls2 = 0\n",
    "\n",
    "            # 2.2 verb sophistication\n",
    "            vs1 = 0\n",
    "            vs2=0\n",
    "            cvs1=0\n",
    "            if verbtokens > 0:\n",
    "                vs1=len(sverbtypes.keys())/float(verbtokens)\n",
    "                vs2=(len(sverbtypes.keys())*len(sverbtypes.keys()))/float(verbtokens)\n",
    "                cvs1=len(sverbtypes.keys())/sqrt(2*verbtokens)\n",
    "\n",
    "            # 3 lexical diversity or variation\n",
    "\n",
    "            # 3.1 NDW, may adjust the values of \"standard\"\n",
    "            ndw=len(wordtypes.keys())\n",
    "\n",
    "            # 3.2 TTR\n",
    "            \n",
    "            if wordtokens > 0:\n",
    "                ttr=len(wordtypes.keys())/float(wordtokens)\n",
    "                if len(lemmalist)>=standard:\n",
    "                    msttr=getmsttr(standard,lemmalist)\n",
    "                cttr=len(wordtypes.keys())/sqrt(2*wordtokens)\n",
    "                rttr=len(wordtypes.keys())/sqrt(wordtokens)\n",
    "            else:\n",
    "                ttr = 0\n",
    "                cttr = 0\n",
    "                rttr = 0\n",
    "            if wordtokens == 0 or len(wordtypes.keys()) == 0:\n",
    "                logttr = 0\n",
    "            else:\n",
    "                logttr=log(len(wordtypes.keys()))/log(wordtokens)\n",
    "            # 3.3 verb diversity\n",
    "            vv1, svv1, cvv1 = 0, 0, 0\n",
    "            if verbtokens > 0:\n",
    "                vv1=len(verbtypes.keys())/float(verbtokens)\n",
    "                svv1=len(verbtypes.keys())*len(verbtypes.keys())/float(verbtokens)\n",
    "                cvv1=len(verbtypes.keys())/sqrt(2*verbtokens)\n",
    "\n",
    "            # 3.4 lexical diversity\n",
    "            if lextokens != 0:\n",
    "                lv=len(lextypes.keys())/float(lextokens)\n",
    "                vv2=len(verbtypes.keys())/float(lextokens)\n",
    "                adjv=len(adjtypes.keys())/float(lextokens)\n",
    "\n",
    "            else:\n",
    "                lv=0\n",
    "                vv2=0\n",
    "                adjv=0\n",
    "\n",
    "            if nountokens != 0:\n",
    "                nv=len(nountypes.keys())/float(nountokens)\n",
    "            else:\n",
    "                nv=0\n",
    "\n",
    "\n",
    "\n",
    "          \n",
    "            lexFeat.append([query, ld, ls1, ls2, vs1, vs2, cvs1, ndw, ttr,\n",
    "                           cttr, rttr, logttr, lv, vv1, svv1, cvv1, vv2, nv, adjv])\n",
    "            pbar.update()\n",
    "    lexical = pd.DataFrame(data = lexFeat, columns = [\"query\", \"ld\", \"ls1\", \"ls2\", \"vs1\", \"vs2\", \"cvs1\", \"ndw\", \"ttr\",\n",
    "                                                      \"cttr\", \"rttr\", \"logttr\", \"lv\", \"vv1\", \"svv1\", \"cvv1\", \"vv2\", \"nv\", \"adjv\"])\n",
    "    return lexical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data Sets\n",
    "\n",
    "This block of code loads the data sets and extracts all unique queries from both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allSessionsSQS = list(pickle.load( open( \"../Data/DataSets/SQS/SQS.p\", \"rb\" ) ))\n",
    "allQueries = allSessions['query'].tolist()\n",
    "allQueries = allQueries + list(allSessionsSQS)\n",
    "setQueries = set(allQueries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Lexical Characteristics\n",
    "\n",
    "This block of code extracts lexical characteristics from each query and starts building a dataframe for the values of these features. Filter warnings are set to ignore, as encountering numbers as well as characters with modifiers such as umlauts throw a \n",
    "\n",
    "*UserWarning: Character not defined in sonority_hierarchy*\n",
    "\n",
    "which leads to the character/number being recast as the same symbol, but in a way that is recognized by NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 19024/70485 [00:57<02:35, 331.58it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-338e6b434e85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mleast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSSP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mrunning\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# if only one vowel return word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvowels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# if only one vowel return word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvowels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "totalSyl = []\n",
    "avgSyl = []\n",
    "simWords = []\n",
    "comWords = []\n",
    "simWordsAvg = []\n",
    "comWordsAvg = []\n",
    "mostSyl = []\n",
    "leastSyl = []\n",
    "SSP = SyllableTokenizer()\n",
    "\n",
    "with tqdm(total = len(setQueries) ) as pbar:\n",
    "    for text in setQueries:\n",
    "        running = 0\n",
    "        count = 0\n",
    "        simpleWords = 0\n",
    "        complexWords = 0\n",
    "        most = 0\n",
    "        least = sys.maxsize\n",
    "        for word in text.split(\" \"):\n",
    "            current = len(SSP.tokenize(word))\n",
    "            running += current\n",
    "            count +=1\n",
    "            if current < 3:\n",
    "                simpleWords += 1\n",
    "            else:\n",
    "                complexWords +=1\n",
    "            if most < current:\n",
    "                most = current\n",
    "            if least > current:\n",
    "                least = current\n",
    "                \n",
    "        totalSyl.append(running)\n",
    "        avgSyl.append(running/count)\n",
    "        simWords.append(simpleWords)\n",
    "        comWords.append(complexWords)\n",
    "        mostSyl.append(most)\n",
    "        leastSyl.append(least)\n",
    "        pbar.update()\n",
    "        \n",
    "lexChar = pd.DataFrame(setQueries)\n",
    "lexChar = lexChar.set_index(0, drop=True)\n",
    "lexChar = lexChar.reset_index().rename(columns={0:'query'})\n",
    "\n",
    "lexChar['totalSyl'] = totalSyl\n",
    "lexChar['avgSyl'] = avgSyl\n",
    "lexChar['simWords'] = simWords\n",
    "lexChar['comWords'] = comWords\n",
    "lexChar['greatestSyl'] = mostSyl\n",
    "lexChar['leastSyl'] = leastSyl\n",
    "lexChar['numChars'] = lexChar['query'].str.len()\n",
    "lexChar['numWords'] = lexChar['query'].str.split().str.len()\n",
    "lexChar['avgLenWord'] = lexChar['numChars']/lexChar['numWords']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Lexical Complexity\n",
    "\n",
    "This block of code below runs the previously defined functions that extract the feature corresponding to lexical complexity. This can be very slow/time consuming.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-12 17:33:13 WARNING: Can not find tokenize: gsd from official model list. Ignoring it.\n",
      "2021-10-12 17:33:13 WARNING: Can not find pos: bnc from official model list. Ignoring it.\n",
      "2021-10-12 17:33:13 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-10-12 17:33:13 INFO: Use device: cpu\n",
      "2021-10-12 17:33:13 INFO: Loading: tokenize\n",
      "2021-10-12 17:33:13 INFO: Loading: pos\n",
      "2021-10-12 17:33:15 INFO: Loading: lemma\n",
      "2021-10-12 17:33:15 INFO: Loading: depparse\n",
      "2021-10-12 17:33:16 INFO: Loading: sentiment\n",
      "2021-10-12 17:33:18 INFO: Loading: ner\n",
      "2021-10-12 17:33:19 INFO: Done loading processors!\n",
      "  0%|          | 268/70485 [00:54<3:57:55,  4.92it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-75e62f4fb1b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlexComplex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetLex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetQueries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-5536535e3fab>\u001b[0m in \u001b[0;36mgetLex\u001b[0;34m(queries)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/stanza/pipeline/core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    164\u001b[0m         assert any([isinstance(doc, str), isinstance(doc, list),\n\u001b[1;32m    165\u001b[0m                     isinstance(doc, Document)]), 'input should be either str, list or Document'\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/stanza/pipeline/core.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPIPELINE_NAMES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprocessor_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/stanza/pipeline/depparse_processor.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_orig_idx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_orig_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/stanza/models/depparse/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, batch, unsort)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordchars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordchars_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeprel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_orig_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentlens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordlens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mhead_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchuliu_edmonds_one_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentlens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# remove attachment for the root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mdeprel_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'deprel'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_seqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/stanza/models/depparse/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel, word_orig_idx, sentlens, wordlens)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mlstm_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mlstm_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparserlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentlens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparserlstm_h_init\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hidden_dim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparserlstm_c_init\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hidden_dim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mlstm_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/stanza/models/common/hlstm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, seqlens, hx)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhighway_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhighway\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lexComplex = getLex(setQueries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine And Return Feature Set\n",
    "\n",
    "This block of code combines all features into one dataframe and outputs that combination as a pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicalFeatures = lexChar.merge(lexComplex)\n",
    "pickle.dump(lexicalFeatures, open( \"Pickles/LexFeat.p\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
