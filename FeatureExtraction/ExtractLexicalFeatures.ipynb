{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook extracts the lexical features from each query, which will then be joined to sessions on queries in feature extraction main. Extracting features relating to lexical complexity can be time consuming, taking approximately 4 hours on a 2.2 i7 processor with 16 gig RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 18.7MB/s]                    \n",
      "2021-10-18 19:24:46 INFO: Downloading default packages for language: en (English)...\n",
      "2021-10-18 19:24:47 INFO: File exists: /Users/bl4z3/stanza_resources/en/default.zip.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a91552bb2dee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/stanza/resources/common.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(lang, dir, package, processors, logging_level, verbose, resources_url, resources_branch, resources_version, model_url)\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'default_md5'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         )\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0munzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m     \u001b[0;31m# Customize: maintain download list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/stanza/resources/common.py\u001b[0m in \u001b[0;36munzip\u001b[0;34m(dir, filename)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Unzip: {dir}/{filename}...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_root_from_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1636\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m              \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;34m\"\"\"copy data from file-like object fsrc to file-like object fdst\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mZIP_DEFLATED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMIN_READ_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m             self._eof = (self._decompressor.eof or\n\u001b[1;32m   1008\u001b[0m                          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import stanza  \n",
    "import string\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from math import sqrt\n",
    "from math import log\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "stanza.download('en') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Lexical Complexity\n",
    "\n",
    "Code taken from:\n",
    "\n",
    "This code is the lexical complexity analyzer described in\n",
    "\n",
    "Lu, Xiaofei (2012). The relationship of lexical richnes to the quality \n",
    "of ESL speakers' oral narratives. The Modern Language Journal, 96(2), 190-208. \n",
    "\n",
    "Version 1.1 Released on February 12, 2013\n",
    "\n",
    "Which can be found at:\n",
    "\n",
    "http://www.personal.psu.edu/xxl13/download.html\n",
    "\n",
    "It has been modified to work with search queries, as it was initially designed for sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust minimum sample size here\n",
    "standard=50\n",
    "\n",
    "# Returns the keys of dictionary d sorted by their values\n",
    "def sort_by_value(d):\n",
    "    items=d.items()\n",
    "    backitems=[ [v[1],v[0]] for v in items]\n",
    "    backitems.sort()\n",
    "    return [ backitems[i][1] for i in range(0,len(backitems))]\n",
    "\n",
    "# NDW for first z words in a sample\n",
    "def getndwfirstz(z,lemmalist):\n",
    "    ndwfirstztype={}\n",
    "    for lemma in lemmalist[:z]:\n",
    "        ndwfirstztype[lemma]=1\n",
    "    return len(ndwfirstztype.keys())\n",
    "\n",
    "# NDW expected random z words, 10 trials\n",
    "def getndwerz(z,lemmalist):\n",
    "    ndwerz=0\n",
    "    for i in range(10):\n",
    "        ndwerztype={}\n",
    "        erzlemmalist=random.sample(lemmalist,z)\n",
    "        for lemma in erzlemmalist:\n",
    "            ndwerztype[lemma]=1\n",
    "        ndwerz+=len(ndwerztype.keys())\n",
    "    return ndwerz/10.0\n",
    "\n",
    "# NDW expected random sequences of z words, 10 trials\n",
    "def getndwesz(z,lemmalist):\n",
    "    ndwesz=0\n",
    "    for i in range(10):\n",
    "        ndwesztype={}\n",
    "        startword=random.randint(0,len(lemmalist)-z)\n",
    "        eszlemmalist=lemmalist[startword:startword+z]\n",
    "        for lemma in eszlemmalist:\n",
    "            ndwesztype[lemma]=1\n",
    "        ndwesz+=len(ndwesztype.keys())\n",
    "    return ndwesz/10.0\n",
    "\n",
    "# MSTTR\n",
    "def getmsttr(z,lemmalist):\n",
    "    samples=0\n",
    "    msttr=0.0\n",
    "    while len(lemmalist)>=z:\n",
    "        samples+=1\n",
    "        msttrtype={}\n",
    "        for lemma in lemmalist[:z]:\n",
    "            msttrtype[lemma]=1\n",
    "        msttr+=len(msttrtype.keys())/float(z)\n",
    "        lemmalist=lemmalist[z:]    \n",
    "    return msttr/samples\n",
    "\n",
    "def isLetterNumber(character):\n",
    "    if character in string.printable and not character in string.punctuation:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def isSentence(line):\n",
    "    for character in line:\n",
    "        if isLetterNumber(character):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# Accepts a list of queries, returns a dataframe of extracted lexical features \n",
    "# that correspond to each query\n",
    "def getLex(queries):\n",
    "    processor_dict = {\n",
    "    'tokenize': 'gsd', \n",
    "    'pos': 'bnc', \n",
    "    'lemma': 'default'\n",
    "    }\n",
    "\n",
    "    nlp = stanza.Pipeline('en', processors=processor_dict)\n",
    "    # reads information from bnc wordlist\n",
    "    lexFeat = []\n",
    "    adjdict={}\n",
    "    verbdict={}\n",
    "    noundict={}\n",
    "    worddict={}\n",
    "    wordlistfile=open(\"DataSets/BNC/bnc_all_filtered.txt\",\"r\")\n",
    "    wordlist=wordlistfile.readlines()\n",
    "    wordlistfile.close()\n",
    "    for word in wordlist:\n",
    "        wordinfo=word.strip()\n",
    "        if not wordinfo or \"Total words\" in wordinfo:\n",
    "            continue\n",
    "        infolist=wordinfo.split()\n",
    "        lemma=infolist[0]\n",
    "        pos=infolist[1]\n",
    "        frequency=int(infolist[2])\n",
    "        worddict[lemma]=worddict.get(lemma,0)+frequency\n",
    "        if pos==\"Adj\":\n",
    "            adjdict[lemma]=adjdict.get(lemma,0)+frequency\n",
    "        elif pos==\"Verb\":\n",
    "            verbdict[lemma]=verbdict.get(lemma,0)+frequency\n",
    "        elif pos==\"NoC\" or pos==\"NoP\":\n",
    "            noundict[lemma]=noundict.get(lemma,0)+frequency\n",
    "    wordranks=sort_by_value(worddict)\n",
    "    verbranks=sort_by_value(verbdict)\n",
    "    length = len(queries)\n",
    "    with tqdm(total = length) as pbar:\n",
    "        for query in queries:\n",
    "            filename=query\n",
    "            doc = nlp(query)\n",
    "            for sentence in doc.sentences:\n",
    "                s = ''\n",
    "                for word in sentence.words:\n",
    "                    s+='{}_{}'.format(word.lemma, word.xpos) + ' '\n",
    "            lemlines= s\n",
    "            #print(lemlines)\n",
    "            # process input file\n",
    "            wordtypes={}\n",
    "            wordtokens=0\n",
    "            swordtypes={}\n",
    "            swordtokens=0\n",
    "            lextypes={}\n",
    "            lextokens=0\n",
    "            slextypes={}\n",
    "            slextokens=0\n",
    "            verbtypes={}\n",
    "            verbtokens=0\n",
    "            sverbtypes={}\n",
    "            adjtypes={}\n",
    "            adjtokens=0\n",
    "            advtypes={}\n",
    "            advtokens=0\n",
    "            nountypes={}\n",
    "            nountokens=0\n",
    "            lemmaposlist=[]\n",
    "            lemmalist=[]\n",
    "\n",
    "            for lemline in lemlines:\n",
    "                lemline=lemline.strip()\n",
    "                lemline=lemline.lower()\n",
    "                if not isSentence(lemline):\n",
    "                    continue\n",
    "                lemmas=lemline.split()\n",
    "                for lemma in lemmas:\n",
    "                    word=lemma.split(\"_\")[0]\n",
    "                    pos=lemma.split(\"_\")[-1]\n",
    "                    if (not pos in string.punctuation) and pos!=\"sent\" and pos!=\"sym\":\n",
    "                        lemmaposlist.append(lemma)\n",
    "                        lemmalist.append(word)  \n",
    "                        wordtokens+=1\n",
    "                        wordtypes[word]=1\n",
    "                        try:\n",
    "\n",
    "                            if (not word in wordranks[-2000:]) and pos != \"cd\":\n",
    "                                swordtypes[word]=1\n",
    "                                swordtokens+=1\n",
    "                            if pos[0]==\"n\":\n",
    "                                lextypes[word]=1\n",
    "                                nountypes[word]=1\n",
    "                                lextokens+=1\n",
    "                                nountokens+=1\n",
    "                                if not word in wordranks[-2000:]:\n",
    "                                    slextypes[word]=1\n",
    "                                    slextokens+=1\n",
    "                            elif pos[0]==\"j\":\n",
    "                                lextypes[word]=1\n",
    "                                adjtypes[word]=1\n",
    "                                lextokens+=1\n",
    "                                adjtokens+=1\n",
    "                                if not word in wordranks[-2000:]:\n",
    "                                    slextypes[word]=1\n",
    "                                    slextokens+=1\n",
    "                            elif pos[0]==\"r\" and (adjdict.has_key(word) or (word[-2:]==\"ly\" and adjdict.has_key(word[:-2]))):\n",
    "                                lextypes[word]=1\n",
    "                                advtypes[word]=1\n",
    "                                lextokens+=1\n",
    "                                advtokens+=1\n",
    "                                if not word in wordranks[-2000:]:\n",
    "                                    slextypes[word]=1\n",
    "                                    slextokens+=1\n",
    "                            elif pos[0]==\"v\" and not word in [\"be\",\"have\"]:\n",
    "                                verbtypes[word]=1\n",
    "                                verbtokens+=1\n",
    "                                lextypes[word]=1\n",
    "                                lextokens+=1\n",
    "                                if not word in wordranks[-2000:]:\n",
    "                                    sverbtypes[word]=1\n",
    "                                    slextypes[word]=1\n",
    "                                    slextokens+=1\n",
    "                        except(AttributeError):\n",
    "                            pass\n",
    "\n",
    "            # 1. lexical density\n",
    "            if wordtokens > 0:\n",
    "                ld=float(lextokens)/wordtokens\n",
    "            else:\n",
    "                ld=0\n",
    "            # 2. lexical sophistication\n",
    "            # 2.1 lexical sophistication\n",
    "            if lextokens != 0:\n",
    "                ls1=slextokens/float(lextokens)\n",
    "            else:\n",
    "                ls1 = 0\n",
    "            if len(wordtypes.keys()) > 0:\n",
    "                ls2=len(swordtypes.keys())/float(len(wordtypes.keys()))\n",
    "            else:\n",
    "                ls2 = 0\n",
    "\n",
    "            # 2.2 verb sophistication\n",
    "            vs1 = 0\n",
    "            vs2=0\n",
    "            cvs1=0\n",
    "            if verbtokens > 0:\n",
    "                vs1=len(sverbtypes.keys())/float(verbtokens)\n",
    "                vs2=(len(sverbtypes.keys())*len(sverbtypes.keys()))/float(verbtokens)\n",
    "                cvs1=len(sverbtypes.keys())/sqrt(2*verbtokens)\n",
    "\n",
    "            # 3 lexical diversity or variation\n",
    "\n",
    "            # 3.1 NDW, may adjust the values of \"standard\"\n",
    "            ndw=len(wordtypes.keys())\n",
    "\n",
    "            # 3.2 TTR\n",
    "            \n",
    "            if wordtokens > 0:\n",
    "                ttr=len(wordtypes.keys())/float(wordtokens)\n",
    "                if len(lemmalist)>=standard:\n",
    "                    msttr=getmsttr(standard,lemmalist)\n",
    "                cttr=len(wordtypes.keys())/sqrt(2*wordtokens)\n",
    "                rttr=len(wordtypes.keys())/sqrt(wordtokens)\n",
    "            else:\n",
    "                ttr = 0\n",
    "                cttr = 0\n",
    "                rttr = 0\n",
    "            if wordtokens == 0 or len(wordtypes.keys()) == 0:\n",
    "                logttr = 0\n",
    "            else:\n",
    "                logttr=log(len(wordtypes.keys()))/log(wordtokens)\n",
    "            # 3.3 verb diversity\n",
    "            vv1, svv1, cvv1 = 0, 0, 0\n",
    "            if verbtokens > 0:\n",
    "                vv1=len(verbtypes.keys())/float(verbtokens)\n",
    "                svv1=len(verbtypes.keys())*len(verbtypes.keys())/float(verbtokens)\n",
    "                cvv1=len(verbtypes.keys())/sqrt(2*verbtokens)\n",
    "\n",
    "            # 3.4 lexical diversity\n",
    "            if lextokens != 0:\n",
    "                lv=len(lextypes.keys())/float(lextokens)\n",
    "                vv2=len(verbtypes.keys())/float(lextokens)\n",
    "                adjv=len(adjtypes.keys())/float(lextokens)\n",
    "\n",
    "            else:\n",
    "                lv=0\n",
    "                vv2=0\n",
    "                adjv=0\n",
    "\n",
    "            if nountokens != 0:\n",
    "                nv=len(nountypes.keys())/float(nountokens)\n",
    "            else:\n",
    "                nv=0\n",
    "\n",
    "\n",
    "\n",
    "          \n",
    "            lexFeat.append([query, ld, ls1, ls2, vs1, vs2, cvs1, ndw, ttr,\n",
    "                           cttr, rttr, logttr, lv, vv1, svv1, cvv1, vv2, nv, adjv])\n",
    "            pbar.update()\n",
    "    lexical = pd.DataFrame(data = lexFeat, columns = [\"query\", \"ld\", \"ls1\", \"ls2\", \"vs1\", \"vs2\", \"cvs1\", \"ndw\", \"ttr\",\n",
    "                                                      \"cttr\", \"rttr\", \"logttr\", \"lv\", \"vv1\", \"svv1\", \"cvv1\", \"vv2\", \"nv\", \"adjv\"])\n",
    "    return lexical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data Sets\n",
    "\n",
    "This block of code loads the data sets and extracts all unique queries from both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allSessionsSQS = list(pickle.load( open( \"../Data/DataSets/SQS/SQS.p\", \"rb\" ) ))\n",
    "allQueries = allSessions['query'].tolist()\n",
    "allQueries = allQueries + list(allSessionsSQS)\n",
    "setQueries = set(allQueries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Lexical Characteristics\n",
    "\n",
    "This block of code extracts lexical characteristics from each query and starts building a dataframe for the values of these features. Filter warnings are set to ignore, as encountering numbers as well as characters with modifiers such as umlauts throw a \n",
    "\n",
    "*UserWarning: Character not defined in sonority_hierarchy*\n",
    "\n",
    "which leads to the character/number being recast as the same symbol, but in a way that is recognized by NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "totalSyl = []\n",
    "avgSyl = []\n",
    "simWords = []\n",
    "comWords = []\n",
    "simWordsAvg = []\n",
    "comWordsAvg = []\n",
    "mostSyl = []\n",
    "leastSyl = []\n",
    "SSP = SyllableTokenizer()\n",
    "\n",
    "with tqdm(total = len(setQueries) ) as pbar:\n",
    "    for text in setQueries:\n",
    "        running = 0\n",
    "        count = 0\n",
    "        simpleWords = 0\n",
    "        complexWords = 0\n",
    "        most = 0\n",
    "        least = sys.maxsize\n",
    "        for word in text.split(\" \"):\n",
    "            current = len(SSP.tokenize(word))\n",
    "            running += current\n",
    "            count +=1\n",
    "            if current < 3:\n",
    "                simpleWords += 1\n",
    "            else:\n",
    "                complexWords +=1\n",
    "            if most < current:\n",
    "                most = current\n",
    "            if least > current:\n",
    "                least = current\n",
    "                \n",
    "        totalSyl.append(running)\n",
    "        avgSyl.append(running/count)\n",
    "        simWords.append(simpleWords)\n",
    "        comWords.append(complexWords)\n",
    "        mostSyl.append(most)\n",
    "        leastSyl.append(least)\n",
    "        pbar.update()\n",
    "        \n",
    "lexChar = pd.DataFrame(setQueries)\n",
    "lexChar = lexChar.set_index(0, drop=True)\n",
    "lexChar = lexChar.reset_index().rename(columns={0:'query'})\n",
    "\n",
    "lexChar['totalSyl'] = totalSyl\n",
    "lexChar['avgSyl'] = avgSyl\n",
    "lexChar['simWords'] = simWords\n",
    "lexChar['comWords'] = comWords\n",
    "lexChar['greatestSyl'] = mostSyl\n",
    "lexChar['leastSyl'] = leastSyl\n",
    "lexChar['numChars'] = lexChar['query'].str.len()\n",
    "lexChar['numWords'] = lexChar['query'].str.split().str.len()\n",
    "lexChar['avgLenWord'] = lexChar['numChars']/lexChar['numWords']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Lexical Complexity\n",
    "\n",
    "This block of code below runs the previously defined functions that extract the feature corresponding to lexical complexity. This can be very slow/time consuming.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexComplex = getLex(setQueries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine And Return Feature Set\n",
    "\n",
    "This block of code combines all features into one dataframe and outputs that combination as a pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicalFeatures = lexChar.merge(lexComplex)\n",
    "pickle.dump(lexicalFeatures, open( \"Pickles/LexFeat.p\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
