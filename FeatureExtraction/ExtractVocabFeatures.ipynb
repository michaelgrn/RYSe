{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code in this notebook extracts vocabulary features from each query and then returns a data frame of those extracted features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries\n",
    "\n",
    "The following block of code loads all libraries needed for this notebook. Numpy has an established to ensure that the random selection of queries drawn to establish certain features, such as top word n-grams; is consistent across this code and future execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "np.random.seed(20200522)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Vocabulary Features\n",
    "\n",
    "Features used in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts a list (lst) into a dictionary\n",
    "\n",
    "def Convert(lst):\n",
    "    res_dct = {lst[i]: lst[i + 1] for i in range(0, len(lst), 2)}\n",
    "    return res_dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data Sets\n",
    "\n",
    "This block of code loads the data sets and extracts all unique queries from both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allSessionsSQS = list(pickle.load( open( \"../Data/DataSets/SQS/SQS.p\", \"rb\" ) ))\n",
    "allQueries = allSessions['query'].tolist() \n",
    "allQueries = allQueries + list(allSessionsSQS)\n",
    "allQueries = set(allQueries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Vocab\n",
    "\n",
    "Loads all vocabulary expected to be learned between Kindergarten to Seventh grade based on Common Core Curriculum, before extracting the ratio of words in each query that are, and are not; found in this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kd = ['a', 'all', 'am', 'an', 'and', 'are', 'as', 'at', 'away', 'back', 'ball', 'bell', 'big', 'bird', 'blue', 'book', 'boot', 'box', 'boy', 'brown', 'but', 'by', 'can', 'car', 'cat', 'come', 'cow', 'day', 'do', 'dog', 'down', 'end', 'fall', 'fan', 'fish', 'fly', 'food', 'for', 'from', 'fun', 'get', 'go', 'good', 'gray', 'green', 'groundhog', 'hat', 'he', 'here', 'hill', 'I', 'in', 'into', 'is', 'it', 'inside', 'kitten', 'little', 'look', 'mad', 'me', 'mud', 'my', 'name', 'no', 'not', 'of', 'on', 'orange', 'out', 'paint', 'pet', 'pin', 'play', 'put', 'rain', 'red', 'run', 'sad', 'say', 'see', 'she', 'sing', 'sit', 'so', 'stay', 'stop', 'story', 'sun', 'take', 'that', 'the', 'them', 'then', 'there', 'they', 'this', 'to', 'too', 'up', 'we', 'wet', 'what', 'where', 'who', 'will', 'with', 'work', 'yellow', 'yes', 'you', 'zoo', 'orange', 'white', 'black', 'monday', 'tuesday', 'wednesday','thursday','friday', 'saturday','sunday']\n",
    "oned = ['a', 'all', 'am', 'and', 'at', 'ball', 'be', 'bed', 'big', 'book', 'box', 'boy', 'but', 'came', 'can', 'car', 'cat', 'come', 'cow', 'dad', 'day', 'did', 'do', 'dog', 'fat', 'for', 'fun', 'get', 'go', 'good', 'got', 'had', 'hat', 'he', 'hen', 'here', 'him', 'his', 'home', 'hot', 'I', 'if', 'in', 'into', 'is', 'it', 'its', 'let', 'like', 'look', 'man', 'may', 'me', 'mom', 'my', 'no', 'not', 'of', 'oh', 'old', 'on', 'one', 'out', 'pan', 'pet', 'pig', 'play', 'ran', 'rat', 'red', 'ride', 'run', 'sat', 'see', 'she', 'sit', 'six', 'so', 'stop', 'sun', 'ten', 'the', 'this', 'to', 'top', 'toy', 'two', 'up', 'us', 'was', 'we', 'will', 'yes', 'you' ]\n",
    "twod = ['about', 'add', 'after', 'ago', 'an ', 'any', 'apple', 'are ', 'as', 'ask', 'ate', 'away', 'baby ', 'back', 'bad', 'bag', 'base', 'bat', 'bee', 'been', 'before', 'being', 'best', 'bike', 'bill', 'bird', 'black', 'blue', 'boat', 'both', 'bring', 'brother ', 'brown', 'bus', 'buy ', 'by', 'cake', 'call', 'candy', 'change', 'child', 'city', 'clean', 'club', 'coat', 'cold', 'coming ', 'corn', 'could', 'cry', 'cup', 'cut', 'daddy ', 'dear', 'deep', 'deer', 'doing', 'doll', 'door', 'down ', 'dress', 'drive', 'drop', 'dry', 'duck', 'each', 'eat', 'eating', 'egg', 'end', 'fall', 'far', 'farm', 'fast', 'father ', 'feed', 'feel', 'feet', 'fell ', 'find', 'fine ', 'fire', 'first ', 'fish', 'five', 'fix', 'flag', 'floor', 'fly', 'food', 'foot', 'four', 'fox', 'from ', 'full', 'funny', 'game', 'gas', 'gave', 'girl', 'give', 'glad', 'goat', 'goes ', 'going ', 'gold', 'gone', 'grade ', 'grass', 'green', 'grow', 'hand', 'happy', 'hard', 'has ', 'have ', 'hear ', 'help', 'here ', 'hill', 'hit', 'hold', 'hole', 'hop', 'hope ', 'horse', 'house ', 'how ', 'ice', 'inch', 'inside ', 'job', 'jump', 'just ', 'keep', 'king', 'know ', 'lake', 'land', 'last', 'late', 'lay', 'left', 'leg', 'light', 'line', 'little ', 'live', 'lives', 'long', 'looking', 'lost', 'lot', 'love', 'mad', 'made ', 'make ', 'many ', 'meat', 'men', 'met', 'mile', 'milk', 'mine', 'miss', 'moon', 'more', 'most', 'mother ', 'move', 'much ', 'must', 'myself ', 'nail', 'name ', 'need', 'new ', 'next', 'nice ', 'night', 'nine', 'north', 'now ', 'nut', 'off ', 'only', 'open', 'or ', 'other', 'our', 'outside ', 'over', 'page', 'park', 'part', 'pay', 'pick', 'plant', 'playing', 'pony', 'post', 'pull', 'put', 'rabbit', 'rain', 'read', 'rest', 'riding', 'road', 'rock', 'room', 'said ', 'same', 'sang', 'saw ', 'say', 'school ', 'sea', 'seat', 'seem', 'seen', 'send', 'set', 'seven', 'sheep', 'ship', 'shoe', 'show ', 'sick', 'side', 'sing', 'sky', 'sleep', 'small', 'snow', 'some ', 'soon ', 'spell', 'start', 'stay', 'still', 'store ', 'story', 'take', 'talk', 'tall', 'teach', 'tell', 'than ', 'thank', 'that', 'them ', 'then ', 'there ', 'they ', 'thing', 'think ', 'three', 'time ', 'today ', 'told', 'too ', 'took', 'train ', 'tree', 'truck', 'try', 'use', 'very ', 'walk', 'want ', 'warm', 'wash', 'way', 'week', 'well ', 'went ', 'were ', 'wet', 'what', 'when ', 'while ', 'white', 'who', 'why', 'wind', 'wish', 'with ', 'woke', 'wood', 'work', 'yellow', 'yet', 'your', 'zoo']\n",
    "threed = ['able', 'above', 'afraid', 'afternoon', 'again', 'age', 'air', 'airplane', 'almost', 'alone', 'along', 'already', 'also', 'always', 'animal', 'another', 'anything', 'around', 'art', 'aunt', 'balloon', 'bark', 'barn', 'basket', 'beach', 'bear', 'because', 'become', 'began', 'begin', 'behind', 'believe', 'below', 'belt', 'better', 'birthday', 'body', 'bones', 'born', 'bought', 'bread', 'bright', 'broke', 'brought', 'busy', 'cabin', 'cage', 'camp', 'can\\'t', 'care', 'carry', 'catch', 'cattle', 'cave', 'children', 'class', 'close', 'cloth', 'coal', 'color', 'corner', 'cotton', 'cover', 'dark', 'desert', 'didn\\'t', 'dinner', 'dishes', 'does', 'done', 'don\\'t', 'dragon', 'draw', 'dream', 'drink', 'early', 'earth', 'east', 'eight', 'even', 'ever', 'every', 'everyone', 'everything', 'eyes', 'face', 'family', 'feeling', 'felt', 'few', 'fight', 'fishing', 'flower', 'flying', 'follow', 'forest', 'forgot', 'form', 'found', 'fourth', 'free', 'Friday', 'friend', 'front', 'getting', 'given', 'grandmother', 'great', 'grew', 'ground', 'guess', 'hair', 'half', 'having', 'head', 'heard', 'he\\'s', 'heat', 'hello', 'high', 'himself', 'hour', 'hundred', 'hurry', 'hurt', 'I\\'d', 'I\\'ll', 'I\\'m', 'inches', 'isn\\'t', 'it\\'s', 'I\\'ve', 'kept', 'kids', 'kind', 'kitten', 'knew', 'knife', 'lady', 'large', 'largest', 'later', 'learn', 'leave', 'let\\'s', 'letter', 'life', 'list', 'living', 'lovely', 'loving', 'lunch', 'mail', 'making', 'maybe', 'mean', 'merry', 'might', 'mind', 'money', 'month', 'morning', 'mouse', 'mouth', 'Mr.', 'Mrs.', 'Ms.', 'music', 'near', 'nearly', 'never', 'news', 'noise', 'nothing', 'number', 'o\\'clock', 'often', 'oil', 'once', 'orange', 'order', 'own', 'pair', 'paint', 'paper', 'party', 'pass', 'past', 'penny', 'people', 'person', 'picture', 'place', 'plan', 'plane', 'please', 'pocket', 'point', 'poor', 'race', 'reach', 'reading', 'ready', 'real', 'rich', 'right', 'river', 'rocket', 'rode', 'round', 'rule', 'running', 'salt', 'says', 'sending', 'sent', 'seventh', 'sew', 'shall', 'short', 'shot', 'should', 'sight', 'sister', 'sitting', 'sixth', 'sled', 'smoke', 'soap', 'someone', 'something', 'sometime', 'song', 'sorry', 'sound', 'south', 'space', 'spelling', 'spent', 'sport', 'spring', 'stairs', 'stand', 'state', 'step', 'stick', 'stood', 'stopped', 'stove', 'street', 'strong', 'study', 'such', 'sugar', 'summer', 'Sunday', 'supper', 'table', 'taken', 'taking', 'talking', 'teacher', 'team', 'teeth', 'tenth', 'that\\'s', 'their', 'these', 'thinking', 'third', 'those', 'thought', 'throw', 'tonight', 'trade', 'trick', 'trip', 'trying', 'turn', 'twelve', 'twenty', 'uncle', 'under', 'upon', 'wagon', 'wait', 'walking', 'wasn\\'t', 'watch', 'water', 'weather', 'we\\'re', 'west', 'wheat', 'where', 'which', 'wife', 'wild', 'win', 'window', 'winter', 'without', 'woman', 'won', 'won\\'t', 'wool', 'word', 'working', 'world', 'would', 'write', 'wrong', 'yard', 'year', 'yesterday', 'you\\'re'  ]\n",
    "fourd = ['across', 'against', 'answer', 'awhile', 'between', 'board', 'bottom', 'breakfast', 'broken', 'build', 'building', 'built', 'captain', 'carried', 'caught', 'charge', 'chicken', 'circus', 'cities', 'clothes', 'company', 'couldn\\'t', 'country', 'discover', 'doctor', 'doesn\\'t', 'dollar', 'during', 'eighth', 'else', 'enjoy', 'enough', 'everybody', 'example', 'except', 'excuse', 'field', 'fifth', 'finish', 'following', 'good-by', 'group', 'happened', 'harden', 'haven\\'t', 'heavy', 'held', 'hospital', 'idea', 'instead', 'known', 'laugh', 'middle', 'minute', 'mountain', 'ninth', 'ocean', 'office', 'parent', 'peanut', 'pencil', 'picnic', 'police', 'pretty', 'prize', 'quite', 'radio', 'raise', 'really', 'reason', 'remember', 'return', 'Saturday', 'scare', 'second', 'since', 'slowly', 'stories', 'student', 'sudden', 'suit', 'sure', 'swimming', 'though', 'threw', 'tired', 'together', 'tomorrow', 'toward', 'tried', 'trouble', 'truly', 'turtle', 'until', 'village', 'visit', 'wear', 'we\\'ll', 'whole', 'whose', 'women', 'wouldn\\'t', 'writing', 'written', 'wrote', 'yell', 'young']\n",
    "fived = ['although', 'America', 'among', 'arrive', 'attention', 'beautiful', 'countries', 'course', 'cousin', 'decide', 'different', 'evening', 'favorite', 'finally', 'future', 'happiest', 'happiness', 'important', 'interest', 'piece', 'planet', 'present', 'president', 'principal', 'probably', 'problem', 'receive', 'sentence', 'several', 'special', 'suddenly', 'suppose', 'surely', 'surprise', 'they\\'re', 'through', 'usually', 'action', 'actor', 'actually', 'addition', 'agreed', 'allowed', 'aloud', 'amendment', 'amount', 'amusement', 'annual', 'appointed', 'arrange', 'attention', 'awhile', 'beginning', 'bruise', 'business', 'calves', 'capital', 'capitol', 'captain', 'carefully', 'caught', 'cause', 'celebrate', 'century', 'chemical', 'chocolate', 'circle', 'climate', 'climbed', 'collar', 'column', 'company', 'condition', 'consider', 'consonant', 'constant', 'continent', 'continued', 'country', 'course', 'crystal', 'current', 'curtain', 'daughter', 'daytime', 'decided', 'decimal', 'delicious', 'desert', 'dessert', 'details', 'determine', 'dictionary', 'difference', 'different', 'difficult', 'direction', 'disappoint', 'division', 'eighth', 'election', 'elements', 'energy', 'enjoyment', 'equal', 'equation', 'errands', 'exact', 'except', 'expect', 'explain', 'explode', 'express', 'factory', 'fault', 'favorite', 'finally', 'finished', 'forward', 'fought', 'fraction', 'furniture', 'future', 'general', 'government', 'graceful', 'graph', 'grasp', 'grease', 'grown-ups', 'guest', 'guide', 'happened', 'happily', 'harvest', 'healthy', 'height', 'hoarse', 'human', 'idea', 'imagine', 'include', 'increase', 'indicate', 'information', 'instrument', 'intention', 'interesting', 'inventor', 'island', 'jewel', 'journey', 'jungle', 'knives', 'known', 'language', 'laughter', 'length', 'limb', 'located', 'lumber', 'major', 'mammal', 'manufacture', 'material', 'mayor', 'measure', 'melody', 'members', 'memories', 'message', 'method', 'million', 'minor', 'modern', 'mountain', 'music', 'natural', 'necessary', 'neither', 'newspaper', 'northern', 'notebook', 'notice', 'noun', 'numeral', 'object', 'observe', 'opposite', 'orphan', 'ought', 'outside', 'oxygen', 'paid', 'paint', 'paragraph', 'pattern', 'pause', 'payment', 'perhaps', 'period', 'permit', 'phone', 'phrase', 'pleasant', 'pleasure', 'plural', 'poison', 'position', 'possible', 'practice', 'prepared', 'president', 'probably', 'problem', 'process', 'produce', 'program', 'promise', 'property', 'protection', 'provide', 'puzzle', 'quickly', 'quietly', 'radio', 'raise', 'rarely', 'rather', 'reached', 'receive', 'record', 'region', 'relax', 'remain', 'remove', 'repay', 'repeat', 'report', 'represent', 'respond', 'result', 'rhythm', 'rising', 'ruin', 'salad', 'sandal', 'scale', 'scent', 'schedule', 'science', 'section', 'separate', 'service', 'settled', 'several', 'shadow', 'shelter', 'shoulder', 'shouted', 'shower', 'signal', 'similar', 'sincerely', 'single', 'size', 'slippery', 'soar', 'soil', 'solution', 'solve', 'southern', 'split', 'spoiled', 'sports', 'square', 'squeeze', 'stain', 'state', 'statement', 'station', 'steer', 'stomach', 'stopping', 'straight', 'straighten', 'stream', 'stretched', 'suggest', 'suitcase', 'sunset', 'supply', 'sure', 'surface', 'surprise', 'surround', 'sweater', 'syllable', 'syrup', 'tablet', 'tasty', 'teaspoon', 'terrible', 'though', 'thoughtful', 'thrown', 'tornado', 'toward', 'traffic', 'trail', 'treasure', 'treatment', 'triangle', 'trouble', 'tunnel', 'type', 'understood', 'unknown', 'usually', 'value', 'various', 'warn', 'weigh', 'weight', 'weird', 'western', 'whisper', 'whoever', 'whole', 'whose', 'wives', 'women', 'wonderful', 'wound', 'wreck', 'x-ray', 'yesterday']\n",
    "sixd = ['Abandon', 'abundant', 'access', 'accommodate', 'accumulate', 'adapt', 'adhere', 'agony', 'allegiance', 'ambition', 'ample', 'anguish', 'anticipate', 'anxious', 'apparel', 'appeal', 'apprehensive', 'arid', 'arrogant', 'awe', 'Barren', 'beacon', 'beneficial', 'blunder', 'boisterous', 'boycott', 'burden', 'Campaign', 'capacity', 'capital', 'chronological', 'civic', 'clarity', 'collaborate', 'collide', 'commend', 'commentary', 'compact', 'composure', 'concise', 'consent', 'consequence', 'conserve', 'conspicuous', 'constant', 'contaminate', 'context', 'continuous', 'controversy', 'convenient', 'cope', 'cordial', 'cultivate', 'cumulative', '', 'Declare', 'deluge', 'dense', 'deplete', 'deposit', 'designate', 'desperate', 'deteriorate', 'dialogue', 'diligent', 'diminish', 'discretion', 'dissent', 'dissolve', 'distinct', 'diversity', 'domestic', 'dominate', 'drastic', 'duration', 'dwell', 'Eclipse', 'economy', 'eerie', 'effect', 'efficient', 'elaborate', 'eligible', 'elude', 'encounter', 'equivalent', 'erupt', 'esteem', 'evolve', 'exaggerate', 'excel', 'exclude', 'expanse', 'exploit', 'extinct', 'extract', 'Factor', 'former', 'formulates', 'fuse', 'futile', 'Generate', 'genre', 'Habitat', 'hazardous', 'hoax', 'hostile', 'Idiom', 'ignite', 'immense', 'improvises', 'inept', 'inevitable', 'influence', 'ingenious', 'innovation', 'intimidate', 'Jovial', 'Knack', 'Leeway', 'legislation', 'leisure', 'liberate', 'likeness', 'linger', 'literal', 'loathe', 'lure', 'Majority', 'makeshift', 'manipulate', 'marvel', 'massive', 'maximum', 'meager', 'mere', 'migration', 'mimic', 'minute', 'monotonous', 'Negotiate', 'Objective', 'obstacle', 'omniscient', 'onset', 'optimist', 'originate', 'Painstaking', 'paraphrase', 'parody', 'persecute', 'plummet', 'possess', 'poverty', 'precise', 'predicament', 'predict', 'prejudice', 'preliminary', 'primitive', 'priority', 'prominent', 'propel', 'prosecute', 'prosper', 'provoke', 'pursue', 'Quest', 'Recount', 'refuge', 'reinforce', 'reluctant', 'remorse', 'remote', 'resolute', 'restrain', 'retaliate', 'retrieve', 'rigorous', 'rural', 'Salvage', 'sanctuary', 'siege', 'significant', 'solar', 'soothe', 'stationary', 'stifle', 'strive', 'subordinate', 'subsequent', 'superior', 'supplement', 'swarm', 'Tangible', 'terminate', 'terrain', 'trait', 'transform', 'transport', 'treacherous', 'Unanimous', 'unique', 'unruly', 'urban', 'Vacate', 'verdict', 'verge', 'vibrant', 'vital', 'vow', 'accept', 'accidentally', 'acquire', 'ambulance', 'ancient', 'appearance', 'appointment', 'arithmetic', 'audience', 'autumn', 'beautifully', 'beliefs', 'blown', 'bough', 'bows', 'calendar', 'canyon', 'capable', 'capacity', 'caution', 'ceiling', 'champion', 'choir', 'cleanse', 'combination', 'comfortable', 'community', 'complain', 'concentration', 'concern', 'connection', 'constitution', 'contagious', 'conversation', 'cooperation', 'correct', 'coupon', 'creative', 'creature', 'crisis', 'culture', 'curious', 'dangerous', 'decision', 'demonstrate', 'denominator', 'department', 'departure', 'depth', 'descendant', 'disagreement', 'disastrous', 'discussion', 'distance', 'distributed', 'earliest', 'echoes', 'edition', 'educate', 'electricity', 'element', 'elevator', 'emergency', 'employer', 'emptiness', 'encouragement', 'encyclopedia', 'entire', 'entrance', 'envelope', 'equator', 'especially', 'establish', 'example', 'excellent', 'excitement', 'exercise', 'experience', 'exterior', 'familiar', 'faucet', 'fierce', 'fireproof', 'following', 'forgetting', 'forgiveness', 'fossil', 'freight', 'frighten', 'fuel', 'further', 'gallon', 'gaze', 'gesture', 'governor', 'graduation', 'grateful', 'grief', 'halves', 'hamburger', 'hangar', 'hanger', 'happiness', 'headache', 'heroes', 'history', 'honorable', 'horizon', 'hunger', 'hyphen', 'ignore', 'imagination', 'immediate', 'importance', 'improvement', 'independence', 'ingredient', 'injury', 'inquire', 'instead', 'instruction', 'intermission', 'interview', 'invisible', 'invitation', 'involve', 'jealous', 'junior', 'knowledge', 'lawyer', 'league', 'legal', 'liberty', 'liquid', 'listening', 'loaves', 'location', 'luggage', 'manager', 'manner', 'manor', 'marriage', 'meant', 'mechanic', 'medicine', 'mention', 'minus', 'minute', 'mistaken', 'misunderstand', 'mixture', 'mourn', 'multiple', 'muscle', 'museum', 'musician', 'mute', 'myth', 'nationality', 'negative', 'noisy', 'noticeable', 'novel', 'numerator', 'obtain', 'occur', 'official', 'operate', 'original', 'outline', 'partial', 'passenger', 'patient', 'penalty', 'penguin', 'percent', 'performance', 'personal', 'persuade', 'physical', 'piano', 'plumber', 'poem', 'poet', 'policy', 'pollute', 'pollution', 'positive', 'potatoes', 'predict', 'prefer', 'pressure', 'prevent', 'principal', 'private', 'project', 'pumpkins', 'purchase', 'purse', 'quote', 'radius', 'rapid', 'ratio', 'realize', 'recently', 'recycle', 'reduce', 'referred', 'regardless', 'regular', 'rehearse', 'relief', 'relieve', 'remarkable', 'remind', 'remote', 'replacement', 'replied', 'reply', 'requirement', 'rescue', 'resident', 'resources', 'respectful', 'review', 'roam', 'routine', 'rumor', 'rural', 'safety', 'sailor', 'salute', 'satisfy', 'scarcely', 'scientific', 'scissors', 'selection', 'senior', 'sentence', 'separately', 'serious', 'session', 'shampoo', 'shelves', 'shorten', 'silent', 'simply', 'sketch', 'skillful', 'solar', 'sought', 'spaghetti', 'sponge', 'squawk', 'storage', 'strain', 'strategy', 'strength', 'strive', 'struggle', 'studios', 'success', 'suggestion', 'support', 'surrounded', 'sword', 'system', 'telephone', 'television', 'temperature', 'theme', 'themselves', 'therefore', 'thicken', 'thousand', 'threat', 'tomatoes', 'trophies', 'tutor', 'unbelievable', 'underneath', 'unite', 'vacuum', 'vain', 'variety', 'vary', 'vault', 'vegetable', 'vein', 'violence', 'visible', 'vision', 'waste', 'who\\'s', 'whose', 'wrestle', 'wrinkle', 'yield']\n",
    "sevend = ['abbreviation', 'absence', 'absolutely', 'absorb', 'abundant', 'accessible', 'accompanied', 'accomplishment', 'accurate', 'achievement', 'acres', 'adequate', 'adjustable', 'admit', 'admittance', 'advice', 'advise', 'afghan', 'alternate', 'alternative', 'amusement', 'analysis', 'analyze', 'ancestor', 'anniversary', 'appreciate', 'artificial', 'assistance', 'association', 'athlete', 'atmosphere', 'attendance', 'authority', 'bacteria', 'bagel', 'baggage', 'benefited', 'benefiting', 'bicycle', 'biscuit', 'bizarre', 'boulevard', 'boundary', 'bouquet', 'brilliant', 'brochure', 'bulletin', 'bureau', 'campaign', 'cancellation', 'candidate', 'capable', 'capital', 'capitol', 'category', 'celery', 'cemetery', 'changeable', 'chaperone', 'character', 'cinnamon', 'civilize', 'commercial', 'committed', 'committee', 'commotion', 'companion', 'competent', 'competition', 'complement', 'complex', 'compliment', 'compressor', 'concentrate', 'concentration', 'conductor', 'confetti', 'congratulations', 'consequently', 'controlling', 'cringe', 'culminate', 'culprit', 'deceive', 'delayed', 'democracy', 'deodorant', 'descendent', 'description', 'diameter', 'diamond', 'discourage', 'disgraceful', 'dismissal', 'distinguished', 'dreadful', 'economics', 'economy', 'elementary', 'embarrass', 'emotion', 'emphasize', 'encircle', 'enclosing', 'encounter', 'endurance', 'engineer', 'environment', 'episode', 'erosion', 'eruption', 'evident', 'exchange', 'executive', 'exhibit', 'expensive', 'extinct', 'extinguish', 'extraordinary', 'extremely', 'fabricate', 'failure', 'fascinating', 'fatigue', 'flagrant', 'foreign', 'forfeit', 'frequently', 'fundamental', 'genuine', 'ghetto', 'gossiping', 'gradual', 'graffiti', 'grammar', 'grievance', 'guarantee', 'harass', 'havoc', 'heroic', 'hesitate', 'horrify', 'hospital', 'humid', 'humility', 'hygiene', 'identical', 'idle', 'idol', 'illegal', 'illustration', 'imaginary', 'immediately', 'immobilize', 'impossibility', 'inconvenient', 'incredible', 'individual', 'infamous', 'influence', 'informant', 'inhabit', 'inherit', 'innocence', 'innocent', 'instructor', 'intelligent', 'interruption', 'introduction', 'involvement', 'irate', 'irresistible', 'jealousy', 'judgment', 'juvenile', 'kettle', 'knitting', 'laboratory', 'language', 'legibly', 'liquidation', 'management', 'maneuver', 'media', 'mileage', 'miniature', 'misbehaved', 'morale', 'mortgage', 'movement', 'murmur', 'musician', 'mysterious', 'negotiate', 'nervous', 'nuisance', 'nurture', 'oases', 'oasis', 'obedient', 'obstacle', 'obviously', 'occasion', 'ordinarily', 'ordinary', 'organization', 'pamphlet', 'panic', 'panicked', 'panicky', 'parallel', 'paralysis', 'paralyze', 'penicillin', 'pedestrian', 'phantom', 'pheasant', 'phrase', 'politely', 'popular', 'precipitation', 'principal', 'principle', 'privilege', 'procedure', 'pronunciation', 'psychology', 'puny', 'qualified', 'qualifying', 'quotation', 'raspberry', 'reasonable', 'receipt', 'receiving', 'recipe', 'recognition', 'recommend', 'recruit', 'reddest', 'reprimand', 'resigned', 'restaurant', 'rotten', 'sandwich', 'scarcity', 'scenery', 'secretary', 'securing', 'significance', 'simile', 'sincerely', 'sincerity', 'situation', 'skeptical', 'slumber', 'smudge', 'solemn', 'souvenir', 'spacious', 'specific', 'stationary', 'stationery', 'statistics', 'subscription', 'substitute', 'superintendent', 'supervisor', 'supposedly', 'threatening', 'tolerate', 'tongue', 'tournament', 'tragedy', 'traitor', 'transferred', 'transferring', 'transmitted', 'traveled', 'traveling', 'unfortunately', 'uniform', 'university', 'unnecessary', 'valuable', 'various', 'vehicle', 'version', 'vertical', 'victim', 'vigorously', 'violation', 'visualize', 'volcano', 'voyage', 'wealthy', 'weapon', 'wheeze', 'wilderness', 'Abate', 'abnormal', 'abode', 'abrupt', 'accelerate', 'acclaim', 'acknowledge', 'acquire', 'aspire', 'acrid', 'addict', 'adjacent', 'admonish', 'affliction', 'agitate', 'ajar', 'akin', 'allege', 'annihilate', 'anonymous', 'antagonize', 'apathy', 'arbitrate', 'astute', 'authentic', 'avert', 'Bellow', 'beseech', 'bestow', 'bewilder', 'bigot', 'blatant', 'bleak', 'braggart', 'brawl', 'browse', 'bystander', 'Candid', 'canine', 'canny', 'capricious', 'capsize', 'casual', 'casualty', 'catastrophe', 'cater', 'chorus', 'citrus', 'clamber', 'climax', 'compromise', 'concur', 'confront', 'congested', 'conjure', 'consult', 'corrupt', 'counterfeit', 'covet', 'customary', 'Debut', 'deceased', 'dependent', 'despondent', 'detach', 'devour', 'dishearten', 'dismal', 'dismantle', 'distraught', 'docile', 'downright', 'drone', 'dumbfound', 'Emblem', 'endure', 'ensue', 'enthrall', 'epidemic', 'erode', 'exuberant', 'Fathom', 'feud', 'figment', 'firebrand', 'flabbergast', 'flagrant', 'flaw', 'fruitless', 'Gaudy', 'geography', 'gratify', 'gravity', 'grim', 'grimy', 'grueling', 'gruesome', 'Haggle', 'headlong', 'hilarious', 'homage', 'homicide', 'hospitable', 'hurtle', 'hybrid', 'Illiterate', 'impede', 'implore', 'incident', 'incredulous', 'infamous', 'infuriate', 'insinuate', 'intensified', 'inundate', 'irate', 'Lavish', 'legacy', 'legitimate', 'lethal', 'loath', 'lurk', 'Magnetic', 'mirth', 'quench', 'magnitude', 'maternal', 'maul', 'melancholy', 'mellow', 'momentum', 'mortify', 'mull', 'murky', 'Narrative', 'negligent', 'nimble', 'nomadic', 'noteworthy', 'notify', 'notorious', 'nurture', 'Obnoxious', 'oration', 'orthodox', 'overwhelm', 'Pamper', 'patronize', 'peevish', 'pelt', 'pending', 'perceived', 'perjury', 'permanent', 'persist', 'perturb', 'pique', 'pluck', 'poised', 'ponder', 'potential', 'predatory', 'presume', 'preview', 'prior', 'prowess', 'Radiant', 'random', 'rant', 'recede', 'reprimand', 'resume', 'retort', 'robust', 'rupture', 'Saga', 'sequel', 'sham', 'shirk', 'simultaneously', 'snare', 'species', 'status', 'stodgy', 'substantial', 'subtle', 'sullen', 'supervise', 'Tamper', 'throb', 'toxic', 'tragedy', 'trickle', 'trivial', 'Uncertainty', 'unscathed', 'upright', 'urgent', 'utmost', 'Vengeance', 'vicious', 'vindictive', 'vista', 'vocation', 'void', 'Wary', 'whim', 'wince', 'wrath', 'Yearn']\n",
    "\n",
    "coreVocab = []\n",
    "\n",
    "st = WordNetLemmatizer()\n",
    "for word in kd:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "for word in oned:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "for word in twod:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "for word in threed:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "for word in fourd:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "for word in fived:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "for word in sixd:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "for word in sevend:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70485/70485 [00:14<00:00, 4914.76it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "nonVocab = []\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "\n",
    "        splitQuery = [st.lemmatize(i.lower()) for i in query.split(' ')]\n",
    "\n",
    "        queryVocab = 0\n",
    "        nonqueryVocab = 0\n",
    "        totalVocab = 0\n",
    "\n",
    "        for word in splitQuery:\n",
    "            if word in coreVocab:\n",
    "                queryVocab  +=1\n",
    "                totalVocab  +=1\n",
    "            else:\n",
    "                nonqueryVocab +=1\n",
    "                totalVocab  +=1\n",
    "\n",
    "        vocab.append(queryVocab/totalVocab) \n",
    "        nonVocab.append(nonqueryVocab/totalVocab) \n",
    "        pbar.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab = pd.DataFrame(data=vocab, columns = ['coreVocab'])\n",
    "Vocab['query'] = allQueries\n",
    "Vocab['nonCoreVocab'] = nonVocab\n",
    "Vocab = Vocab.set_index('query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age of Acquisition features\n",
    "\n",
    "In this block of code we first load up the Age of Acquistion data set and process it into a dictionary where the key is the word, and the value is AoA rating. We then find the AoA rating for each word in the query, extracting the min, max, average (known as query complexity), and ratio of words expected to be learned by the age of 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "AoAvocab = []\n",
    "\n",
    "with open('DataSets/AoA/AoA_51715_words.csv') as csvFile:\n",
    "    csvReader = csv.reader(csvFile)\n",
    "    lineCount = 0\n",
    "    for row in csvReader:\n",
    "        if lineCount == 0:\n",
    "            lineCount += 1\n",
    "        else:\n",
    "            AoAvocab.append(row[7])\n",
    "            AoAvocab.append(row[10])\n",
    "            \n",
    "AoAVConv = Convert(AoAvocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70485/70485 [00:10<00:00, 6535.57it/s]\n"
     ]
    }
   ],
   "source": [
    "minAoA = []\n",
    "maxAoA = []\n",
    "averageVocab = []\n",
    "ratioAoA = []\n",
    "\n",
    "st = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "        count = 0\n",
    "        vocab = []\n",
    "\n",
    "        for word in query.split(' '):\n",
    "            word = word.lower().strip()\n",
    "            word = re.sub(r'[^\\w\\s]','',word)\n",
    "            word = st.lemmatize(word)\n",
    "            if word in AoAVConv:\n",
    "                vocab.append(float(AoAVConv[word]))\n",
    "            else:\n",
    "                vocab.append(0)\n",
    "\n",
    "\n",
    "        vocab = np.array(vocab)\n",
    "        \n",
    "        if vocab.size == 0:\n",
    "            minAoA.append(-1) \n",
    "            maxAoA.append(-1) \n",
    "            averageVocab.append(-1)\n",
    "            ratioAoA.append(0)\n",
    "        elif vocab.size > 0:\n",
    "            minAoA.append(np.min(vocab))\n",
    "            maxAoA.append(np.max(vocab))\n",
    "            averageVocab.append(np.mean(vocab))\n",
    "            for entry in vocab:\n",
    "                if entry < 13 and entry > 0:\n",
    "                    count +=1\n",
    "            ratioAoA.append(count/len(vocab))\n",
    "        \n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab['minAoA'] = minAoA\n",
    "Vocab['maxAoA'] = maxAoA\n",
    "Vocab['ratioAoA'] = ratioAoA\n",
    "Vocab['queryComplexity'] = averageVocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sven Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVENwords = []\n",
    "st = WordNetLemmatizer()\n",
    "with open('DataSets/Sven/ChildrenDict.tsv') as csvFile:\n",
    "    csvReader = csv.reader(csvFile, delimiter = '\\t')\n",
    "    lineCount = 0\n",
    "    for row in csvReader:\n",
    "        if lineCount == 0:\n",
    "            lineCount +=1\n",
    "        else:\n",
    "            SVENwords.append((row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70485/70485 [05:51<00:00, 200.70it/s]\n"
     ]
    }
   ],
   "source": [
    "SVENcount = []\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "        vocab = []\n",
    "        countWord = 0\n",
    "        wordCount = 0\n",
    "        for word in query.split(' '):\n",
    "            wordCount +=1\n",
    "            if word in SVENwords:\n",
    "                countWord +=1\n",
    "\n",
    "        SVENcount.append(countWord/wordCount)\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab['SVEN'] = SVENcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Stereotype Uni-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['class'] == 1]\n",
    "sID = allSessionsQ['sID'].unique()\n",
    "corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['sID'].isin(corpus)]\n",
    "queries = allSessionsQ['query'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for query in queries:\n",
    "    text += query.lower() + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "top250Arch = stopwords.words()\n",
    "querywords = text.split()\n",
    "\n",
    "resultwords  = [word for word in querywords if word.lower() not in top250Arch]\n",
    "text = ' '.join(resultwords)\n",
    "text1 = text.split(' ')\n",
    "fdist1 = nltk.FreqDist(text1)\n",
    "top250 = []\n",
    "\n",
    "for x in fdist1.most_common(250):\n",
    "    top250.append(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "top250count = []\n",
    "top250avg = []\n",
    "for query in allQueries:\n",
    "    vocab = []\n",
    "    countWord = 0\n",
    "    wordCount = 0\n",
    "    for word in query.split(' '):\n",
    "        wordCount +=1\n",
    "        if word in top250:\n",
    "            countWord +=1\n",
    "        else:\n",
    "            pass\n",
    "    top250count.append(countWord)\n",
    "    top250avg.append(countWord/wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab['top250All'] = top250count\n",
    "Vocab['top250avgAll'] = top250avg\n",
    "Vocab['top250avgNotAll'] = 1-Vocab['top250avgAll']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Non-Stereotype Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['class'] == 0]\n",
    "sID = allSessionsQ['sID'].unique()\n",
    "corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['sID'].isin(corpus)]\n",
    "queries = allSessionsQ['query'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for query in queries:\n",
    "    text += query.lower() + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "top250Arch = stopwords.words()\n",
    "querywords = text.split()\n",
    "\n",
    "resultwords  = [word for word in querywords if word.lower() not in top250Arch]\n",
    "text = ' '.join(resultwords)\n",
    "text1 = text.split(' ')\n",
    "fdist1 = nltk.FreqDist(text1)\n",
    "top250 = []\n",
    "\n",
    "for x in fdist1.most_common(250):\n",
    "    top250.append(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "top250count = []\n",
    "top250avg = []\n",
    "for query in allQueries:\n",
    "    vocab = []\n",
    "    countWord = 0\n",
    "    wordCount = 0\n",
    "    for word in query.split(' '):\n",
    "        wordCount +=1\n",
    "        if word in top250:\n",
    "            countWord +=1\n",
    "        else:\n",
    "            pass\n",
    "    top250count.append(countWord)\n",
    "    top250avg.append(countWord/wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab['top250AllNA'] = top250count\n",
    "Vocab['top250avgAllNA'] = top250avg\n",
    "Vocab['top250avgNotAllNA'] = 1-Vocab['top250avgAllNA']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Stereotype Bi-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allQueries = list(set(allQueries))\n",
    "allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['class'] == 1]\n",
    "sID = allSessionsQ['sID'].unique()\n",
    "corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['sID'].isin(corpus)]\n",
    "queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "queries2 = []\n",
    "for query in queries:\n",
    "    queries2.append(query.lower())\n",
    "queries = queries2\n",
    "\n",
    "bigrams = [b for l in queries for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "\n",
    "fdist1 = nltk.FreqDist(bigrams)\n",
    "\n",
    "top50 = []\n",
    "\n",
    "for x in fdist1.most_common(10):\n",
    "     top50.append(x[0])\n",
    "        \n",
    "top50count = []\n",
    "top50avg = []\n",
    "\n",
    "for query in allQueries:\n",
    "    vocab = []\n",
    "    countWord = 0\n",
    "    wordCount = 0\n",
    "    query = query.lower()\n",
    "    query = query.split(\" \")\n",
    "    split = nltk.bigrams(query)\n",
    "    for word in split:\n",
    "        wordCount +=1\n",
    "        if word in top50:\n",
    "            countWord +=1\n",
    "        else:\n",
    "            pass\n",
    "    top50count.append(countWord)\n",
    "    if wordCount > 0:\n",
    "        top50avg.append(countWord/wordCount)\n",
    "    else:\n",
    "        top50avg.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab['top50bi'] = top50count\n",
    "Vocab['top50biAvg'] = top50avg\n",
    "Vocab['top50biNot'] = 1-Vocab['top50biAvg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Stereotype Bi-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allQueries = list(set(allQueries))\n",
    "allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['class'] == 0]\n",
    "sID = allSessionsQ['sID'].unique()\n",
    "corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['sID'].isin(corpus)]\n",
    "queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "queries2 = []\n",
    "for query in queries:\n",
    "    queries2.append(query.lower())\n",
    "queries = queries2\n",
    "\n",
    "bigrams = [b for l in queries for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "\n",
    "fdist1 = nltk.FreqDist(bigrams)\n",
    "\n",
    "top50 = []\n",
    "\n",
    "for x in fdist1.most_common(10):\n",
    "     top50.append(x[0])\n",
    "        \n",
    "top50count = []\n",
    "top50avg = []\n",
    "\n",
    "for query in allQueries:\n",
    "    vocab = []\n",
    "    countWord = 0\n",
    "    wordCount = 0\n",
    "    query = query.lower()\n",
    "    query = query.split(\" \")\n",
    "    split = nltk.bigrams(query)\n",
    "    for word in split:\n",
    "        wordCount +=1\n",
    "        if word in top50:\n",
    "            countWord +=1\n",
    "        else:\n",
    "            pass\n",
    "    top50count.append(countWord)\n",
    "    if wordCount > 0:\n",
    "        top50avg.append(countWord/wordCount)\n",
    "    else:\n",
    "        top50avg.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab['top50biNA'] = top50count\n",
    "Vocab['top50biavgNA'] = top50avg\n",
    "Vocab['top50biNotNA'] = 1-Vocab['top50biavgNA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:27: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allQueries = list(set(allQueries))\n",
    "allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "sID = allSessionsQ['sID'].unique()\n",
    "corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['sID'].isin(corpus)]\n",
    "queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "text = ''\n",
    "for query in queries:\n",
    "    text += query + \" \"\n",
    "    \n",
    "stopwords = stopwords.words()\n",
    "querywords = text.split()\n",
    "\n",
    "resultwords  = [word for word in querywords if word.lower() not in stopwords]\n",
    "text = ' '.join(resultwords)\n",
    "\n",
    "queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vector = vectorizer.fit(queries)\n",
    "vectors = vector.transform(allQueries)\n",
    "\n",
    "listTFIDF = []\n",
    "for m in vectors:\n",
    "    listTFIDF.append(m.sum() / m.count_nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "VocabTFIDFAll = pd.DataFrame(data=listTFIDF, columns = ['tfidfAll']).fillna(-1)\n",
    "VocabTFIDFAll['query'] = allQueries\n",
    "Vocab = Vocab.merge(VocabTFIDFAll, on = 'query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Stereotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:27: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allQueries = list(set(allQueries))\n",
    "allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['class'] == 1]\n",
    "sID = allSessionsQ['sID'].unique()\n",
    "corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['sID'].isin(corpus)]\n",
    "queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "text = ''\n",
    "for query in queries:\n",
    "    text += query + \" \"\n",
    "    \n",
    "querywords = text.split()\n",
    "\n",
    "resultwords  = [word for word in querywords if word.lower() not in stopwords]\n",
    "text = ' '.join(resultwords)\n",
    "\n",
    "queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vector = vectorizer.fit(queries)\n",
    "vectors = vector.transform(allQueries)\n",
    "\n",
    "listTFIDF = []\n",
    "for m in vectors:\n",
    "    listTFIDF.append(m.sum() / m.count_nonzero())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "VocabTFIDF = pd.DataFrame(data=listTFIDF, columns = ['tfidf']).fillna(-1)\n",
    "VocabTFIDF['query'] = allQueries\n",
    "Vocab = Vocab.merge(VocabTFIDF, on = 'query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Non-Stereotype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:27: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allQueries = list(set(allQueries))\n",
    "allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['class'] == 0]\n",
    "sID = allSessionsQ['sID'].unique()\n",
    "corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['sID'].isin(corpus)]\n",
    "queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "text = ''\n",
    "for query in queries:\n",
    "    text += query + \" \"\n",
    "    \n",
    "querywords = text.split()\n",
    "\n",
    "resultwords  = [word for word in querywords if word.lower() not in stopwords]\n",
    "text = ' '.join(resultwords)\n",
    "\n",
    "queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vector = vectorizer.fit(queries)\n",
    "vectors = vector.transform(allQueries)\n",
    "\n",
    "listTFIDF = []\n",
    "for m in vectors:\n",
    "    listTFIDF.append(m.sum() / m.count_nonzero())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "VocabTFIDFNA = pd.DataFrame(data=listTFIDF, columns = ['tfidfNA']).fillna(-1)\n",
    "VocabTFIDFNA['query'] = allQueries\n",
    "Vocab = Vocab.merge(VocabTFIDFNA, on = 'query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = []\n",
    "st = WordNetLemmatizer()\n",
    "with open('DataSets/stopwords.txt') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        if (row):\n",
    "            stopWords.append(st.lemmatize(row[0]))\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopCount = []\n",
    "stopAverage= []\n",
    "st = WordNetLemmatizer()\n",
    "import re\n",
    "\n",
    "\n",
    "for query in allQueries:\n",
    "    count = 0\n",
    "    for word in query.split(' '):\n",
    "        word = word.lower().strip()\n",
    "        word = re.sub(r'[^\\w\\s]','',word)\n",
    "        word = st.lemmatize(word)\n",
    "        if word in stopWords:\n",
    "            count +=1\n",
    "        else:\n",
    "            pass\n",
    "    stopCount.append(count)\n",
    "    stopAverage.append(count/len(query.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "VocabStop = pd.DataFrame(data=stopCount, columns = ['stopCount'])\n",
    "VocabStop['query'] = allQueries\n",
    "Vocab = Vocab.merge(VocabStop, on = 'query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "www = []\n",
    "com = []\n",
    "net = []\n",
    "org = []\n",
    "gov = []\n",
    "edu = []\n",
    "http = []\n",
    "\n",
    "for query in allQueries:\n",
    "\n",
    "\n",
    "\n",
    "    if \"www.\" in query:\n",
    "        www.append(1)\n",
    "    else:\n",
    "        www.append(0)\n",
    "        \n",
    "    if \".com\" in query:\n",
    "        com.append(1)\n",
    "    else:\n",
    "        com.append(0)\n",
    "                \n",
    "    if \".net\" in query:\n",
    "        net.append(1)\n",
    "    else:\n",
    "        net.append(0)\n",
    "            \n",
    "    if \".org\" in query:\n",
    "        org.append(1)\n",
    "    else:\n",
    "        org.append(0)\n",
    "            \n",
    "    if \".edu\" in query:\n",
    "        edu.append(1)\n",
    "    else:\n",
    "        edu.append(0)\n",
    "            \n",
    "    if \".gov\" in query:\n",
    "        gov.append(1)\n",
    "    else:\n",
    "        gov.append(0)\n",
    "            \n",
    "    if \"http\" in query:\n",
    "        http.append(1)\n",
    "    else:\n",
    "        http.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "VocabNet = pd.DataFrame(data=com, columns = ['com'])\n",
    "VocabNet['net'] = net\n",
    "VocabNet['org'] = org\n",
    "VocabNet['edu'] = edu\n",
    "VocabNet['gov'] = gov\n",
    "VocabNet['http'] = http\n",
    "VocabNet['query'] = allQueries\n",
    "Vocab = Vocab.merge(VocabNet, on = 'query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pest control\" how to\n",
      "define:\"cosmetic laser treatment*\"\n",
      "long term care insurance \"united kingdom\" OR britain OR england\n",
      "long term care insurance \"united kingdom\" OR britain OR england\n",
      "\"what's good about internet phoning\"\n",
      "\"pest control\" courses\n",
      "Isabelle \"face transplants\"\n",
      "RHEUMATOLOGY SPORTS\n",
      "us law prohibiting \"redbull\"\n",
      "\"Pseudocyesis advances\"\n",
      "won NCAA \"march madness\" 2012\n",
      "research culture OR race infant OR child development stages OR milestones\n",
      "\"jasper fforde\" chronology\n",
      "\"skagit valley herald\" founded\n",
      "infant OR child development \"cultural differences\"\n",
      "infant OR child development \"cultural differences\"\n",
      "\"world snooker\" tournaments\n",
      "\"connecticut fire academy\"\n",
      "infant development \"cultural effects\"\n",
      "NCAA tournament \"march madness\" wikipedia\n",
      "first \"full face transplant\" Oscar\n",
      "\"Charles Basil\"\n",
      "\"non-extinct\" marsupials\n",
      "hawaii real estate family resale value house OR condo news\n",
      "\"SUNY albany hospital\" location\n",
      "\"cosmetic laser treatment*\" and safe*\n",
      "\"jeopardy game show\"\n",
      "Red Bull AND health\n",
      "infant development milestone \"cultural competence\n",
      "infant OR child development \"effect of culture\"\n",
      "infant OR child development \"effect of culture\"\n",
      "\"cosmetic laser treatment*\" and efficac*\n",
      "\"Richard Norris\"\n",
      "infant OR child \"development milestones\" site:ac.uk\n",
      "infant OR child \"development milestones\" site:ac.uk\n",
      "\"skagit valley herald\" circulation\n",
      "\"Connecticut Fire Academy\" fireman\n",
      "babies development \"cultural differences\"\n",
      "2005 \"world snooker\" championship\n",
      "derivation of \"nickname\"\n",
      "\"Martin Bryant\" australia\n",
      "JP Morgan Chase \"data centers\"\n",
      "\"water turbines\"\n",
      "'EUROPEAN UNION\" \"ECONOMIC CRISIS\" portugal\n",
      "\"skagit valley herald\"\n",
      "location \"JP Morgan Chase\" + center\n",
      "infant development milestone \"cultural differences\"\n",
      "\"skagit county\"\n",
      "'EUROPEAN UNION\" \"ECONOMIC CRISIS\"\n",
      "\"world snooker\" \"tournaments annually\"\n",
      "\"russian politics\" kursk submarine\n",
      "hawaii real estate family resale OR foreclosure news\n",
      "\"churchill downs\" horse racing park\n",
      "NCAA tournament \"march madness\" 2012 championship runner-up\n",
      "(euro OR eurozone) crisis\n",
      "\"world snooker\" \"reigning superstar\"\n",
      "\"Nicknames\" what are\n",
      "NCAA \"march madness\" 2012 results\n",
      "\"space sounds like\" astronauts\n",
      "What son has \"clapalong if you feel\" in it?\n",
      "\"event planning\" studying college\n",
      "\"march madness\" NCAA\n",
      "\"Connie Culp\"\n",
      "\"skagit valley\" demographics\n",
      "\"State university of new york\" bioinformatics\n",
      "\"Connie Culp\" wikipedia\n",
      "tax on \"junk food\"\n",
      "\"cosmetic laser treatment*\" and effective*\n",
      "\"cell phone plan\"\n",
      "\"water turnbines\"\n",
      "\"Connecticut Fire Academy\" training\n",
      "+(Black OR african-american) history month events\n",
      "'EUROPEAN UNION\" \"ECONOMIC CRISIS\" greece\n",
      "location \"JP Morgan Chase\" \"data centers\"\n",
      "Show me the movie called \"The Martian\"\n",
      "\"hospital anxiety and depression scale\" scales references\n",
      "\"data centers\"\n",
      "NCAA tournament \"march madness\"\n",
      "\"connecticut fire academy\" fireman\n",
      "What is the movie \"The Secret life of pets\"\n",
      "tax on \"junk food\" good idea\n",
      "\"collagen vascular disease\" sport\n",
      "\"history of face transplants\"\n",
      "What songs has a name with the lyrics of \"You use to call my on my self phone\"\n",
      "hawaii real estate family resale OR foreclosure\n",
      "\"2005 world snooker championship\"\n",
      "\"Akira Kurosawa\" indian cinema\n",
      "\"developmental milestones in east asia\"\n",
      "infant OR child development milestones research\n",
      "\"march madness\"\n",
      "first \"face transplant\" surgery in the world\n",
      "\"jasper fforde\" questions answers contents\n",
      "\"churchill downs\" horse racing schedule\n",
      "First \"Face Transplant\"\n",
      "\"Guinness brewmaster\"  2006\n",
      "culture OR race infant OR child development stages OR milestones\n",
      "\"connecticut fire academy\" wiki\n",
      "\"2005 world snooker championships\"\n",
      "\"skagit valley\" noted\n",
      "\"marsupial manure\"\n",
      "\"Isabelle Dinoire\"\n",
      "location \"JP Morgan Chase\" +\"data centers\"\n",
      "\"skagit valley herald\" created\n",
      "NCAA \"march madness\" 2012 broadcast\n",
      "\"greenland high school\" new hampshire\n",
      "infant OR child development intitle:culture\n",
      "infant OR child development milestones\n",
      "define: \"long-term care insurance\"\n",
      "infant OR child \"development milestones\" +research\n",
      "infant OR child \"development milestones\" +research\n"
     ]
    }
   ],
   "source": [
    "AND = []\n",
    "OR = []\n",
    "quotes = []\n",
    "for query in allQueries:\n",
    "    \n",
    "    if \"AND\" in query:\n",
    "        AND.append(1)\n",
    "    else:\n",
    "        AND.append(0)\n",
    "        \n",
    "    if \"OR\" in query:\n",
    "        OR.append(1)\n",
    "    else:\n",
    "        OR.append(0)\n",
    "        \n",
    "    if \"\\\"\" in query:\n",
    "        quotes.append(1) \n",
    "    else:\n",
    "        quotes.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "VocabOP = pd.DataFrame(data=AND, columns = ['AND'])\n",
    "VocabOP['OR'] = OR\n",
    "VocabOP['quotes'] = quotes\n",
    "VocabOP['query'] = allQueries\n",
    "Vocab = Vocab.merge(VocabOP, on = 'query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interogatives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = []\n",
    "VocabInter = pd.DataFrame(data=vocab, columns = ['coreVocab'])\n",
    "\n",
    "x = len(allQueries)\n",
    "for num in range(x):\n",
    "    query = allQueries[num]\n",
    "\n",
    "    if re.match(r\"who( |'re|'s|s)\", query, flags=re.IGNORECASE):\n",
    "        inter.append(1)\n",
    "\n",
    "    elif re.match(r\"what( |'re|re|'s|s)\", query, flags=re.IGNORECASE):\n",
    "        inter.append(1)\n",
    "\n",
    "    elif re.match(r\"when( |'re|re|'s|s)\", query, flags=re.IGNORECASE):\n",
    "        inter.append(1)\n",
    "\n",
    "    elif re.match(r\"where( |'re|re|'s|s)\", query, flags=re.IGNORECASE):\n",
    "        inter.append(1)\n",
    "   \n",
    "    elif re.match(r\"why( |'re|re|'s|s)\", query, flags=re.IGNORECASE):\n",
    "        inter.append(1)\n",
    "\n",
    "    elif re.match(r\"how( |'re|re|'s|s)\", query, flags=re.IGNORECASE):\n",
    "        inter.append(1)\n",
    "        \n",
    "    elif re.match(r\"is \", query, flags=re.IGNORECASE):\n",
    "        inter.append(1)\n",
    "    elif re.match(r\"are \", query, flags=re.IGNORECASE):\n",
    "        inter.append(1)\n",
    "    elif re.match(r\"can \", query, flags=re.IGNORECASE):\n",
    "        inter.append(1)\n",
    "\n",
    "    elif re.match(r\"could \", query, flags=re.IGNORECASE):\n",
    "        inter.append(1)\n",
    "\n",
    "    elif re.match(r\"should \", query, flags=re.IGNORECASE):\n",
    "        inter.append(1)\n",
    "\n",
    "    elif re.match(r\"would \", query, flags=re.IGNORECASE):\n",
    "        inter.append(1)\n",
    "\n",
    "    else:\n",
    "        inter.append(0)\n",
    "\n",
    "VocabInter = pd.DataFrame(data=inter, columns = ['inter'])\n",
    "VocabInter['query'] = allQueries\n",
    "Vocab = Vocab.merge(VocabInter, on = 'query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Vocab, open( \"Pickles/VocabFeat.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
