{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook extracts syntatical features from the queries found in SWC and SQS, returning a data frame containing those features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import stanza \n",
    "import pickle\n",
    "import textstat\n",
    "import nltk\n",
    "import subprocess\n",
    "import shlex\n",
    "import time\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "from subprocess import Popen, PIPE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allSessionsSQS = list(pickle.load( open( \"../Data/DataSets/SQS/SQS.p\", \"rb\" ) ))\n",
    "allQueries = allSessions['query'].tolist() \n",
    "allQueries = allQueries + list(allSessionsSQS)\n",
    "setQueries = set(allQueries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract D-Level Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70621/70621 [11:40:52<00:00,  1.68it/s]  \n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "input_file = 'DLA/data/lemmatize_pos_sentences.tagged'\n",
    "loc_file =  '../../data/lemmatize_pos_sentences.tagged'\n",
    "\n",
    "processor_dict = {\n",
    "    'tokenize': 'gsd',\n",
    "    'pos': 'bnc',\n",
    "    'lemma': 'default'\n",
    "}\n",
    "\n",
    "nlp = stanza.Pipeline('en', processors=processor_dict)\n",
    "\n",
    "from tqdm import tqdm\n",
    "with tqdm(total = len(setQueries) ) as pbar:\n",
    "    for text in setQueries:\n",
    "        doc = nlp(text)\n",
    "        out = open(input_file, 'w')\n",
    "        \n",
    "        for sentence in doc.sentences:\n",
    "            s = ''\n",
    "            l = 0\n",
    "            for word in sentence.words:\n",
    "                s+='{} {}'.format(word.lemma, word.xpos) + ' ' # needs to be xpos so it uses Penn Treebank\n",
    "                l+=1\n",
    "            out.write('{} {}\\n'.format(l, s.strip()))\n",
    "        out.close()\n",
    "        \n",
    "        cmd = 'cd DLA/d-level-analyzer/COLLINS-PARSER;'\n",
    "        cmd += ' code/parser {} models/model2/grammar 10000 1 1 1 1 > ../../data/parsed.m2;'.format(loc_file)\n",
    "        cmd += 'cd ..;'\n",
    "        cmd += 'python d-level.py ../data/parsed.m2 > ../data/dlevel.dla;'\n",
    "        proc = subprocess.Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True).wait()\n",
    "        if count == 0:\n",
    "            dl = pd.read_csv('DLA/data/dlevel.dla')\n",
    "            dl['query'] = text\n",
    "            dLevel = dl\n",
    "            count += 1\n",
    "        else:\n",
    "            dl = pd.read_csv('DLA/data/dlevel.dla')\n",
    "            dl['query'] = text\n",
    "            dLevel = dLevel.append(dl)\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Part of Speech Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "posData = []\n",
    "for document in setQueries:\n",
    "    text = nltk.word_tokenize(document)\n",
    "    tags = np.array(nltk.pos_tag(text)).flatten()\n",
    "    posData.append(tags[1::2])\n",
    "\n",
    "posMod = []\n",
    "\n",
    "for pos in posData: \n",
    "    string = []\n",
    "    for entry in pos:\n",
    "        string += str(entry) + \" \"\n",
    "    posMod.append(\"\".join(string))\n",
    "\n",
    "    \n",
    "posUni = []\n",
    "posBi = []\n",
    "posTri = []\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generate_ngrams(document,1)\n",
    "    posUni.append(doc)\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generate_ngrams(document,2)\n",
    "    posBi.append(doc)\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generate_ngrams(document,3)\n",
    "    posTri.append(doc)  \n",
    "    \n",
    "posDF = pd.DataFrame(setQueries)\n",
    "\n",
    "posDF['all'] = posMod\n",
    "posDF['uniPos'] = posUni\n",
    "posDF['biPos'] = posBi\n",
    "posDF['triPos']= posTri\n",
    "posDF = posDF.rename(columns={0: \"query\"})\n",
    "\n",
    "allSessionsuni = pd.concat([posDF,pd.get_dummies(posDF['uniPos'].apply(pd.Series).stack()).sum(level=0)],axis=1).drop(['uniPos', 'all', 'biPos', 'triPos'],axis=1)\n",
    "allSessionsbi = pd.concat([posDF,pd.get_dummies(posDF['biPos'].apply(pd.Series).stack()).sum(level=0)],axis=1).drop(['biPos', 'uniPos', 'all', 'triPos'],axis=1)\n",
    "allSessionstri = pd.concat([posDF,pd.get_dummies(posDF['triPos'].apply(pd.Series).stack()).sum(level=0)],axis=1).drop(['uniPos', 'all', 'biPos', 'triPos'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSessionsbiLanding = allSessionsbi[[\n",
    "'nn nn',\n",
    "'jj nn',\n",
    "'nn nns',\n",
    "'to vb',\n",
    "'jj nns',\n",
    "'jj to',\n",
    "'nn in',\n",
    "'nns in',\n",
    "'in nn',\n",
    "'dt nn',\n",
    "'query']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSessionstriLanding = allSessionstri[[\n",
    "'jj nn nn',\n",
    "'nn nn nn',\n",
    "'jj to vb',\n",
    "'nn nn nns',\n",
    "'to vb nn',\n",
    "'query']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "synFeats = allSessionsuni.merge(allSessionsbiLanding)\n",
    "synFeats = synFeats.merge(allSessionstriLanding)\n",
    "synFeats = synFeats.merge(allSessionstriLanding)\n",
    "synFeats['length'] = synFeats['query'].str.split().str.len()\n",
    "\n",
    "for col in listCols:\n",
    "    synFeats[col] = synFeats[col]/synFeats['length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "synFeats = synFeats.merge(lexComp, on = 'query')\n",
    "synFeats.drop(columns = [' Sentences', 'length'], inplace = True)\n",
    "pickle.dump(synFeats, open( \"Pickles/SynFeat.p\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
