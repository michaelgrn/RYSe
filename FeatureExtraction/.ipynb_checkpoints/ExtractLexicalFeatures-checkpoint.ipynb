{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import stanza # Stanford's stanza package\n",
    "#stanza.download('en') # run this once\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Lexical Complexity\n",
    "\n",
    "Code taken from:\n",
    "\n",
    "This code is the lexical complexity analyzer described in\n",
    "\n",
    "Lu, Xiaofei (2012). The relationship of lexical richnes to the quality \n",
    "of ESL speakers' oral narratives. The Modern Language Journal, 96(2), 190-208. \n",
    "\n",
    "Version 1.1 Released on February 12, 2013\n",
    "\n",
    "Which can be found at:\n",
    "\n",
    "http://www.personal.psu.edu/xxl13/download.html\n",
    "\n",
    "It has been modified to work with search queries, as it was initially designed for sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string,re,sys,os,random\n",
    "from math import sqrt,log\n",
    "\n",
    "# adjust minimum sample size here\n",
    "standard=50\n",
    "\n",
    "# Returns the keys of dictionary d sorted by their values\n",
    "def sort_by_value(d):\n",
    "    items=d.items()\n",
    "    backitems=[ [v[1],v[0]] for v in items]\n",
    "    backitems.sort()\n",
    "    return [ backitems[i][1] for i in range(0,len(backitems))]\n",
    "\n",
    "# NDW for first z words in a sample\n",
    "def getndwfirstz(z,lemmalist):\n",
    "    ndwfirstztype={}\n",
    "    for lemma in lemmalist[:z]:\n",
    "        ndwfirstztype[lemma]=1\n",
    "    return len(ndwfirstztype.keys())\n",
    "\n",
    "# NDW expected random z words, 10 trials\n",
    "def getndwerz(z,lemmalist):\n",
    "    ndwerz=0\n",
    "    for i in range(10):\n",
    "        ndwerztype={}\n",
    "        erzlemmalist=random.sample(lemmalist,z)\n",
    "        for lemma in erzlemmalist:\n",
    "            ndwerztype[lemma]=1\n",
    "        ndwerz+=len(ndwerztype.keys())\n",
    "    return ndwerz/10.0\n",
    "\n",
    "# NDW expected random sequences of z words, 10 trials\n",
    "def getndwesz(z,lemmalist):\n",
    "    ndwesz=0\n",
    "    for i in range(10):\n",
    "        ndwesztype={}\n",
    "        startword=random.randint(0,len(lemmalist)-z)\n",
    "        eszlemmalist=lemmalist[startword:startword+z]\n",
    "        for lemma in eszlemmalist:\n",
    "            ndwesztype[lemma]=1\n",
    "        ndwesz+=len(ndwesztype.keys())\n",
    "    return ndwesz/10.0\n",
    "\n",
    "# MSTTR\n",
    "def getmsttr(z,lemmalist):\n",
    "    samples=0\n",
    "    msttr=0.0\n",
    "    while len(lemmalist)>=z:\n",
    "        samples+=1\n",
    "        msttrtype={}\n",
    "        for lemma in lemmalist[:z]:\n",
    "            msttrtype[lemma]=1\n",
    "        msttr+=len(msttrtype.keys())/float(z)\n",
    "        lemmalist=lemmalist[z:]    \n",
    "    return msttr/samples\n",
    "\n",
    "def isLetterNumber(character):\n",
    "    if character in string.printable and not character in string.punctuation:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def isSentence(line):\n",
    "    for character in line:\n",
    "        if isLetterNumber(character):\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLex(queries):\n",
    "    processor_dict = {\n",
    "    'tokenize': 'gsd', \n",
    "    'pos': 'bnc', \n",
    "    'lemma': 'default'\n",
    "    }\n",
    "\n",
    "    nlp = stanza.Pipeline('en', processors=processor_dict)\n",
    "    # reads information from bnc wordlist\n",
    "    lexFeat = []\n",
    "    adjdict={}\n",
    "    verbdict={}\n",
    "    noundict={}\n",
    "    worddict={}\n",
    "    wordlistfile=open(\"DataSets/bnc_all_filtered.txt\",\"r\")\n",
    "    wordlist=wordlistfile.readlines()\n",
    "    wordlistfile.close()\n",
    "    for word in wordlist:\n",
    "        wordinfo=word.strip()\n",
    "        if not wordinfo or \"Total words\" in wordinfo:\n",
    "            continue\n",
    "        infolist=wordinfo.split()\n",
    "        lemma=infolist[0]\n",
    "        pos=infolist[1]\n",
    "        frequency=int(infolist[2])\n",
    "        worddict[lemma]=worddict.get(lemma,0)+frequency\n",
    "        if pos==\"Adj\":\n",
    "            adjdict[lemma]=adjdict.get(lemma,0)+frequency\n",
    "        elif pos==\"Verb\":\n",
    "            verbdict[lemma]=verbdict.get(lemma,0)+frequency\n",
    "        elif pos==\"NoC\" or pos==\"NoP\":\n",
    "            noundict[lemma]=noundict.get(lemma,0)+frequency\n",
    "    wordranks=sort_by_value(worddict)\n",
    "    verbranks=sort_by_value(verbdict)\n",
    "    length = len(queries)\n",
    "    with tqdm(total = length) as pbar:\n",
    "        for query in queries:\n",
    "            filename=query\n",
    "            doc = nlp(query)\n",
    "            for sentence in doc.sentences:\n",
    "                s = ''\n",
    "                for word in sentence.words:\n",
    "                    s+='{}_{}'.format(word.lemma, word.xpos) + ' '\n",
    "            lemlines= s\n",
    "            #print(lemlines)\n",
    "            # process input file\n",
    "            wordtypes={}\n",
    "            wordtokens=0\n",
    "            swordtypes={}\n",
    "            swordtokens=0\n",
    "            lextypes={}\n",
    "            lextokens=0\n",
    "            slextypes={}\n",
    "            slextokens=0\n",
    "            verbtypes={}\n",
    "            verbtokens=0\n",
    "            sverbtypes={}\n",
    "            adjtypes={}\n",
    "            adjtokens=0\n",
    "            advtypes={}\n",
    "            advtokens=0\n",
    "            nountypes={}\n",
    "            nountokens=0\n",
    "            lemmaposlist=[]\n",
    "            lemmalist=[]\n",
    "\n",
    "            for lemline in lemlines:\n",
    "                lemline=lemline.strip()\n",
    "                lemline=lemline.lower()\n",
    "                if not isSentence(lemline):\n",
    "                    continue\n",
    "                lemmas=lemline.split()\n",
    "                for lemma in lemmas:\n",
    "                    word=lemma.split(\"_\")[0]\n",
    "                    pos=lemma.split(\"_\")[-1]\n",
    "                    if (not pos in string.punctuation) and pos!=\"sent\" and pos!=\"sym\":\n",
    "                        lemmaposlist.append(lemma)\n",
    "                        lemmalist.append(word)  \n",
    "                        wordtokens+=1\n",
    "                        wordtypes[word]=1\n",
    "                        try:\n",
    "\n",
    "                            if (not word in wordranks[-2000:]) and pos != \"cd\":\n",
    "                                swordtypes[word]=1\n",
    "                                swordtokens+=1\n",
    "                            if pos[0]==\"n\":\n",
    "                                lextypes[word]=1\n",
    "                                nountypes[word]=1\n",
    "                                lextokens+=1\n",
    "                                nountokens+=1\n",
    "                                if not word in wordranks[-2000:]:\n",
    "                                    slextypes[word]=1\n",
    "                                    slextokens+=1\n",
    "                            elif pos[0]==\"j\":\n",
    "                                lextypes[word]=1\n",
    "                                adjtypes[word]=1\n",
    "                                lextokens+=1\n",
    "                                adjtokens+=1\n",
    "                                if not word in wordranks[-2000:]:\n",
    "                                    slextypes[word]=1\n",
    "                                    slextokens+=1\n",
    "                            elif pos[0]==\"r\" and (adjdict.has_key(word) or (word[-2:]==\"ly\" and adjdict.has_key(word[:-2]))):\n",
    "                                lextypes[word]=1\n",
    "                                advtypes[word]=1\n",
    "                                lextokens+=1\n",
    "                                advtokens+=1\n",
    "                                if not word in wordranks[-2000:]:\n",
    "                                    slextypes[word]=1\n",
    "                                    slextokens+=1\n",
    "                            elif pos[0]==\"v\" and not word in [\"be\",\"have\"]:\n",
    "                                verbtypes[word]=1\n",
    "                                verbtokens+=1\n",
    "                                lextypes[word]=1\n",
    "                                lextokens+=1\n",
    "                                if not word in wordranks[-2000:]:\n",
    "                                    sverbtypes[word]=1\n",
    "                                    slextypes[word]=1\n",
    "                                    slextokens+=1\n",
    "                        except(AttributeError):\n",
    "                            pass\n",
    "\n",
    "            # 1. lexical density\n",
    "            if wordtokens > 0:\n",
    "                ld=float(lextokens)/wordtokens\n",
    "            else:\n",
    "                ld=0\n",
    "            # 2. lexical sophistication\n",
    "            # 2.1 lexical sophistication\n",
    "            if lextokens != 0:\n",
    "                ls1=slextokens/float(lextokens)\n",
    "            else:\n",
    "                ls1 = 0\n",
    "            if len(wordtypes.keys()) > 0:\n",
    "                ls2=len(swordtypes.keys())/float(len(wordtypes.keys()))\n",
    "            else:\n",
    "                ls2 = 0\n",
    "\n",
    "            # 2.2 verb sophistication\n",
    "            vs1 = 0\n",
    "            vs2=0\n",
    "            cvs1=0\n",
    "            if verbtokens > 0:\n",
    "                vs1=len(sverbtypes.keys())/float(verbtokens)\n",
    "                vs2=(len(sverbtypes.keys())*len(sverbtypes.keys()))/float(verbtokens)\n",
    "                cvs1=len(sverbtypes.keys())/sqrt(2*verbtokens)\n",
    "\n",
    "            # 3 lexical diversity or variation\n",
    "\n",
    "            # 3.1 NDW, may adjust the values of \"standard\"\n",
    "            ndw=len(wordtypes.keys())\n",
    "\n",
    "            # 3.2 TTR\n",
    "            \n",
    "            if wordtokens > 0:\n",
    "                ttr=len(wordtypes.keys())/float(wordtokens)\n",
    "                if len(lemmalist)>=standard:\n",
    "                    msttr=getmsttr(standard,lemmalist)\n",
    "                cttr=len(wordtypes.keys())/sqrt(2*wordtokens)\n",
    "                rttr=len(wordtypes.keys())/sqrt(wordtokens)\n",
    "            else:\n",
    "                ttr = 0\n",
    "                cttr = 0\n",
    "                rttr = 0\n",
    "            if wordtokens == 0 or len(wordtypes.keys()) == 0:\n",
    "                logttr = 0\n",
    "            else:\n",
    "                logttr=log(len(wordtypes.keys()))/log(wordtokens)\n",
    "            # 3.3 verb diversity\n",
    "            vv1, svv1, cvv1 = 0, 0, 0\n",
    "            if verbtokens > 0:\n",
    "                vv1=len(verbtypes.keys())/float(verbtokens)\n",
    "                svv1=len(verbtypes.keys())*len(verbtypes.keys())/float(verbtokens)\n",
    "                cvv1=len(verbtypes.keys())/sqrt(2*verbtokens)\n",
    "\n",
    "            # 3.4 lexical diversity\n",
    "            if lextokens != 0:\n",
    "                lv=len(lextypes.keys())/float(lextokens)\n",
    "                vv2=len(verbtypes.keys())/float(lextokens)\n",
    "                adjv=len(adjtypes.keys())/float(lextokens)\n",
    "\n",
    "            else:\n",
    "                lv=0\n",
    "                vv2=0\n",
    "                adjv=0\n",
    "\n",
    "            if nountokens != 0:\n",
    "                nv=len(nountypes.keys())/float(nountokens)\n",
    "            else:\n",
    "                nv=0\n",
    "\n",
    "\n",
    "\n",
    "          \n",
    "            lexFeat.append([query, ld, ls1, ls2, vs1, vs2, cvs1, ndw, ttr,\n",
    "                           cttr, rttr, logttr, lv, vv1, svv1, cvv1, vv2, nv, adjv])\n",
    "            pbar.update()\n",
    "    lexical = pd.DataFrame(data = lexFeat, columns = [\"query\", \"ld\", \"ls1\", \"ls2\", \"vs1\", \"vs2\", \"cvs1\", \"ndw\", \"ttr\",\n",
    "                                                      \"cttr\", \"rttr\", \"logttr\", \"lv\", \"vv1\", \"svv1\", \"cvv1\", \"vv2\", \"nv\", \"adjv\"])\n",
    "    return lexical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSessions = pickle.load( open( \"../../thesis/Data/Session/allSessionsProc.p\", \"rb\" ) )\n",
    "#allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allSessionsSQS = list(pickle.load( open( \"../Data/DataSets/SQS/SQS.p\", \"rb\" ) ))\n",
    "allQueries = allSessions['query'].tolist() \n",
    "allQueries = allQueries + list(allSessionsSQS)\n",
    "setQueries = set(allQueries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69826"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(allSessions['query'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/70114 [00:00<?, ?it/s]/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '3'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '8'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '6'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '2'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '0'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '4'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '1'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '9'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '5'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '7'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "  3%|▎         | 2035/70114 [00:00<00:16, 4184.34it/s]/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ó'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "  4%|▍         | 2642/70114 [00:00<00:21, 3167.86it/s]/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '·'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "  5%|▌         | 3664/70114 [00:01<00:33, 1961.57it/s]/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'á'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      " 47%|████▋     | 33190/70114 [01:49<04:06, 149.61it/s]/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'é'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      " 54%|█████▍    | 38020/70114 [02:25<05:32, 96.51it/s] /Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ö'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ü'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      " 56%|█████▌    | 39067/70114 [02:33<03:17, 157.29it/s]/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'î'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      " 59%|█████▉    | 41487/70114 [02:52<04:47, 99.74it/s] /Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ú'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      " 77%|███████▋  | 54001/70114 [04:56<02:45, 97.12it/s] /Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ê'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ç'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      " 82%|████████▏ | 57297/70114 [05:35<02:46, 76.85it/s] /Users/bl4z3/miniconda3/lib/python3.7/site-packages/nltk/tokenize/sonority_sequencing.py:104: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ñ'\n",
      "  \" assigning as vowel: '{}'\".format(c)\n",
      "100%|██████████| 70114/70114 [09:41<00:00, 120.54it/s]\n"
     ]
    }
   ],
   "source": [
    "#Lexical Characteristics\n",
    "\n",
    "totalSyl = []\n",
    "avgSyl = []\n",
    "simWords = []\n",
    "comWords = []\n",
    "simWordsAvg = []\n",
    "comWordsAvg = []\n",
    "mostSyl = []\n",
    "leastSyl = []\n",
    "SSP = SyllableTokenizer()\n",
    "\n",
    "with tqdm(total = len(setQueries) ) as pbar:\n",
    "    for text in setQueries:\n",
    "        running = 0\n",
    "        count = 0\n",
    "        simpleWords = 0\n",
    "        complexWords = 0\n",
    "        most = 0\n",
    "        least = 19\n",
    "        for word in text.split(\" \"):\n",
    "            current = len(SSP.tokenize(word))\n",
    "            running += current\n",
    "            count +=1\n",
    "            if current < 3:\n",
    "                simpleWords += 1\n",
    "            else:\n",
    "                complexWords +=1\n",
    "            if most < current:\n",
    "                most = current\n",
    "            if least > current:\n",
    "                least = current\n",
    "                \n",
    "        totalSyl.append(running)\n",
    "        avgSyl.append(running/count)\n",
    "        simWords.append(simpleWords)\n",
    "        comWords.append(complexWords)\n",
    "        mostSyl.append(most)\n",
    "        leastSyl.append(least)\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "textComplex = pd.DataFrame(setQueries)\n",
    "textComplex = textComplex.set_index(0, drop=True)\n",
    "textComplex = textComplex.reset_index().rename(columns={0:'query'})\n",
    "textComplex['totalSyl'] = totalSyl\n",
    "textComplex['avgSyl'] = avgSyl\n",
    "textComplex['simWords'] = simWords\n",
    "textComplex['comWords'] = comWords\n",
    "textComplex['greatestSyl'] = mostSyl\n",
    "textComplex['leastSyl'] = leastSyl\n",
    "textComplex['numChars'] = textComplex['query'].str.len()\n",
    "textComplex['numWords'] = textComplex['query'].str.split().str.len()\n",
    "textComplex['avgLenWord'] = textComplex['numChars']/textComplex['numWords']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-14 00:55:25 WARNING: Can not find tokenize: gsd from official model list. Ignoring it.\n",
      "2021-09-14 00:55:25 WARNING: Can not find pos: bnc from official model list. Ignoring it.\n",
      "2021-09-14 00:55:25 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-09-14 00:55:25 INFO: Use device: cpu\n",
      "2021-09-14 00:55:25 INFO: Loading: tokenize\n",
      "2021-09-14 00:55:25 INFO: Loading: pos\n",
      "2021-09-14 00:55:26 INFO: Loading: lemma\n",
      "2021-09-14 00:55:26 INFO: Loading: depparse\n",
      "2021-09-14 00:55:27 INFO: Loading: sentiment\n",
      "2021-09-14 00:55:28 INFO: Loading: ner\n",
      "2021-09-14 00:55:29 INFO: Done loading processors!\n",
      "100%|██████████| 70114/70114 [3:11:15<00:00,  6.11it/s]  \n"
     ]
    }
   ],
   "source": [
    "#Lexical Complexity\n",
    "\n",
    "lexFeats = getLex(setQueries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicalFeatures = textComplex.merge(lexFeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lexicalFeatures, open( \"Pickles/LexFeat.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
