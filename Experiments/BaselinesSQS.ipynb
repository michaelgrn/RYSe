{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "from sklearn import model_selection \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from nltk.util import ngrams\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowIndex(sentence):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    return lda_model[dictionary.doc2bow(tokens)] \n",
    "\n",
    "def generateNgrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSessions = pickle.load( open( \"../Data/DataSets/SQS/SQS.p\", \"rb\" ) )\n",
    "# allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "allSessionsA = pickle.load( open( \"../Data/DataSets/SQS/SQSA.p\", \"rb\" ) )\n",
    "allSessionsNA = pickle.load( open( \"../Data/DataSets/SQS/SQSNA.p\", \"rb\" ) )\n",
    "ovaResults = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How did paul die from fast in the fouris',\n",
       " 'Fire belly toad',\n",
       " 'Now im stressed out',\n",
       " 'fast in the fouris',\n",
       " 'How many phones sell in a day',\n",
       " 'How long do toads live',\n",
       " 'amozan',\n",
       " 'Fast and the ferious',\n",
       " \"Dominic's early life in fast an ferious\",\n",
       " 'what is a cheeta',\n",
       " 'The song at the end of the fast and ferious 7 movie',\n",
       " 'The Fast and ferious 7 movie',\n",
       " 'When is the Fast and ferious 9 coming in theaters',\n",
       " 'hew was the 2nd president',\n",
       " 'movies for to day at the edwads movie theater',\n",
       " 'Who is the voice of rupnnzel',\n",
       " 'tortoises',\n",
       " 'Hey I just met you',\n",
       " 'Jurassic world',\n",
       " 'How many langenges was Jurassic world made in',\n",
       " 'TTM',\n",
       " 'Google classroom',\n",
       " '23rd president',\n",
       " 'cute girls hairstyles',\n",
       " 'pintrest',\n",
       " 'Fantastic hair doos',\n",
       " 'Character from monster high',\n",
       " 'example of a leamer',\n",
       " 'taler swift',\n",
       " 'monster high move boo york boo york',\n",
       " 'why does the poler bear have white fur',\n",
       " 'Whoh played Ioin hide from transformers',\n",
       " 'how the chetta has spots',\n",
       " 'florider',\n",
       " 'tranksformers moviey',\n",
       " 'When is the new ghost busters comeing out',\n",
       " 'spelling lity',\n",
       " 'cool math',\n",
       " 'armer games',\n",
       " 'who plays esan in the movie',\n",
       " 'is sloth the laziest animal',\n",
       " 'selelena gomez songs',\n",
       " 'frozen full movie online',\n",
       " 'when is Selena gomez new songs are coming out',\n",
       " 'google classroom',\n",
       " 'How does black and whitee cats look like',\n",
       " 'Who palyed kyioren',\n",
       " 'Why do spiders have venom',\n",
       " 'star wars episode 7',\n",
       " 'When is the new finding nemo movie comieng out',\n",
       " 'Presedians',\n",
       " 'movies',\n",
       " 'shows',\n",
       " 'Highschool musical 2',\n",
       " 'Who was the actor for troy in highschool musical?',\n",
       " 'how long have lions been extirced',\n",
       " 'what song has these words in it',\n",
       " 'Highschool musical',\n",
       " 'how many miles is it to the moon?',\n",
       " 'weather',\n",
       " 'facrs',\n",
       " 'The actor of oh from home',\n",
       " 'Facts about pigs',\n",
       " 'whatsthatsong.net',\n",
       " 'Jurrasic wortl',\n",
       " 'When is Pokemon sun and moon coming out',\n",
       " 'Google docs',\n",
       " 'images for project',\n",
       " 'games',\n",
       " 'images for fun',\n",
       " 'Who plays baymax?',\n",
       " 'What is a bear?',\n",
       " 'Who is the name of a Macdre song?',\n",
       " 'Draft Day movie',\n",
       " 'oldest person ever',\n",
       " 'websites for facts',\n",
       " 'images',\n",
       " 'Alrin and the chickmuelcs  ',\n",
       " 'Why do flamingo get there feey on to other',\n",
       " 'Songs buy Charlie Puth',\n",
       " 'titanic full movie',\n",
       " 'how many four movies are there',\n",
       " 'i skarch of word work',\n",
       " 'Who plays Harry Potter',\n",
       " 'fact on tigers',\n",
       " 'Songs by talor swift',\n",
       " 'Harry potter sward in the stone',\n",
       " 'Who is the 30th president of the usa',\n",
       " 'weather',\n",
       " 'games',\n",
       " 'who was the huray in the moive the Larax?',\n",
       " 'Facts about dogs',\n",
       " 'why is the sun hot',\n",
       " 'Fast and the furiouse cast',\n",
       " 'monkeys',\n",
       " 'Deathly Hallows part 2',\n",
       " 'new finding nemo movie',\n",
       " 'give me information about a giraf',\n",
       " 'finding nemo movie',\n",
       " 'how do scintest know all of this knoledg',\n",
       " 'youtube videos',\n",
       " 'math problems',\n",
       " 'from superman',\n",
       " 'facts about tigers',\n",
       " 'new maclelamore song',\n",
       " 'Superman movie',\n",
       " 'how do seahorces swim',\n",
       " 'basketball cards',\n",
       " 'lighting rods',\n",
       " 'Actor of Hulk in Avengers',\n",
       " 'Info on foxes',\n",
       " 'G-Fazzy newst song',\n",
       " 'Avengers movie',\n",
       " \"Date of Jonh F Kennady's death\",\n",
       " 'TTM ',\n",
       " 'youtube',\n",
       " 'How is b-b-8 so real?',\n",
       " 'Wgat is so important about tigers',\n",
       " 'What is the weeknds most famous song',\n",
       " 'The new starwars',\n",
       " 'When is the new ghost busters coming out',\n",
       " 'Parts of speech',\n",
       " 'funny dog pictures',\n",
       " 'Who played sander in The Walking Dead',\n",
       " 'How long can a Draffs neck',\n",
       " \"I'm going home\",\n",
       " 'The walking dead',\n",
       " 'when is the next season of stanly supperre humans',\n",
       " 'youtube',\n",
       " 'String explosin',\n",
       " 'How did Anabell flote',\n",
       " 'Why are poler Bear white',\n",
       " \"When I'm your go to miss me\",\n",
       " 'Anabell',\n",
       " 'When is season 5 Vingis comeing out',\n",
       " 'YouTub',\n",
       " 'I like Insidous 1 2 and 3',\n",
       " 'What was Lucy (from chronicels of Narnia) age',\n",
       " 'Why do dogs wag their tail',\n",
       " 'Whats the song I cut my hangs with some rusty kitchen sissors',\n",
       " 'What the name of the movie based on a C.S Lewis book?',\n",
       " 'Is there a place in Africa named Kimberly',\n",
       " 'envirormental propums in missouri',\n",
       " 'When does the new m donlads goin to be buiolt',\n",
       " 'Star Wars who plays Kylo ren',\n",
       " 'Drake',\n",
       " 'When is star wars going to come out',\n",
       " 'How old can chewbacca get',\n",
       " 'How does a wolf catch its prey',\n",
       " 'what is love',\n",
       " 'Star Wars',\n",
       " 'When will be a full moon this year',\n",
       " 'Caculator',\n",
       " \"Assassin's Creed\",\n",
       " 'Who plays the good terminater in Terminater two',\n",
       " 'Information about tigers',\n",
       " 'When is the new Finding Dory coming out',\n",
       " 'how old is Han Solo in the new star wars movie',\n",
       " 'how many Star wars movies have Han solo in them?',\n",
       " 'How many cheetahs are in the world?',\n",
       " 'What son has \"clapalong if you feel\" in it?',\n",
       " 'Order kungfu panda',\n",
       " 'How many Star Wars movies will be made?',\n",
       " 'How do they make the chewey voice starwars',\n",
       " 'monkey info',\n",
       " 'mincraft songs',\n",
       " 'star wars ep. 5',\n",
       " 'How old is the wookie',\n",
       " 'mamoth facts',\n",
       " 'How old is TinkleBell',\n",
       " 'What is the movie called with a Doll named Anebelle?',\n",
       " 'When is finding Dory coming out?',\n",
       " 'How much are the Justin beiber tickets',\n",
       " 'Online calculator for math',\n",
       " 'What is the color of a leopard',\n",
       " \"Who's lukes mom?\",\n",
       " 'how old can dogs grow to be',\n",
       " 'starwars episode the force returns',\n",
       " 'when does far cry primal come out for xbox360',\n",
       " 'How old DJ is in Fuller House',\n",
       " 'Why are Polar bears white',\n",
       " 'Anibelle',\n",
       " 'When will they make more episodes of Fuller House',\n",
       " 'youtube',\n",
       " 'Who plays Kylo Ren from StarWars?',\n",
       " 'What are coll fact about a horse?',\n",
       " 'What songs has a name with the lyrics of \"You use to call my on my self phone\"',\n",
       " 'Star Wars Ep. 7',\n",
       " \"How do Feathre's grow\",\n",
       " 'How do lights work',\n",
       " 'Images of dragons',\n",
       " 'Fast and Furiou',\n",
       " 'Sloth',\n",
       " 'Fast and Furiouse',\n",
       " 'How old Star Wars is',\n",
       " 'Star wars',\n",
       " 'How old is Johny enlgish?',\n",
       " 'facts on Sabertooth tigers',\n",
       " 'Johny english reborn movie',\n",
       " 'new movies 2016',\n",
       " 'games',\n",
       " 'how fast is flash',\n",
       " 'why are grizzly Bares Brown',\n",
       " 'songs for 5th graders',\n",
       " 'Netflix',\n",
       " 'What do foxes eat',\n",
       " 'picts of dragons',\n",
       " 'The Incredibles',\n",
       " 'who voiced Mr.Incredible',\n",
       " 'how long are giraffes necks',\n",
       " 'the Incredibles',\n",
       " 'facts about Ufos',\n",
       " 'five night at freddys',\n",
       " 'who played Gorden',\n",
       " 'facts about lizards',\n",
       " 'Who plays Kylo Ren in the new Star wars',\n",
       " 'Facts about geckos',\n",
       " 'Star Wars the Force Awakens',\n",
       " 'When is the new Guardians of the Galaxy going to come out',\n",
       " 'Computer games',\n",
       " 'Sumdog Math games',\n",
       " 'Who plays the villain in Ant-man',\n",
       " 'How long is a lions Lifespan?',\n",
       " 'Ant-man movie',\n",
       " 'When is Batman v. Superman coming out?',\n",
       " 'Youtube',\n",
       " 'math games',\n",
       " 'How long can a polr bear swin under water?',\n",
       " 'When will the movie Zootopia come out',\n",
       " 'Pointless website',\n",
       " 'world war II pictures',\n",
       " 'Who plays the rabbit from the movie H.O.P.?',\n",
       " 'name of my Favourite movie',\n",
       " 'When is the new movie Race about Jessie Owens coming out on DVD',\n",
       " 'staggering bautey (a game)',\n",
       " 'Who plays in the new goose bumps?',\n",
       " 'How fast can a Cheeta run?',\n",
       " 'What is the new son by son and son?',\n",
       " 'Fuller House Full episodes',\n",
       " 'When is Fuller House coming out?',\n",
       " 'How to multiply a fraction by a fraction?',\n",
       " 'Krois? How dose dame not die in the last part',\n",
       " 'Sna',\n",
       " 'The lirycs of the song',\n",
       " 'the tittle of the movie',\n",
       " 'about April something',\n",
       " 'school games',\n",
       " 'cool cars',\n",
       " 'who plays as the man of steel',\n",
       " 'when will the new movie is coming out',\n",
       " 'my favourite movie',\n",
       " 'who sang a song called watch me whip nae',\n",
       " 'Batman vs superman ',\n",
       " 'the minions movie',\n",
       " 'information of words',\n",
       " 'sports and news',\n",
       " 'Who is the voice for Dory in Finding Nemo?',\n",
       " 'How long can a shark live for',\n",
       " 'Mama ohohoh song',\n",
       " 'Starwars',\n",
       " 'When is the next StarWars movie coming out',\n",
       " 'MIG games',\n",
       " 'google docs',\n",
       " 'How old are the kids now in Stand By Me?',\n",
       " 'What sound does a girrafe make?',\n",
       " \"Hello it's me\",\n",
       " 'movie Zootopia',\n",
       " 'How old is google?',\n",
       " 'facts information',\n",
       " 'youtube',\n",
       " 'How old is Ray from StarWars?',\n",
       " 'How many breeds of Dogs are there?',\n",
       " 'Songs by Adele',\n",
       " 'Spirited away',\n",
       " 'When is Zootopia coming out?',\n",
       " 'ideas for poems',\n",
       " 'games to play when you board',\n",
       " 'Billbow - Bagan in the Hobbit',\n",
       " 'A whole with a horn',\n",
       " 'What songs does Jeff play?',\n",
       " 'What is the movie \"The Secret life of pets\"',\n",
       " 'Who plays Nemo in finding Nemo?',\n",
       " 'How plays the Rafe on teenager mutan',\n",
       " 'Star wars',\n",
       " 'calculator',\n",
       " 'Who voices Mabel in Gravity Falls?',\n",
       " 'The Alpaca',\n",
       " 'Math games',\n",
       " 'Dan and Phil',\n",
       " 'Who plays Kylo Ren in the new Star wars movie?',\n",
       " 'How long can a wolf live?',\n",
       " 'What is the ame of the song with the lyrics ? Now watch me whip watch me nay nay!',\n",
       " 'start wars',\n",
       " 'When does summer break start?',\n",
       " 'What was the best video game at 2015?',\n",
       " 'What is the capitol of New York?',\n",
       " 'Who plays the bad guy in Star Wars the Horde awakends?',\n",
       " \"What is a fox's favorite kind of food?\",\n",
       " 'Show me the movie called \"The Martian\"',\n",
       " 'What is the biggest rock found on Mars?',\n",
       " 'What is the top game this week?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(allSessionsA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "newList = {}\n",
    "newList['entry']=[]\n",
    "newList['class']=[]\n",
    "\n",
    "for session in allSessionsA:\n",
    "    newList['entry'].append(session)\n",
    "    #newList['tag'].append('kids')\n",
    "    newList['class'].append(1)\n",
    "\n",
    "\n",
    "for session in allSessionsNA:\n",
    "    newList['entry'].append(session)\n",
    "    newList['class'].append(0)\n",
    "\n",
    "data = pd.DataFrame(newList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How did paul die from fast in the fouris</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fire belly toad</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Now im stressed out</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fast in the fouris</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many phones sell in a day</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>apply for a State University of New York college</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>supreme court on banning guns</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>philadelphia nyc distance</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>themed road trips</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>eurozone debt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1505 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 entry  class\n",
       "0             How did paul die from fast in the fouris      1\n",
       "1                                      Fire belly toad      1\n",
       "2                                  Now im stressed out      1\n",
       "3                                   fast in the fouris      1\n",
       "4                        How many phones sell in a day      1\n",
       "...                                                ...    ...\n",
       "1500  apply for a State University of New York college      0\n",
       "1501                     supreme court on banning guns      0\n",
       "1502                         philadelphia nyc distance      0\n",
       "1503                                 themed road trips      0\n",
       "1504                                     eurozone debt      0\n",
       "\n",
       "[1505 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSplits = 5\n",
    "kfold = KFold(n_splits=nSplits, random_state=20210530, shuffle=True)\n",
    "\n",
    "data['prediction'] = 0\n",
    "\n",
    "tnOva = 0\n",
    "fpOva = 0\n",
    "fnOva = 0\n",
    "tpOva = 0\n",
    "testAccOva = 0\n",
    "outputMaj = []\n",
    "trI = []\n",
    "tI = []\n",
    "for test_index,train_index in kfold.split(data):\n",
    "    trI.append(train_index)\n",
    "    tI.append(test_index)\n",
    "    test = data.iloc[test_index]\n",
    "    outputMaj.append(test['prediction'].tolist())\n",
    "    testAccOva +=  accuracy_score(test['class'], test['prediction'])\n",
    "    tn, fp, fn, tp = confusion_matrix( test['class'], test['prediction']).ravel()\n",
    "    tnOva += tn\n",
    "    fpOva += fp\n",
    "    fnOva += fn\n",
    "    tpOva += tp\n",
    "pickle.dump( outputMaj, open( \"Pickles/OutputMajoritySQS.p\", \"wb\" ) )\n",
    "majorityResults = ['Majority', round(testAccOva/nSplits,3), tnOva/nSplits, fpOva/nSplits, round(((tnOva/nSplits)/((tnOva/nSplits)+ (fpOva/nSplits))),3), fnOva/nSplits, tpOva/nSplits,round(((tpOva/nSplits)/((tpOva/nSplits)+ (fnOva/nSplits))),3),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Majority', 0.8, 963.2, 0.0, 1.0, 240.8, 0.0, 0.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majorityResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovaResults.append(majorityResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duarte Torres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b23e5dbb39ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0march\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"../Data/DataSets/SQS/SQSA.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnotArch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"../Data/DataSets/SQS/SQSNA.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "arch = pickle.load( open( \"../Data/DataSets/SQS/SQSA.p\", \"rb\" ) )\n",
    "notArch = pickle.load( open( \"../Data/DataSets/SQS/SQSNA.p\", \"rb\" ) )\n",
    "\n",
    "from datetime import datetime\n",
    "documents = []\n",
    "sessionDuration = []\n",
    "seshClass = []\n",
    "for session in arch:\n",
    "    #print(session)\n",
    "    sites = []\n",
    "    doc = \"\"\n",
    "    documents.append(sites)\n",
    "    length = 0\n",
    "    sessionDuration.append(length)\n",
    "    seshClass.append(1)\n",
    "for session in notArch:\n",
    "    #print(session)\n",
    "    sites = []\n",
    "    doc = \"\"\n",
    "    documents.append(sites)\n",
    "    length = 0\n",
    "    sessionDuration.append(length)\n",
    "    seshClass.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data=documents)\n",
    "data = data[data.columns[1:]].apply(\n",
    "    lambda x: ' '.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "data = pd.DataFrame(data, columns=['sites'])\n",
    "\n",
    "data['duration'] = sessionDuration\n",
    "data['class'] = seshClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sites</th>\n",
       "      <th>duration</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1505 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sites  duration  class\n",
       "0                  0      1\n",
       "1                  0      1\n",
       "2                  0      1\n",
       "3                  0      1\n",
       "4                  0      1\n",
       "...    ...       ...    ...\n",
       "1500               0      0\n",
       "1501               0      0\n",
       "1502               0      0\n",
       "1503               0      0\n",
       "1504               0      0\n",
       "\n",
       "[1505 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sites = data['sites'].tolist()\n",
    "Duration = data['duration'].tolist()\n",
    "\n",
    "durationClassification = []\n",
    "\n",
    "for sD in Duration:\n",
    "    try:\n",
    "        if float(sD) <= (60*30):\n",
    "            durationClassification.append(1)\n",
    "        else:\n",
    "            durationClassification.append(0)    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "kids_sites = []\n",
    "with open('../Data/DataSources/DMOZ/URL Classification.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    line_count = -1\n",
    "    for row in csv_reader:\n",
    "        \n",
    "        ## consume first line\n",
    "        if( 'Kids' in row[2]):\n",
    "            #print(row)\n",
    "            kids_sites.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1505/1505 [00:39<00:00, 38.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "siteClassification = []\n",
    "with tqdm(total=len(Sites)) as pbar:\n",
    "    for visitedSites in Sites:\n",
    "        found = 0\n",
    "        for kidsSite in kids_sites:\n",
    "            if kidsSite in visitedSites:\n",
    "                siteClassification.append(1)\n",
    "                found = 1\n",
    "                break\n",
    "        if found == 0:\n",
    "            siteClassification.append(0)\n",
    "        pbar.update()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['durationClassification'] = durationClassification\n",
    "data['siteClassification'] = siteClassification\n",
    "data['prediction'] = data['durationClassification'] & data['siteClassification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnOva = 0\n",
    "fpOva = 0\n",
    "fnOva = 0\n",
    "tpOva = 0\n",
    "testAccOva = 0\n",
    "outputDT = []\n",
    "for test_index, train_index in kfold.split(data):\n",
    "    test = data.iloc[test_index]\n",
    "    outputDT.append( test['prediction'].tolist())\n",
    "    testAccOva +=  accuracy_score(test['class'], test['prediction'])\n",
    "    tn, fp, fn, tp = confusion_matrix(test['class'], test['prediction']).ravel()\n",
    "    tnOva += tn\n",
    "    fpOva += fp\n",
    "    fnOva += fn\n",
    "    tpOva += tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( outputDT, open( \"Pickles/OutputRuleSQS.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruleResults = ['Rule', round(testAccOva/5,3), tnOva/nSplits, fpOva/nSplits, round(((tnOva/nSplits)/((tnOva/nSplits)+ (fpOva/nSplits))),3), fnOva/nSplits, tpOva/nSplits,round(((tpOva/nSplits)/((tpOva/nSplits)+ (fnOva/nSplits))),3),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rule', 0.8, 963.2, 0.0, 1.0, 240.8, 0.0, 0.0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruleResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovaResults.append(ruleResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSessions = pickle.load( open( \"Pickles/SWCNoTune.p\", \"rb\" ) )\n",
    "allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "allSessionsA = allSessionsQ.loc[allSessionsQ['class']==1].groupby('sID')['query'].apply(pd.Series.tolist).tolist()\n",
    "allSessionsNA = allSessionsQ.loc[allSessionsQ['class']==0].groupby('sID')['query'].apply(pd.Series.tolist).tolist()\n",
    "\n",
    "\n",
    "SQSA = np.asarray(pickle.load( open( \"../Data/DataSets/SQS/SQSA.p\", \"rb\" ) ))\n",
    "SQSNA = np.asarray(pickle.load( open( \"../Data/DataSets/SQS/SQSNA.p\", \"rb\" ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "newList = {}\n",
    "newList['entry']=[]\n",
    "newList['tag']=[]\n",
    "\n",
    "\n",
    "for session in allSessionsA:\n",
    "    string = \"\"\n",
    "    for query in session:\n",
    "        string += query + \" \"\n",
    "    newList['entry'].append(string)\n",
    "    #newList['tag'].append('kids')\n",
    "    newList['tag'].append(1)\n",
    "\n",
    "\n",
    "for session in allSessionsNA:\n",
    "    string = \"\"\n",
    "    for query in session:\n",
    "        string += query + \" \"\n",
    "    newList['entry'].append(string)\n",
    "    newList['tag'].append(0)\n",
    "\n",
    "newList = pd.DataFrame(data = newList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rNum = 20210530    \n",
    "rng = np.random.RandomState(rNum)\n",
    "\n",
    "newList2 = {}\n",
    "newList2['entry']=[]\n",
    "newList2['tag']=[]\n",
    "\n",
    "for x in range((len(SQSA))+(len(SQSNA))):\n",
    "    if( x < (len(SQSA))):\n",
    "        newList2['tag'].append(1)\n",
    "    else:\n",
    "        newList2['tag'].append(0)\n",
    "\n",
    "\n",
    "for x in range((len(SQSA))+(len(SQSNA))):\n",
    "    if( x < (len(SQSA))):\n",
    "        newList2['entry'].append(SQSA[x])\n",
    "\n",
    "    else:\n",
    "        newList2['entry'].append(SQSNA[x-(len(SQSA))])\n",
    "\n",
    "data2 = pd.DataFrame(data = newList2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnOva = 0\n",
    "fpOva = 0\n",
    "fnOva = 0\n",
    "tpOva = 0\n",
    "testAccOva = 0\n",
    "\n",
    "outputTamSing = []\n",
    "for train_index, test_index in kfold.split(data2):\n",
    "\n",
    "    Test_X, Train_X = data2.iloc[train_index]['entry'], data2.iloc[test_index]['entry']\n",
    "    Test_Y, Train_Y = data2.iloc[train_index]['tag'], data2.iloc[test_index]['tag']\n",
    "    Encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "    rng = np.random.RandomState(rNum)\n",
    "    test = newList.sample(frac=0.20, random_state=rng)\n",
    "    train_mask = pd.Series(True, index=newList.index)\n",
    "    train_mask[test.index] = False\n",
    "    train = newList[train_mask].copy()\n",
    "    train.head()\n",
    "    Train_X = pd.concat([Train_X, train['entry']])\n",
    "    Train_Y = pd.concat([Train_Y, train['tag']])\n",
    "\n",
    "    Encoder = LabelEncoder()\n",
    "    Train_Y = Encoder.fit_transform(Train_Y)\n",
    "    Test_Y = Encoder.fit_transform(Test_Y)\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(3, 3))\n",
    "    vectorizer.fit_transform(newList['entry'])\n",
    "    Train_X_Tfidf = vectorizer.transform(Train_X)\n",
    "    Test_X_Tfidf = vectorizer.transform(Test_X)\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "    outputTamSing.append(predictions_SVM)\n",
    "    testAccOva += accuracy_score(predictions_SVM, Test_Y)\n",
    "    tn, fp, fn, tp = confusion_matrix(Test_Y, predictions_SVM).ravel()\n",
    "    tnOva += tn\n",
    "    fpOva += fp\n",
    "    fnOva += fn\n",
    "    tpOva += tp\n",
    "    rNum+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( outputTamSing, open( \"Pickles/OutputTextSQS.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "textResults = ['Text', round(testAccOva/nSplits,3), tnOva/nSplits, fpOva/nSplits, round(((tnOva/nSplits)/((tnOva/nSplits)+ (fpOva/nSplits))),3), fnOva/nSplits, tpOva/nSplits,round(((tpOva/nSplits)/((tpOva/nSplits)+ (fnOva/nSplits))),3),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text', 0.808, 962.8, 0.4, 1.0, 230.2, 10.6, 0.044]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovaResults.append(textResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Santosh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSessions = pickle.load( open( \"Pickles/SWCNoTune.p\", \"rb\" ) )\n",
    "allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "allSessionsA = allSessionsQ.loc[allSessionsQ['class']==1].groupby('sID')['query'].apply(pd.Series.tolist).tolist()\n",
    "allSessionsNA = allSessionsQ.loc[allSessionsQ['class']==0].groupby('sID')['query'].apply(pd.Series.tolist).tolist()\n",
    "\n",
    "\n",
    "SQSA = np.asarray(pickle.load( open( \"../Data/DataSets/SQS/SQSA.p\", \"rb\" ) ))\n",
    "SQSNA = np.asarray(pickle.load( open( \"../Data/DataSets/SQS/SQSNA.p\", \"rb\" ) ))\n",
    "\n",
    "documents = []\n",
    "classList = []\n",
    "for session in allSessionsA:\n",
    "    \n",
    "    doc = \"\"\n",
    "    for query in session:\n",
    "        doc += \" \" + query\n",
    "    documents.append(doc)\n",
    "    classList.append(1)\n",
    "    \n",
    "for session in allSessionsNA:\n",
    "    \n",
    "    doc = \"\"\n",
    "    for query in session:\n",
    "        doc += \" \" + query\n",
    "    documents.append(doc)\n",
    "    classList.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "newList = pd.DataFrame(data = newList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "classList = []\n",
    "for session in allSessionsA:\n",
    "    \n",
    "    doc = \"\"\n",
    "    for query in session:\n",
    "        doc += \" \" + query\n",
    "    documents.append(doc)\n",
    "    classList.append(1)\n",
    "    \n",
    "for session in allSessionsNA:\n",
    "    \n",
    "    doc = \"\"\n",
    "    for query in session:\n",
    "        doc += \" \" + query\n",
    "    documents.append(doc)\n",
    "    classList.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(documents, columns = ['text'])\n",
    "posData = []\n",
    "for document in documents:\n",
    "    text = nltk.word_tokenize(document)\n",
    "    tags = np.array(nltk.pos_tag(text)).flatten()\n",
    "    posData.append(tags[1::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pos'] = posData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "docUni = []\n",
    "docBi = []\n",
    "docTri = []\n",
    "\n",
    "for document in documents:\n",
    "    doc = generateNgrams(document,1)\n",
    "    docUni.append(doc)\n",
    "    \n",
    "for document in documents:\n",
    "    doc = generateNgrams(document,2)\n",
    "    docBi.append(doc)\n",
    "    \n",
    "for document in documents:\n",
    "    doc = generateNgrams(document,3)\n",
    "    docTri.append(doc)\n",
    "    \n",
    "data['uniWord'] = docUni\n",
    "data['biWord'] = docBi\n",
    "data['triWord']= docTri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "posMod = []\n",
    "\n",
    "for pos in posData: \n",
    "    string = []\n",
    "    for entry in pos:\n",
    "        string += str(entry) + \" \"\n",
    "    posMod.append(\"\".join(string))\n",
    "\n",
    "    \n",
    "posUni = []\n",
    "posBi = []\n",
    "posTri = []\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generateNgrams(document,1)\n",
    "    posUni.append(doc)\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generateNgrams(document,2)\n",
    "    posBi.append(doc)\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generateNgrams(document,3)\n",
    "    posTri.append(doc)  \n",
    "    \n",
    "data['uniPos'] = posUni\n",
    "data['biPos'] = posBi\n",
    "data['triPos']= posTri\n",
    "data['posMod'] = posMod\n",
    "data['class'] = classList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "text = \"\".join(data['text'].tolist())\n",
    "token = word_tokenize(text)\n",
    "\n",
    "unigrams = list(ngrams(token, 1))\n",
    "uniwords = collections.Counter(unigrams)\n",
    "n = len(uniwords)-40000\n",
    "stopWordsUni = uniwords.most_common()[:-n-1:-1]\n",
    "\n",
    "bigrams = list(ngrams(token, 2))\n",
    "biwords = collections.Counter(bigrams)\n",
    "n = len(biwords)-40000\n",
    "stopWordsBi =  biwords.most_common()[:-n-1:-1]\n",
    "\n",
    "trigrams = list(ngrams(token, 3))\n",
    "triwords = collections.Counter(trigrams)\n",
    "n = len(triwords)-40000\n",
    "stopWordsTri = triwords.most_common()[:-n-1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "classList = []\n",
    "\n",
    "for query in SQSA:\n",
    "    doc = \"\"\n",
    "    doc += \" \" + query\n",
    "    documents.append(doc)\n",
    "    classList.append(1)\n",
    " \n",
    "for query in SQSNA:\n",
    "    doc = \"\"   \n",
    "    doc += \" \" + query\n",
    "    documents.append(doc)\n",
    "    classList.append(0)\n",
    "\n",
    "data2 = pd.DataFrame(documents, columns = ['text'])\n",
    "\n",
    "posData = []\n",
    "for document in documents:\n",
    "    text = nltk.word_tokenize(document)\n",
    "    tags = np.array(nltk.pos_tag(text)).flatten()\n",
    "    posData.append(tags[1::2])\n",
    "    \n",
    "data2['pos'] = posData\n",
    "\n",
    "docUni = []\n",
    "docBi = []\n",
    "docTri = []\n",
    "\n",
    "for document in documents:\n",
    "    doc = generateNgrams(document,1)\n",
    "    docUni.append(doc)\n",
    "    \n",
    "for document in documents:\n",
    "    doc = generateNgrams(document,2)\n",
    "    docBi.append(doc)\n",
    "    \n",
    "for document in documents:\n",
    "    doc = generateNgrams(document,3)\n",
    "    docTri.append(doc)\n",
    "    \n",
    "data2['uniWord'] = docUni\n",
    "data2['biWord'] = docBi\n",
    "data2['triWord']= docTri\n",
    "\n",
    "posMod = []\n",
    "\n",
    "for pos in posData: \n",
    "    string = []\n",
    "    for entry in pos:\n",
    "        string += str(entry) + \" \"\n",
    "    posMod.append(\"\".join(string))\n",
    "\n",
    "    \n",
    "posUni = []\n",
    "posBi = []\n",
    "posTri = []\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generateNgrams(document,1)\n",
    "    posUni.append(doc)\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generateNgrams(document,2)\n",
    "    posBi.append(doc)\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generateNgrams(document,3)\n",
    "    posTri.append(doc)  \n",
    "    \n",
    "data2['uniPos'] = posUni\n",
    "data2['biPos'] = posBi\n",
    "data2['triPos']= posTri\n",
    "data2['posMod'] = posMod\n",
    "data2['class'] = classList\n",
    "data2['numQ'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>uniWord</th>\n",
       "      <th>biWord</th>\n",
       "      <th>triWord</th>\n",
       "      <th>uniPos</th>\n",
       "      <th>biPos</th>\n",
       "      <th>triPos</th>\n",
       "      <th>posMod</th>\n",
       "      <th>class</th>\n",
       "      <th>numQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How did paul die from fast in the fouris</td>\n",
       "      <td>[WRB, VBD, VB, NN, IN, NN, IN, DT, NN]</td>\n",
       "      <td>[how, did, paul, die, from, fast, in, the, fou...</td>\n",
       "      <td>[how did, did paul, paul die, die from, from f...</td>\n",
       "      <td>[how did paul, did paul die, paul die from, di...</td>\n",
       "      <td>[wrb, vbd, vb, nn, in, nn, in, dt, nn]</td>\n",
       "      <td>[wrb vbd, vbd vb, vb nn, nn in, in nn, nn in, ...</td>\n",
       "      <td>[wrb vbd vb, vbd vb nn, vb nn in, nn in nn, in...</td>\n",
       "      <td>WRB VBD VB NN IN NN IN DT NN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fire belly toad</td>\n",
       "      <td>[NNP, RB, VBD]</td>\n",
       "      <td>[fire, belly, toad]</td>\n",
       "      <td>[fire belly, belly toad]</td>\n",
       "      <td>[fire belly toad]</td>\n",
       "      <td>[nnp, rb, vbd]</td>\n",
       "      <td>[nnp rb, rb vbd]</td>\n",
       "      <td>[nnp rb vbd]</td>\n",
       "      <td>NNP RB VBD</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Now im stressed out</td>\n",
       "      <td>[RB, RB, VBN, RP]</td>\n",
       "      <td>[now, im, stressed, out]</td>\n",
       "      <td>[now im, im stressed, stressed out]</td>\n",
       "      <td>[now im stressed, im stressed out]</td>\n",
       "      <td>[rb, rb, vbn, rp]</td>\n",
       "      <td>[rb rb, rb vbn, vbn rp]</td>\n",
       "      <td>[rb rb vbn, rb vbn rp]</td>\n",
       "      <td>RB RB VBN RP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fast in the fouris</td>\n",
       "      <td>[RB, IN, DT, NN]</td>\n",
       "      <td>[fast, in, the, fouris]</td>\n",
       "      <td>[fast in, in the, the fouris]</td>\n",
       "      <td>[fast in the, in the fouris]</td>\n",
       "      <td>[rb, in, dt, nn]</td>\n",
       "      <td>[rb in, in dt, dt nn]</td>\n",
       "      <td>[rb in dt, in dt nn]</td>\n",
       "      <td>RB IN DT NN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many phones sell in a day</td>\n",
       "      <td>[WRB, JJ, NNS, VBP, IN, DT, NN]</td>\n",
       "      <td>[how, many, phones, sell, in, a, day]</td>\n",
       "      <td>[how many, many phones, phones sell, sell in, ...</td>\n",
       "      <td>[how many phones, many phones sell, phones sel...</td>\n",
       "      <td>[wrb, jj, nns, vbp, in, dt, nn]</td>\n",
       "      <td>[wrb jj, jj nns, nns vbp, vbp in, in dt, dt nn]</td>\n",
       "      <td>[wrb jj nns, jj nns vbp, nns vbp in, vbp in dt...</td>\n",
       "      <td>WRB JJ NNS VBP IN DT NN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>apply for a State University of New York college</td>\n",
       "      <td>[NN, IN, DT, NNP, NNP, IN, NNP, NNP, NN]</td>\n",
       "      <td>[apply, for, a, state, university, of, new, yo...</td>\n",
       "      <td>[apply for, for a, a state, state university, ...</td>\n",
       "      <td>[apply for a, for a state, a state university,...</td>\n",
       "      <td>[nn, in, dt, nnp, nnp, in, nnp, nnp, nn]</td>\n",
       "      <td>[nn in, in dt, dt nnp, nnp nnp, nnp in, in nnp...</td>\n",
       "      <td>[nn in dt, in dt nnp, dt nnp nnp, nnp nnp in, ...</td>\n",
       "      <td>NN IN DT NNP NNP IN NNP NNP NN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>supreme court on banning guns</td>\n",
       "      <td>[JJ, NN, IN, VBG, NNS]</td>\n",
       "      <td>[supreme, court, on, banning, guns]</td>\n",
       "      <td>[supreme court, court on, on banning, banning ...</td>\n",
       "      <td>[supreme court on, court on banning, on bannin...</td>\n",
       "      <td>[jj, nn, in, vbg, nns]</td>\n",
       "      <td>[jj nn, nn in, in vbg, vbg nns]</td>\n",
       "      <td>[jj nn in, nn in vbg, in vbg nns]</td>\n",
       "      <td>JJ NN IN VBG NNS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>philadelphia nyc distance</td>\n",
       "      <td>[NN, NN, NN]</td>\n",
       "      <td>[philadelphia, nyc, distance]</td>\n",
       "      <td>[philadelphia nyc, nyc distance]</td>\n",
       "      <td>[philadelphia nyc distance]</td>\n",
       "      <td>[nn, nn, nn]</td>\n",
       "      <td>[nn nn, nn nn]</td>\n",
       "      <td>[nn nn nn]</td>\n",
       "      <td>NN NN NN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>themed road trips</td>\n",
       "      <td>[VBN, NN, NNS]</td>\n",
       "      <td>[themed, road, trips]</td>\n",
       "      <td>[themed road, road trips]</td>\n",
       "      <td>[themed road trips]</td>\n",
       "      <td>[vbn, nn, nns]</td>\n",
       "      <td>[vbn nn, nn nns]</td>\n",
       "      <td>[vbn nn nns]</td>\n",
       "      <td>VBN NN NNS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>eurozone debt</td>\n",
       "      <td>[NN, NN]</td>\n",
       "      <td>[eurozone, debt]</td>\n",
       "      <td>[eurozone debt]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nn, nn]</td>\n",
       "      <td>[nn nn]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NN NN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1505 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0              How did paul die from fast in the fouris   \n",
       "1                                       Fire belly toad   \n",
       "2                                   Now im stressed out   \n",
       "3                                    fast in the fouris   \n",
       "4                         How many phones sell in a day   \n",
       "...                                                 ...   \n",
       "1500   apply for a State University of New York college   \n",
       "1501                      supreme court on banning guns   \n",
       "1502                          philadelphia nyc distance   \n",
       "1503                                  themed road trips   \n",
       "1504                                      eurozone debt   \n",
       "\n",
       "                                           pos  \\\n",
       "0       [WRB, VBD, VB, NN, IN, NN, IN, DT, NN]   \n",
       "1                               [NNP, RB, VBD]   \n",
       "2                            [RB, RB, VBN, RP]   \n",
       "3                             [RB, IN, DT, NN]   \n",
       "4              [WRB, JJ, NNS, VBP, IN, DT, NN]   \n",
       "...                                        ...   \n",
       "1500  [NN, IN, DT, NNP, NNP, IN, NNP, NNP, NN]   \n",
       "1501                    [JJ, NN, IN, VBG, NNS]   \n",
       "1502                              [NN, NN, NN]   \n",
       "1503                            [VBN, NN, NNS]   \n",
       "1504                                  [NN, NN]   \n",
       "\n",
       "                                                uniWord  \\\n",
       "0     [how, did, paul, die, from, fast, in, the, fou...   \n",
       "1                                   [fire, belly, toad]   \n",
       "2                              [now, im, stressed, out]   \n",
       "3                               [fast, in, the, fouris]   \n",
       "4                 [how, many, phones, sell, in, a, day]   \n",
       "...                                                 ...   \n",
       "1500  [apply, for, a, state, university, of, new, yo...   \n",
       "1501                [supreme, court, on, banning, guns]   \n",
       "1502                      [philadelphia, nyc, distance]   \n",
       "1503                              [themed, road, trips]   \n",
       "1504                                   [eurozone, debt]   \n",
       "\n",
       "                                                 biWord  \\\n",
       "0     [how did, did paul, paul die, die from, from f...   \n",
       "1                              [fire belly, belly toad]   \n",
       "2                   [now im, im stressed, stressed out]   \n",
       "3                         [fast in, in the, the fouris]   \n",
       "4     [how many, many phones, phones sell, sell in, ...   \n",
       "...                                                 ...   \n",
       "1500  [apply for, for a, a state, state university, ...   \n",
       "1501  [supreme court, court on, on banning, banning ...   \n",
       "1502                   [philadelphia nyc, nyc distance]   \n",
       "1503                          [themed road, road trips]   \n",
       "1504                                    [eurozone debt]   \n",
       "\n",
       "                                                triWord  \\\n",
       "0     [how did paul, did paul die, paul die from, di...   \n",
       "1                                     [fire belly toad]   \n",
       "2                    [now im stressed, im stressed out]   \n",
       "3                          [fast in the, in the fouris]   \n",
       "4     [how many phones, many phones sell, phones sel...   \n",
       "...                                                 ...   \n",
       "1500  [apply for a, for a state, a state university,...   \n",
       "1501  [supreme court on, court on banning, on bannin...   \n",
       "1502                        [philadelphia nyc distance]   \n",
       "1503                                [themed road trips]   \n",
       "1504                                                 []   \n",
       "\n",
       "                                        uniPos  \\\n",
       "0       [wrb, vbd, vb, nn, in, nn, in, dt, nn]   \n",
       "1                               [nnp, rb, vbd]   \n",
       "2                            [rb, rb, vbn, rp]   \n",
       "3                             [rb, in, dt, nn]   \n",
       "4              [wrb, jj, nns, vbp, in, dt, nn]   \n",
       "...                                        ...   \n",
       "1500  [nn, in, dt, nnp, nnp, in, nnp, nnp, nn]   \n",
       "1501                    [jj, nn, in, vbg, nns]   \n",
       "1502                              [nn, nn, nn]   \n",
       "1503                            [vbn, nn, nns]   \n",
       "1504                                  [nn, nn]   \n",
       "\n",
       "                                                  biPos  \\\n",
       "0     [wrb vbd, vbd vb, vb nn, nn in, in nn, nn in, ...   \n",
       "1                                      [nnp rb, rb vbd]   \n",
       "2                               [rb rb, rb vbn, vbn rp]   \n",
       "3                                 [rb in, in dt, dt nn]   \n",
       "4       [wrb jj, jj nns, nns vbp, vbp in, in dt, dt nn]   \n",
       "...                                                 ...   \n",
       "1500  [nn in, in dt, dt nnp, nnp nnp, nnp in, in nnp...   \n",
       "1501                    [jj nn, nn in, in vbg, vbg nns]   \n",
       "1502                                     [nn nn, nn nn]   \n",
       "1503                                   [vbn nn, nn nns]   \n",
       "1504                                            [nn nn]   \n",
       "\n",
       "                                                 triPos  \\\n",
       "0     [wrb vbd vb, vbd vb nn, vb nn in, nn in nn, in...   \n",
       "1                                          [nnp rb vbd]   \n",
       "2                                [rb rb vbn, rb vbn rp]   \n",
       "3                                  [rb in dt, in dt nn]   \n",
       "4     [wrb jj nns, jj nns vbp, nns vbp in, vbp in dt...   \n",
       "...                                                 ...   \n",
       "1500  [nn in dt, in dt nnp, dt nnp nnp, nnp nnp in, ...   \n",
       "1501                  [jj nn in, nn in vbg, in vbg nns]   \n",
       "1502                                         [nn nn nn]   \n",
       "1503                                       [vbn nn nns]   \n",
       "1504                                                 []   \n",
       "\n",
       "                               posMod  class  numQ  \n",
       "0       WRB VBD VB NN IN NN IN DT NN       1     1  \n",
       "1                         NNP RB VBD       1     1  \n",
       "2                       RB RB VBN RP       1     1  \n",
       "3                        RB IN DT NN       1     1  \n",
       "4            WRB JJ NNS VBP IN DT NN       1     1  \n",
       "...                               ...    ...   ...  \n",
       "1500  NN IN DT NNP NNP IN NNP NNP NN       0     1  \n",
       "1501                JJ NN IN VBG NNS       0     1  \n",
       "1502                        NN NN NN       0     1  \n",
       "1503                      VBN NN NNS       0     1  \n",
       "1504                           NN NN       0     1  \n",
       "\n",
       "[1505 rows x 11 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1204\n",
      "start\n",
      "1204\n",
      "start\n",
      "1204\n",
      "start\n",
      "1204\n",
      "start\n",
      "1204\n",
      "start\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FlatSingle = pd.DataFrame(data= data2)\n",
    "rn = 20210530\n",
    "\n",
    "nSplits = 5\n",
    "tnOva = 0\n",
    "fpOva = 0\n",
    "fnOva = 0\n",
    "tpOva = 0\n",
    "testAccOva = 0\n",
    "x = 0\n",
    "accByNum = []\n",
    "kfold = KFold(n_splits=nSplits, random_state=20210530, shuffle=True)\n",
    "outputSanSing = []\n",
    "for train_index, test_index in kfold.split(data2):\n",
    "    \n",
    "\n",
    "    train = data2.iloc[test_index]\n",
    "    test = data2.iloc[train_index]\n",
    "    \n",
    "    rng = np.random.RandomState(rn)\n",
    "    testSing = data.sample(frac=0.20, random_state=rng)\n",
    "    train_mask = pd.Series(True, index=data.index)\n",
    "    train_mask[testSing.index] = False\n",
    "    trainSing = data[train_mask].copy()\n",
    "\n",
    "\n",
    "#     print(len(trainSing))\n",
    "    train.head()\n",
    "    train = pd.concat([train,trainSing])\n",
    "#     print(len(train))\n",
    "    #print(len(test))\n",
    "    subtest = train.sample(frac=0.20, random_state=rng)\n",
    "    subtrain_mask = pd.Series(True, index=train.index)\n",
    "    subtrain_mask[subtest.index] = False\n",
    "    subtrain = train[subtrain_mask].copy()\n",
    "    subtrain.head()\n",
    "\n",
    "    print('start')\n",
    "\n",
    "    subtest = train.sample(frac=0.20, random_state=rng)\n",
    "    subtrain_mask = pd.Series(True, index=train.index)\n",
    "    subtrain_mask[subtest.index] = False\n",
    "    subtrain = train[subtrain_mask].copy()\n",
    "    subtrain.head()\n",
    "#     print(subtest)\n",
    "    Encoder = LabelEncoder()\n",
    "    Train_Y = Encoder.fit_transform(subtrain['class'])\n",
    "    Test_Y = Encoder.fit_transform(subtest['class'])\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words=stopWordsUni)\n",
    "    vectorizer.fit_transform(train['text'])\n",
    "    Train_X_Tfidf = vectorizer.transform(subtrain['text'])\n",
    "    Test_X_Tfidf = vectorizer.transform(subtest['text'])\n",
    "\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "    predictions_SVM_Uni = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    Test_X_Tfidf = vectorizer.transform(test['text'])\n",
    "    test_predictions_SVM_uni = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    Encoder = LabelEncoder()\n",
    "    Train_Y = Encoder.fit_transform(subtrain['class'])\n",
    "    Test_Y = Encoder.fit_transform(subtest['class'])\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2),stop_words=stopWordsBi)\n",
    "    vectorizer.fit_transform(train['text'])\n",
    "    Train_X_Tfidf = vectorizer.transform(subtrain['text'])\n",
    "    Test_X_Tfidf = vectorizer.transform(subtest['text'])\n",
    "\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', )\n",
    "    SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "    predictions_SVM_bi = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    Test_X_Tfidf = vectorizer.transform(test['text'])\n",
    "    test_predictions_SVM_bi = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    Encoder = LabelEncoder()\n",
    "    Train_Y = Encoder.fit_transform(subtrain['class'])\n",
    "    Test_Y = Encoder.fit_transform(subtest['class'])\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(3, 3), stop_words=stopWordsTri)\n",
    "    vectorizer.fit_transform(train['text'])\n",
    "    Train_X_Tfidf = vectorizer.transform(subtrain['text'])\n",
    "    Test_X_Tfidf = vectorizer.transform(subtest['text'])\n",
    "\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "    predictions_SVM_tri = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    Test_X_Tfidf = vectorizer.transform(test['text'])\n",
    "    test_predictions_SVM_tri = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    Encoder = LabelEncoder()\n",
    "    Train_Y = Encoder.fit_transform(subtrain['class'])\n",
    "    Test_Y = Encoder.fit_transform(subtest['class'])\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit_transform(train['posMod'])\n",
    "    Train_X_Tfidf = vectorizer.transform(subtrain['posMod'])\n",
    "    Test_X_Tfidf = vectorizer.transform(subtest['posMod'])\n",
    "\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "    predictions_SVM_pos_uni = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "\n",
    "    Test_X_Tfidf = vectorizer.transform(test['posMod'])\n",
    "    test_predictions_SVM_pos_uni = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    Encoder = LabelEncoder()\n",
    "    Train_Y = Encoder.fit_transform(subtrain['class'])\n",
    "    Test_Y = Encoder.fit_transform(subtest['class'])\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "    vectorizer.fit_transform(train['posMod'])\n",
    "    Train_X_Tfidf = vectorizer.transform(subtrain['posMod'])\n",
    "    Test_X_Tfidf = vectorizer.transform(subtest['posMod'])\n",
    "\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "    predictions_SVM_pos_bi = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "\n",
    "    Test_X_Tfidf = vectorizer.transform(test['posMod'])\n",
    "    test_predictions_SVM_pos_bi = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    Encoder = LabelEncoder()\n",
    "    Train_Y = Encoder.fit_transform(subtrain['class'])\n",
    "    Test_Y = Encoder.fit_transform(subtest['class'])\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(3, 3))\n",
    "    vectorizer.fit_transform(train['posMod'])\n",
    "    Train_X_Tfidf = vectorizer.transform(subtrain['posMod'])\n",
    "    Test_X_Tfidf = vectorizer.transform(subtest['posMod'])\n",
    "\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "    predictions_SVM_pos_tri = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "\n",
    "\n",
    "    Test_X_Tfidf = vectorizer.transform(test['posMod'])\n",
    "    test_predictions_SVM_pos_tri = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    LDAtest = train.sample(frac=0.4, random_state=rng)\n",
    "    LDAtrain_mask = pd.Series(True, index=train.index)\n",
    "    LDAtrain_mask[LDAtest.index] = False\n",
    "    LDAtrain = train[LDAtrain_mask].copy()\n",
    "\n",
    "\n",
    "    processed_docs = LDAtrain['uniWord']\n",
    "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "    lda_model = models.LdaModel(corpus, num_topics=200, \\\n",
    "                                      id2word=dictionary, \\\n",
    "                                      passes=4, alpha=[0.01]*200, \\\n",
    "                                      eta=[0.01]*len(dictionary.keys()))\n",
    "\n",
    "    subtest['ldaOneOverall'] = subtest['text'].apply(lambda x: rowIndex(x))\n",
    "    test['ldaOneOverall'] = test['text'].apply(lambda x: rowIndex(x))\n",
    "    LDAtest['ldaOneOverall'] = LDAtest['text'].apply(lambda x: rowIndex(x))\n",
    "\n",
    "    ldaOOList = LDAtest['ldaOneOverall'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(200):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "\n",
    "    LDAtest['ldaOneOverallPad'] =  ldaAllPad\n",
    "\n",
    "    ldaOOList = test['ldaOneOverall'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(200):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    test['ldaOneOverallPad'] =  ldaAllPad\n",
    "\n",
    "    ldaOOList = subtest['ldaOneOverall'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(200):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    subtest['ldaOneOverallPad'] =  ldaAllPad\n",
    "\n",
    "    processed_docs = LDAtrain.loc[LDAtrain['class'] ==1]['uniWord']\n",
    "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "    lda_model = models.LdaModel(corpus, num_topics=100, \\\n",
    "                                      id2word=dictionary, \\\n",
    "                                      passes=4, alpha=[0.01]*100, \\\n",
    "                                      eta=[0.01]*len(dictionary.keys()))\n",
    "\n",
    "    subtest['ldaOneTrue'] = subtest['text'].apply(lambda x: rowIndex(x))\n",
    "    test['ldaOneTrue'] = test['text'].apply(lambda x: rowIndex(x))\n",
    "    LDAtest['ldaOneTrue'] = LDAtest['text'].apply(lambda x: rowIndex(x))\n",
    "\n",
    "    ldaOOList = LDAtest['ldaOneTrue'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(100):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    LDAtest['ldaOneTrue'] =  ldaAllPad\n",
    "\n",
    "    ldaOOList = test['ldaOneTrue'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(100):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    test['ldaOneTrue'] =  ldaAllPad\n",
    "\n",
    "    ldaOOList = subtest['ldaOneTrue'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(100):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    subtest['ldaOneTrue'] =  ldaAllPad\n",
    "\n",
    "    processed_docs = LDAtrain.loc[LDAtrain['class'] ==0]['uniWord']\n",
    "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "    lda_model = models.LdaModel(corpus, num_topics=100, \\\n",
    "                                      id2word=dictionary, \\\n",
    "                                      passes=4, alpha=[0.01]*100, \\\n",
    "                                      eta=[0.01]*len(dictionary.keys()))\n",
    "\n",
    "    subtest['ldaOneFalse'] = subtest['text'].apply(lambda x: rowIndex(x))\n",
    "    test['ldaOneFalse'] = test['text'].apply(lambda x: rowIndex(x))\n",
    "    LDAtest['ldaOneFalse'] = LDAtest['text'].apply(lambda x: rowIndex(x))\n",
    "\n",
    "    ldaOOList = LDAtest['ldaOneFalse'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(100):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    LDAtest['ldaOneFalse'] =  ldaAllPad\n",
    "\n",
    "    ldaOOList = test['ldaOneFalse'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(100):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    test['ldaOneFalse'] =  ldaAllPad\n",
    "\n",
    "    ldaOOList = subtest['ldaOneFalse'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(100):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    subtest['ldaOneFalse'] =  ldaAllPad\n",
    "\n",
    "    LDAtest['LDAClassOne'] = LDAtest['ldaOneTrue'] + LDAtest['ldaOneFalse']\n",
    "    test['LDAClassOne'] = test['ldaOneTrue'] + test['ldaOneFalse'] \n",
    "    subtest['LDAClassOne'] = subtest['ldaOneTrue'] + subtest['ldaOneFalse'] \n",
    "\n",
    "    LDAtest['LDAallOne'] = LDAtest['ldaOneOverallPad'] + LDAtest['LDAClassOne']\n",
    "    test['LDAallOne'] = test['ldaOneOverallPad'] + test['LDAClassOne'] \n",
    "    subtest['LDAallOne'] = subtest['ldaOneOverallPad'] + subtest['LDAClassOne'] \n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(random_state=0, solver = 'liblinear').fit(list(LDAtest['ldaOneOverallPad']),LDAtest['class'])\n",
    "\n",
    "    resultsOne = clf.predict(list(subtest['ldaOneOverallPad']))\n",
    "    testresultsOne = clf.predict(list(test['ldaOneOverallPad']))\n",
    "\n",
    "    clf = LogisticRegression(random_state=0, solver = 'liblinear').fit(list(LDAtest['LDAClassOne']),LDAtest['class'])\n",
    "\n",
    "    resultsTwo = clf.predict(list(subtest['LDAClassOne']))\n",
    "    testresultsTwo = clf.predict(list(test['LDAClassOne']))\n",
    "\n",
    "    clf = LogisticRegression(random_state=0, solver = 'liblinear').fit(list(LDAtest['LDAallOne']),LDAtest['class'])\n",
    "\n",
    "    resultsThree = clf.predict(list(subtest['LDAallOne']))\n",
    "    testresultsThree = clf.predict(list(test['LDAallOne']))\n",
    "\n",
    "    subtest['SVMUni'] = predictions_SVM_Uni\n",
    "    subtest['SVMBi'] = predictions_SVM_bi\n",
    "    subtest['SVMTri'] = predictions_SVM_tri\n",
    "    subtest['SVMUniPOS'] = predictions_SVM_pos_uni\n",
    "    subtest['SVMBiPOS'] = predictions_SVM_pos_bi\n",
    "    subtest['SVMTriPOS'] = predictions_SVM_pos_tri\n",
    "    subtest['SVMUniLDA'] = resultsOne\n",
    "    subtest['SVMBiLDA'] = resultsTwo\n",
    "    subtest['SVMTriLDA'] = resultsThree\n",
    "\n",
    "    test['SVMUni'] = test_predictions_SVM_uni\n",
    "    test['SVMBi'] = test_predictions_SVM_bi\n",
    "    test['SVMTri'] = test_predictions_SVM_tri\n",
    "    test['SVMUniPOS'] = test_predictions_SVM_pos_uni\n",
    "    test['SVMBiPOS'] = test_predictions_SVM_pos_bi\n",
    "    test['SVMTriPOS'] = test_predictions_SVM_pos_tri\n",
    "    test['SVMUniLDA'] = testresultsOne\n",
    "    test['SVMBiLDA'] = testresultsTwo\n",
    "    test['SVMTriLDA'] = testresultsThree\n",
    "\n",
    "\n",
    "    feat_cols = [\n",
    "        'SVMUni',\n",
    "        'SVMBi',\n",
    "        'SVMTri',\n",
    "        'SVMUniPOS',\n",
    "        'SVMBiPOS',\n",
    "        'SVMTriPOS',\n",
    "        'SVMUniLDA',\n",
    "        'SVMBiLDA',\n",
    "        'SVMTriLDA',\n",
    "\n",
    "\n",
    "    ]\n",
    "\n",
    "    out_col = 'class'\n",
    "\n",
    "    train_x = subtest[feat_cols]\n",
    "    train_y = subtest[out_col]\n",
    "    test_x = test[feat_cols]\n",
    "    test_y = test[out_col]\n",
    "\n",
    "    pure_pipeL2 = Pipeline([\n",
    "        ('standardize', StandardScaler()),\n",
    "        ('classify', DecisionTreeClassifier(criterion='entropy',random_state= 20210518, class_weight = 'balanced'))\n",
    "        ])\n",
    "\n",
    "    pure_pipeL2.fit(train_x, train_y)\n",
    "\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    prediction = pure_pipeL2.predict(test_x)\n",
    "    testAccOva += accuracy_score(test_y, prediction)\n",
    "    outputSanSing.append(prediction)\n",
    "    SanAQtn, SanAQfp, SanAQfn, SanAQtp = confusion_matrix(test_y, prediction).ravel()\n",
    "    tnOva += SanAQtn\n",
    "    fpOva += SanAQfp\n",
    "    fnOva += SanAQfn\n",
    "    tpOva += SanAQtp\n",
    "    rn+=1\n",
    "    x+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( outputSanSing, open( \"Pickles/OutputMultiFeatureSQS.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 1, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 1, ..., 0, 0, 0])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputSanSing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiFeatureResults = ['MultiFeature', round(testAccOva/nSplits,3), tnOva/nSplits, fpOva/nSplits, round(((tnOva/nSplits)/((tnOva/nSplits)+ (fpOva/nSplits))),3), fnOva/nSplits, tpOva/nSplits,round(((tpOva/nSplits)/((tpOva/nSplits)+ (fnOva/nSplits))),3),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MultiFeature', 0.832, 955.2, 8.0, 0.992, 194.8, 46.0, 0.191]\n"
     ]
    }
   ],
   "source": [
    "print(multiFeatureResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovaResults.append(multiFeatureResults )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nemati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "nSplits = 5\n",
    "kfold = KFold(n_splits=nSplits, random_state=20210530, shuffle=True)\n",
    "\n",
    "allSessions = pickle.load( open( \"Pickles/SWCNoTune.p\", \"rb\" ) )\n",
    "allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "allSessionsK = allSessionsQ.loc[allSessionsQ['class']==1]\n",
    "allSessionsA = allSessionsQ.loc[allSessionsQ['class']==0]\n",
    "\n",
    "kidsAll = allSessionsK.groupby('sID')['query'].apply(pd.Series.tolist).tolist()\n",
    "TRECS =  allSessionsA.groupby('sID')['query'].apply(pd.Series.tolist).tolist()\n",
    "kidsSolo = np.asarray(pickle.load( open( \"../Data/DataSets/SQS/SQSA.p\", \"rb\" ) ))\n",
    "millQ = np.asarray(pickle.load( open( \"../Data/DataSets/SQS/SQSNA.p\", \"rb\" ) ))\n",
    "\n",
    "newList = {}\n",
    "newList['entry']=[]\n",
    "newList['class']=[]\n",
    "newList['numQ']=[]\n",
    "\n",
    "for session in kidsAll:\n",
    "    string = \"\"\n",
    "    numQ = 0\n",
    "    for query in session:\n",
    "        string += query + \" \"\n",
    "        numQ += 1\n",
    "    newList['entry'].append(string)\n",
    "    #newList['tag'].append('kids')\n",
    "    newList['class'].append(1)\n",
    "    newList['numQ'].append(numQ)\n",
    "\n",
    "\n",
    "for session in TRECS:\n",
    "    string = \"\"\n",
    "    numQ = 0\n",
    "    for query in session:\n",
    "        string += query + \" \"\n",
    "        numQ += 1\n",
    "    newList['entry'].append(string)\n",
    "    #newList['tag'].append('kids')\n",
    "    newList['class'].append(0)\n",
    "    newList['numQ'].append(numQ)\n",
    "data = pd.DataFrame(newList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Y1 = []\n",
    "for x in range((301 + 1204)):\n",
    "    if( x < (301)):\n",
    "        Test_Y1.append(1)\n",
    "    else:\n",
    "        Test_Y1.append(0)\n",
    "Test_X2 = []\n",
    "for x in range((301 + 1204)):\n",
    "    if( x < (301)):\n",
    "        Test_X2.append(kidsSolo[x])\n",
    "    else:\n",
    "        Test_X2.append(millQ[x-301])\n",
    "        \n",
    "single = pd.DataFrame(data =[Test_Y1,Test_X2]).T\n",
    "single.rename(columns ={0:'class', 1:'entry'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    ('lr', make_pipeline(CountVectorizer(),\n",
    "                           LogisticRegressionCV(solver = 'liblinear', max_iter=1000))),\n",
    "    ('nb', make_pipeline(TfidfVectorizer(),\n",
    "                          MultinomialNB(alpha = .13))),\n",
    "    ('gb', make_pipeline(CountVectorizer(),\n",
    "                            GradientBoostingClassifier(learning_rate = .2, max_depth = 2))),\n",
    "    ('mlp', make_pipeline(CountVectorizer(),\n",
    "                           MLPClassifier(hidden_layer_sizes= 21, max_iter=1500, shuffle=False, tol=0.012 ))),   \n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf1 = VotingClassifier(estimators=estimators, voting='hard')\n",
    "\n",
    "tnOva = 0\n",
    "fpOva = 0\n",
    "fnOva = 0\n",
    "tpOva = 0\n",
    "testAccOva = 0\n",
    "accByNum = []\n",
    "outputNem = []\n",
    "outputQ = []\n",
    "rn = 20210530\n",
    "rng = np.random.RandomState(rn)\n",
    "tnOva = 0\n",
    "fpOva = 0\n",
    "fnOva = 0\n",
    "tpOva = 0\n",
    "testAccOva = 0\n",
    "outputNemSing = []\n",
    "x = 0\n",
    "for train_index, test_index in kfold.split(single):\n",
    "\n",
    "    rng = np.random.RandomState(rn)\n",
    "    test = data.sample(frac=0.20, random_state=rng)\n",
    "    train_mask = pd.Series(True, index=data.index)\n",
    "    train_mask[test.index] = False\n",
    "    train = data[train_mask].copy()\n",
    "    train.head()\n",
    "\n",
    "    X_train, X_test = single.iloc[train_index]['entry'], single.iloc[test_index]['entry']\n",
    "    y_train, y_test = single.iloc[train_index]['class'], single.iloc[test_index]['class']\n",
    "    #print(X_test)\n",
    "    X_test = pd.concat([X_test, test['entry']])\n",
    "    y_test = pd.concat([y_test, test['class']])\n",
    "    #print(X_test.tolist())\n",
    "    eclf1 = eclf1.fit(X_test.tolist(), y_test.tolist())\n",
    "    prediction = eclf1.predict(X_train)\n",
    "    outputNemSing.append(prediction)\n",
    "    testAccOva +=  accuracy_score(prediction, y_train.tolist())\n",
    "    tn, fp, fn, tp = confusion_matrix( y_train.tolist(), prediction).ravel()\n",
    "    tnOva += tn\n",
    "    fpOva += fp\n",
    "    fnOva += fn\n",
    "    tpOva += tp\n",
    "    rn+=1\n",
    "    x+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( outputNemSing, open( \"Pickles/OutputMultiModelSQS.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiModelResults = ['MultiModel', round(testAccOva/nSplits,3), tnOva/nSplits, fpOva/nSplits, round(((tnOva/nSplits)/((tnOva/nSplits)+ (fpOva/nSplits))),3), fnOva/nSplits, tpOva/nSplits,round(((tpOva/nSplits)/((tpOva/nSplits)+ (fnOva/nSplits))),3),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MultiModel', 0.834, 961.2, 2.0, 0.998, 198.4, 42.4, 0.176]\n"
     ]
    }
   ],
   "source": [
    "print(multiModelResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovaResults.append(multiModelResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "allResults = pd.DataFrame(data = ovaResults, columns = [\"Type\",\"Acc\", \"TN\", \"FP\", \"TNR\", \"FN\", \"TP\", \"TPR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Acc</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>TNR</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Majority</td>\n",
       "      <td>0.800</td>\n",
       "      <td>963.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>240.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rule</td>\n",
       "      <td>0.800</td>\n",
       "      <td>963.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>240.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Text</td>\n",
       "      <td>0.808</td>\n",
       "      <td>962.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.000</td>\n",
       "      <td>230.2</td>\n",
       "      <td>10.6</td>\n",
       "      <td>0.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MultiFeature</td>\n",
       "      <td>0.832</td>\n",
       "      <td>955.2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.992</td>\n",
       "      <td>194.8</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MultiModel</td>\n",
       "      <td>0.834</td>\n",
       "      <td>961.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.998</td>\n",
       "      <td>198.4</td>\n",
       "      <td>42.4</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Type    Acc     TN   FP    TNR     FN    TP    TPR\n",
       "0      Majority  0.800  963.2  0.0  1.000  240.8   0.0  0.000\n",
       "1          Rule  0.800  963.2  0.0  1.000  240.8   0.0  0.000\n",
       "2          Text  0.808  962.8  0.4  1.000  230.2  10.6  0.044\n",
       "3  MultiFeature  0.832  955.2  8.0  0.992  194.8  46.0  0.191\n",
       "4    MultiModel  0.834  961.2  2.0  0.998  198.4  42.4  0.176"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( allResults, open( \"Pickles/BaselineSQS.p\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
