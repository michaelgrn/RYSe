{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we perform our ablation study on the SQS data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries\n",
    "\n",
    "In the following block of code we import the libraries used in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions\n",
    "\n",
    "In the following block of code we define the features used in our ablation study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performs the classification process determining whether or not a user belongs to our stereotype\n",
    "## params is the dictionary that contains the best hyperparameters\n",
    "## feats is the list of features to tets and train one\n",
    "## featType is a string of the feature set being used\n",
    "## returns an array of results in terms of Accuracy, TN, FP, TNR, FN, TP, TPR\n",
    "\n",
    "def recognizeSearcher(params, feats, featType):\n",
    "    \n",
    "    tnOva = 0\n",
    "    fpOva = 0\n",
    "    fnOva = 0\n",
    "    tpOva = 0\n",
    "    testAccOva = 0\n",
    "    nSplits = 5\n",
    "    x = 0\n",
    "    \n",
    "    accByNum = []\n",
    "    outputAll = []\n",
    "    outputQ = []\n",
    "\n",
    "    kfold = KFold(n_splits=nSplits, random_state=20210530, shuffle=True)\n",
    "    \n",
    "    randomNumber = 20210530 \n",
    "    \n",
    "    X = SWC[feats]\n",
    "    y = SWC['class']\n",
    "   \n",
    "    if params['scaler']:\n",
    "        if params['classWeight']:\n",
    "            tunePipe = Pipeline([\n",
    "            ('standardize', params['scaler']),\n",
    "            ('classify', RandomForestClassifier(n_estimators=params['numEstimators'], bootstrap = params['bootStrap'],\n",
    "                                                criterion= params['criterion'], class_weight = params['classWeight'], \n",
    "                                                random_state= randomNumber, n_jobs = -1))\n",
    "            ])\n",
    "        else:\n",
    "            tunePipe = Pipeline([\n",
    "            ('standardize', params['scaler']),\n",
    "            ('classify', RandomForestClassifier(n_estimators=params['numEstimators'], bootstrap = params['bootStrap'],\n",
    "                                                criterion= params['criterion'], random_state= randomNumber, n_jobs = -1))\n",
    "            ])\n",
    "    else:\n",
    "        if params['classWeight']:\n",
    "            tunePipe = Pipeline([\n",
    "            ('classify', RandomForestClassifier(n_estimators=params['numEstimators'], bootstrap = params['bootStrap'],\n",
    "                                                criterion= params['criterion'], class_weight = params['classWeight'], \n",
    "                                                random_state= randomNumber, n_jobs = -1))\n",
    "            ])\n",
    "        else:\n",
    "            tunePipe = Pipeline([\n",
    "            ('classify', RandomForestClassifier(n_estimators=params['numEstimators'], bootstrap = params['bootStrap'],\n",
    "                                                criterion= params['criterion'], random_state= randomNumber, n_jobs = -1))\n",
    "            ])\n",
    "                \n",
    "\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "\n",
    "        rng = np.random.RandomState(randomNumber)\n",
    "    \n",
    "        trainSQS = SQS.sample(frac=0.20, random_state=rng)\n",
    "        testMask = pd.Series(True, index=SQS.index)\n",
    "        testMask[trainSQS.index] = False\n",
    "        test = SQS[testMask].copy()\n",
    "\n",
    "        \n",
    "        subTrainX = trainSQS[feats]\n",
    "        subTrainY = trainSQS['class']\n",
    "        \n",
    "        trainX, testX = X.iloc[train_index], X.iloc[test_index]\n",
    "        trainY, testY = y.iloc[train_index], y.iloc[test_index]\n",
    "        trainX = pd.concat([trainX,subTrainX])\n",
    "        trainY = pd.concat([trainY,subTrainY])\n",
    "        trainX = trainX.fillna(0)\n",
    "        tunePipe.fit(trainX, trainY)\n",
    "\n",
    "        testAccOva +=  accuracy_score(test['class'], tunePipe.predict(test[feats]))\n",
    "        tn, fp, fn, tp = confusion_matrix(test['class'], tunePipe.predict(test[feats])).ravel()\n",
    "        tnOva += tn\n",
    "        fpOva += fp\n",
    "        fnOva += fn\n",
    "        tpOva += tp\n",
    "        outputAll.append(tunePipe.predict(test[feats]))\n",
    "\n",
    "        \n",
    "    pickle.dump( outputAll, open( \"Pickles/OutputAll\" + str(featType) + \"SQS.p\", \"wb\" ) )\n",
    "\n",
    "   \n",
    "    results = [featType, round(testAccOva/5,3), tnOva/5, fpOva/5, round(((tnOva/5)/((tnOva/5)+ (fpOva/5))),3), fnOva/5, tpOva/5,round(((tpOva/5)/((tpOva/5)+ (fnOva/5))),3),]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data Set and Parameters\n",
    "\n",
    "The following block of code loads up the features of the data set and the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWC = pickle.load( open( \"Pickles/SWCFeatNoTune.p\", \"rb\" ) )\n",
    "SQS =  pickle.load( open( \"../FeatureExtraction/DataSets/SQSFeatures/SQSFeat.p\", \"rb\" ) )\n",
    "bestParameters = pickle.load( open( \"Pickles/BestParam.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Feature Sets\n",
    "\n",
    "The following blocks of code define feature sets that we perform our ablation study on. We choose to seperate this code into several blocks for the sake of legibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(SQS.columns)\n",
    "features.remove('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "P3Feats = ['ld',\n",
    " 'ls1',\n",
    " 'ls2',\n",
    " 'vs1',\n",
    " 'vs2',\n",
    " 'cvs1',\n",
    " 'ndw',\n",
    " 'ttr',\n",
    " 'cttr',\n",
    " 'rttr',\n",
    " 'logttr',\n",
    " 'lv',\n",
    " 'vv1',\n",
    " 'svv1',\n",
    " 'cvv1',\n",
    " 'vv2',\n",
    " 'nv',\n",
    " 'adjv',\n",
    " 'numSpellingErrors',\n",
    " 'offByOne',\n",
    " 'kidsError',\n",
    " 'coreVocab',\n",
    " 'nonCoreVocab',\n",
    " 'minAoA',\n",
    " 'maxAoA',\n",
    " 'ratioAoA',\n",
    " 'queryComplexity',\n",
    " 'SVEN',\n",
    " 'top250SterCount',\n",
    " 'top250SterRatAnt',\n",
    " 'top250SterRatCon',\n",
    " 'top250NonSterCount',\n",
    " 'top250NonSterRatAnt',\n",
    " 'top250NonSterRatCon',\n",
    " 'top50SterCount',\n",
    " 'top50SterRatAnt',\n",
    " 'top50SterAntCon',\n",
    " 'top50NonSterCount',\n",
    " 'top50NonSterRatAnt',\n",
    " 'top50NonSterAntCon',\n",
    " 'tfidfAll',\n",
    " 'tfidfS',\n",
    " 'tfidfNS'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DC1Feats = ['ndw',\n",
    " 'ttr',\n",
    " 'cttr',\n",
    " 'rttr',\n",
    " 'logttr',\n",
    " 'lv',\n",
    " 'vv1',\n",
    " 'svv1',\n",
    " 'cvv1',\n",
    " 'vv2',\n",
    " 'nv',\n",
    " 'adjv',\n",
    " 'totalSyl',\n",
    " 'avgSyl',\n",
    " 'simWords',\n",
    " 'comWords',\n",
    " 'greatestSyl',\n",
    " 'leastSyl',\n",
    " 'numChars',\n",
    " 'numWords',\n",
    " 'avgLenWord',\n",
    " 'minAoA',\n",
    " 'maxAoA',\n",
    " 'queryComplexity',\n",
    " 'stopCount',\n",
    " 'com',\n",
    " 'net',\n",
    " 'org',\n",
    " 'edu',\n",
    " 'gov',\n",
    " 'http',\n",
    " 'AND',\n",
    " 'OR',\n",
    " 'quotes',\n",
    " 'inter',\n",
    " 'numSpellingErrors',\n",
    " 'offByOne',\n",
    " 'kidsError',\n",
    " 'punct',\n",
    " 'casing',\n",
    " ' Level0',\n",
    " ' Level1',\n",
    " ' Level2',\n",
    " ' Level3',\n",
    " ' Level4',\n",
    " ' Level5',\n",
    " ' Level6',\n",
    " ' Level7',\n",
    " ' MeanLevel',\n",
    " 'cc',\n",
    " 'cd',\n",
    " 'dt',\n",
    " 'ex',\n",
    " 'fw',\n",
    " 'in',\n",
    " 'jj',\n",
    " 'jjr',\n",
    " 'jjs',\n",
    " 'md',\n",
    " 'nn',\n",
    " 'nnp',\n",
    " 'nnps',\n",
    " 'nns',\n",
    " 'pdt',\n",
    " 'pos',\n",
    " 'prp',\n",
    " 'rb',\n",
    " 'rbr',\n",
    " 'rbs',\n",
    " 'rp',\n",
    " 'sym',\n",
    " 'to',\n",
    " 'uh',\n",
    " 'vb',\n",
    " 'vbd',\n",
    " 'vbg',\n",
    " 'vbn',\n",
    " 'vbp',\n",
    " 'vbz',\n",
    " 'wdt',\n",
    " 'wp',\n",
    " 'wrb',\n",
    " 'nn nn',\n",
    " 'jj nn',\n",
    " 'nn nns',\n",
    " 'to vb',\n",
    " 'jj nns',\n",
    " 'jj to',\n",
    " 'nn in',\n",
    " 'nns in',\n",
    " 'in nn',\n",
    " 'dt nn',\n",
    " 'jj nn nn',\n",
    " 'nn nn nn',\n",
    " 'jj to vb',\n",
    " 'nn nn nns',\n",
    " 'to vb nn',\n",
    " 'repeatClicks',\n",
    " 'clickDistance',\n",
    " 'meanClickPosition',\n",
    " 'numClicks',\n",
    " 'numClicksPerQuery',\n",
    " 'numQueries',\n",
    " 'timeClicks',\n",
    " 'uniqueQueries',\n",
    " 'allSameClicks',\n",
    " 'uniqueClicks',\n",
    " 'allSameQueries',\n",
    " 'queryDistance',\n",
    " 'timeQueries',\n",
    " 'repeatQueries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextFeat = ['cc',\n",
    " 'cd',\n",
    " 'dt',\n",
    " 'ex',\n",
    " 'fw',\n",
    " 'in',\n",
    " 'jj',\n",
    " 'jjr',\n",
    " 'jjs',\n",
    " 'md',\n",
    " 'nn',\n",
    " 'nnp',\n",
    " 'nnps',\n",
    " 'nns',\n",
    " 'pdt',\n",
    " 'pos',\n",
    " 'prp',\n",
    " 'rb',\n",
    " 'rbr',\n",
    " 'rbs',\n",
    " 'rp',\n",
    " 'sym',\n",
    " 'to',\n",
    " 'uh',\n",
    " 'vb',\n",
    " 'vbd',\n",
    " 'vbg',\n",
    " 'vbn',\n",
    " 'vbp',\n",
    " 'vbz',\n",
    " 'wdt',\n",
    " 'wp',\n",
    " 'wrb',\n",
    " 'nn nn',\n",
    " 'jj nn',\n",
    " 'nn nns',\n",
    " 'to vb',\n",
    " 'jj nns',\n",
    " 'jj to',\n",
    " 'nn in',\n",
    " 'nns in',\n",
    " 'in nn',\n",
    " 'dt nn',\n",
    " 'jj nn nn',\n",
    " 'nn nn nn',\n",
    " 'jj to vb',\n",
    " 'nn nn nns',\n",
    " 'to vb nn',\n",
    " ' Level0',\n",
    " ' Level1',\n",
    " ' Level2',\n",
    " ' Level3',\n",
    " ' Level4',\n",
    " ' Level5',\n",
    " ' Level6',\n",
    " ' Level7',\n",
    " ' MeanLevel',\n",
    " 'totalSyl',\n",
    " 'avgSyl',\n",
    " 'simWords',\n",
    " 'comWords',\n",
    " 'greatestSyl',\n",
    " 'leastSyl',\n",
    " 'numChars',\n",
    " 'numWords',\n",
    " 'avgLenWord',\n",
    " 'ld',\n",
    " 'ls1',\n",
    " 'ls2',\n",
    " 'vs1',\n",
    " 'vs2',\n",
    " 'cvs1',\n",
    " 'ndw',\n",
    " 'ttr',\n",
    " 'cttr',\n",
    " 'rttr',\n",
    " 'logttr',\n",
    " 'lv',\n",
    " 'vv1',\n",
    " 'svv1',\n",
    " 'cvv1',\n",
    " 'vv2',\n",
    " 'nv',\n",
    " 'adjv',\n",
    " 'numSpellingErrors',\n",
    " 'offByOne',\n",
    " 'kidsError',\n",
    " 'punct',\n",
    " 'casing',\n",
    " 'coreVocab',\n",
    " 'nonCoreVocab',\n",
    " 'minAoA',\n",
    " 'maxAoA',\n",
    " 'ratioAoA',\n",
    " 'queryComplexity',\n",
    " 'SVEN',\n",
    " 'top250SterCount',\n",
    " 'top250SterRatAnt',\n",
    " 'top250SterRatCon',\n",
    " 'top250NonSterCount',\n",
    " 'top250NonSterRatAnt',\n",
    " 'top250NonSterRatCon',\n",
    " 'top50SterCount',\n",
    " 'top50SterRatAnt',\n",
    " 'top50SterAntCon',\n",
    " 'top50NonSterCount',\n",
    " 'top50NonSterRatAnt',\n",
    " 'top50NonSterAntCon',\n",
    " 'tfidfAll',\n",
    " 'tfidfS',\n",
    " 'tfidfNS',\n",
    " 'stopCount',\n",
    " 'com',\n",
    " 'net',\n",
    " 'org',\n",
    " 'edu',\n",
    " 'gov',\n",
    " 'http',\n",
    " 'AND',\n",
    " 'OR',\n",
    " 'quotes',\n",
    " 'inter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SessionFeats = ['repeatClicks',\n",
    " 'clickDistance',\n",
    " 'meanClickPosition',\n",
    " 'numClicks',\n",
    " 'numClicksPerQuery',\n",
    " 'numQueries',\n",
    " 'timeClicks',\n",
    " 'uniqueQueries',\n",
    " 'allSameClicks',\n",
    " 'uniqueClicks',\n",
    " 'allSameQueries',\n",
    " 'queryDistance',\n",
    " 'timeQueries',\n",
    " 'repeatQueries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Ablation Study\n",
    "\n",
    "In the following block of code we perform the ablation study and store all the results in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RYSe = recognizeSearcher(bestParameters, features, 'RYSe')\n",
    "P3Results = recognizeSearcher(bestParameters, P3Feats, 'P3')\n",
    "DC1Results = recognizeSearcher(bestParameters, DC1Feats, 'DC1')\n",
    "textResults = recognizeSearcher(bestParameters, TextFeat, 'TextBased')\n",
    "sessionResults = recognizeSearcher(bestParameters, SessionFeats, 'SessionBased')\n",
    "\n",
    "allResults = pd.DataFrame(data = [RYSe,P3Results,DC1Results,textResults,sessionResults  ], \n",
    "                          columns = [\"Type\",\"Acc\", \"TN\", \"FP\", \"TNR\", \"FN\", \"TP\", \"TPR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results\n",
    "\n",
    "In the following block of code we save those results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( allResults, open( \"Pickles/AblationSQS.p\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
