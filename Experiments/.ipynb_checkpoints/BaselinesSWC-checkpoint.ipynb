{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "from sklearn import model_selection \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from nltk.util import ngrams\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowIndex(sentence):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    return lda_model[dictionary.doc2bow(tokens)] \n",
    "\n",
    "def generateNgrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSessions = pickle.load( open( \"Pickles/SWCNoTune.p\", \"rb\" ) )\n",
    "allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "allSessionsA = allSessionsQ.loc[allSessionsQ['class']==1].groupby('sID')['query'].apply(pd.Series.tolist).tolist()\n",
    "allSessionsNA = allSessionsQ.loc[allSessionsQ['class']==0].groupby('sID')['query'].apply(pd.Series.tolist).tolist()\n",
    "allSessionsSQS = pickle.load( open( \"../Data/DataSets/SQS/SQS.p\", \"rb\" ) )\n",
    "SQSA = allSessionsSQS[allSessionsSQS['class']==1]['query'].tolist()\n",
    "SQSNA = allSessionsSQS[allSessionsSQS['class']==0]['query'].tolist()\n",
    "ovaResults = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "newList = {}\n",
    "newList['entry']=[]\n",
    "newList['class']=[]\n",
    "\n",
    "for session in allSessionsA:\n",
    "    string = \"\"\n",
    "    for query in session:\n",
    "        string += query + \" \"\n",
    "    newList['entry'].append(string)\n",
    "    #newList['tag'].append('kids')\n",
    "    newList['class'].append(1)\n",
    "\n",
    "\n",
    "for session in allSessionsNA:\n",
    "    string = \"\"\n",
    "    for query in session:\n",
    "        string += query + \" \"\n",
    "    newList['entry'].append(string)\n",
    "    newList['class'].append(0)\n",
    "\n",
    "data = pd.DataFrame(newList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bob the builder bob the builder bob the builder</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>toolband.com craigslist.org</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spanish to english translation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>black party lines racism racism usa racism in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>prom hair styles prom hair styles prom hair st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32991</th>\n",
       "      <td>\"space sounds like\" astronauts \"space sounds l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32992</th>\n",
       "      <td>\"pest control\" how to \"pest control\" how to \"p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32993</th>\n",
       "      <td>how accurate are home pregnancy tests how accu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32994</th>\n",
       "      <td>history of crossword puzzle history of crosswo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32995</th>\n",
       "      <td>\"hospital anxiety and depression scale\" scales...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32996 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   entry  class\n",
       "0       bob the builder bob the builder bob the builder       1\n",
       "1                           toolband.com craigslist.org       1\n",
       "2                        spanish to english translation       1\n",
       "3      black party lines racism racism usa racism in ...      1\n",
       "4      prom hair styles prom hair styles prom hair st...      1\n",
       "...                                                  ...    ...\n",
       "32991  \"space sounds like\" astronauts \"space sounds l...      0\n",
       "32992  \"pest control\" how to \"pest control\" how to \"p...      0\n",
       "32993  how accurate are home pregnancy tests how accu...      0\n",
       "32994  history of crossword puzzle history of crosswo...      0\n",
       "32995  \"hospital anxiety and depression scale\" scales...      0\n",
       "\n",
       "[32996 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "nSplits = 5\n",
    "kfold = KFold(n_splits=nSplits, random_state=20210530, shuffle=True)\n",
    "\n",
    "data['prediction'] = 0\n",
    "\n",
    "tnOva = 0\n",
    "fpOva = 0\n",
    "fnOva = 0\n",
    "tpOva = 0\n",
    "testAccOva = 0\n",
    "outputMaj = []\n",
    "trI = []\n",
    "tI = []\n",
    "for train_index, test_index in kfold.split(data):\n",
    "    trI.append(train_index)\n",
    "    tI.append(test_index)\n",
    "    test = data.iloc[test_index]\n",
    "    outputMaj.append(test['prediction'].tolist())\n",
    "    testAccOva +=  accuracy_score(test['class'], test['prediction'])\n",
    "    tn, fp, fn, tp = confusion_matrix( test['class'], test['prediction']).ravel()\n",
    "    tnOva += tn\n",
    "    fpOva += fp\n",
    "    fnOva += fn\n",
    "    tpOva += tp\n",
    "pickle.dump( outputMaj, open( \"Pickles/OutputMajoritySWC.p\", \"wb\" ) )\n",
    "majorityResults = ['Majority', round(testAccOva/nSplits,3), tnOva/nSplits, fpOva/nSplits, round(((tnOva/nSplits)/((tnOva/nSplits)+ (fpOva/nSplits))),3), fnOva/nSplits, tpOva/nSplits,round(((tpOva/nSplits)/((tpOva/nSplits)+ (fnOva/nSplits))),3),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Majority', 0.8, 5280.2, 0.0, 1.0, 1319.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majorityResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovaResults.append(majorityResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7980\n"
     ]
    }
   ],
   "source": [
    "\n",
    "arch = np.asarray(pickle.load( open( \"../Data/DataSets/SWC/sterComplete.p\", \"rb\" ) ))\n",
    "notArch = np.asarray(pickle.load( open( \"../Data/DataSets/SWC/notSterComplete.p\", \"rb\" ) ))\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "print(len(arch))\n",
    "documents = []\n",
    "sessionDuration = []\n",
    "seshClass = []\n",
    "for session in arch:\n",
    "    #print(session)\n",
    "    sites = []\n",
    "    doc = \"\"\n",
    "    for query in session:\n",
    "        #print(query[1])\n",
    "        #if not query[3]:\n",
    "            sites.append(query[4])\n",
    "    documents.append(sites)\n",
    "    length = session[len(session)-1][2] \n",
    "    sessionDuration.append(length)\n",
    "    seshClass.append(1)\n",
    "for session in notArch:\n",
    "    sites = []\n",
    "    doc = \"\"\n",
    "    for query in session:\n",
    "        #print(query[1])\n",
    "        #if not query[3]:\n",
    "        sites.append(query[4])\n",
    "    documents.append(sites)\n",
    "    if ':' not in str(session[0][2]):\n",
    "        sessionDuration.append(float(session[len(session)-1][2]) - float(session[0][2]))\n",
    "    else:\n",
    "        length = int(datetime.timestamp(datetime.strptime(session[len(session)-1][2], \"%H:%M:%S.%f\")) - datetime.timestamp(datetime.strptime(session[0][2], \"%H:%M:%S.%f\")))\n",
    "        sessionDuration.append(length)\n",
    "    seshClass.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data=documents)\n",
    "data = data[data.columns[1:]].apply(\n",
    "    lambda x: ' '.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "data = pd.DataFrame(data, columns=['sites'])\n",
    "\n",
    "data['duration'] = sessionDuration\n",
    "data['class'] = seshClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sites</th>\n",
       "      <th>duration</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://nickjr.co.uk http://www.hitentertainme...</td>\n",
       "      <td>259.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.craigslist.org</td>\n",
       "      <td>157.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://forums.civfanatics.com http://www.socia...</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://spanish-translator.580it.com http://www...</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.syvum.com http://www.wordreference....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39895</th>\n",
       "      <td>http://coe.berkeley.edu/news-center/publicatio...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39896</th>\n",
       "      <td>http://www.doyourownpestcontrol.com/silverfish...</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39897</th>\n",
       "      <td>http://www.whattoexpect.com/preconception/fert...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39898</th>\n",
       "      <td>http://crosswordtournament.com/more/wynne.html...</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39899</th>\n",
       "      <td>http://idacc.healthbase.info/questionnaires.html</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39900 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   sites  duration  class\n",
       "0       http://nickjr.co.uk http://www.hitentertainme...     259.0      1\n",
       "1                              http://www.craigslist.org     157.0      1\n",
       "2      http://forums.civfanatics.com http://www.socia...     170.0      1\n",
       "3      http://spanish-translator.580it.com http://www...     102.0      1\n",
       "4      http://www.syvum.com http://www.wordreference....       0.0      1\n",
       "...                                                  ...       ...    ...\n",
       "39895  http://coe.berkeley.edu/news-center/publicatio...      20.0      0\n",
       "39896  http://www.doyourownpestcontrol.com/silverfish...     168.0      0\n",
       "39897  http://www.whattoexpect.com/preconception/fert...      30.0      0\n",
       "39898  http://crosswordtournament.com/more/wynne.html...     141.0      0\n",
       "39899   http://idacc.healthbase.info/questionnaires.html       7.0      0\n",
       "\n",
       "[39900 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sites = data['sites'].tolist()\n",
    "Duration = data['duration'].tolist()\n",
    "\n",
    "durationClassification = []\n",
    "\n",
    "for sD in Duration:\n",
    "    try:\n",
    "        if float(sD) <= (60*30):\n",
    "            durationClassification.append(1)\n",
    "        else:\n",
    "            durationClassification.append(0)    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "kids_sites = []\n",
    "with open('../Data/DataSources/DMOZ/URL Classification.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    line_count = -1\n",
    "    for row in csv_reader:\n",
    "        \n",
    "        ## consume first line\n",
    "        if( 'Kids' in row[2]):\n",
    "            #print(row)\n",
    "            kids_sites.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39900/39900 [03:43<00:00, 178.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "siteClassification = []\n",
    "with tqdm(total=len(Sites)) as pbar:\n",
    "    for visitedSites in Sites:\n",
    "        found = 0\n",
    "        for kidsSite in kids_sites:\n",
    "            if kidsSite in visitedSites:\n",
    "                siteClassification.append(1)\n",
    "                found = 1\n",
    "                break\n",
    "        if found == 0:\n",
    "            siteClassification.append(0)\n",
    "        pbar.update()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['durationClassification'] = durationClassification\n",
    "data['siteClassification'] = siteClassification\n",
    "data['prediction'] = data['durationClassification'] & data['siteClassification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.index.isin(allSessions.sID)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnOva = 0\n",
    "fpOva = 0\n",
    "fnOva = 0\n",
    "tpOva = 0\n",
    "testAccOva = 0\n",
    "outputDT = []\n",
    "for train_index, test_index in kfold.split(data):\n",
    "    test = data.iloc[test_index]\n",
    "    outputDT.append( test['prediction'].tolist())\n",
    "    testAccOva +=  accuracy_score(test['class'], test['prediction'])\n",
    "    tn, fp, fn, tp = confusion_matrix(test['class'], test['prediction']).ravel()\n",
    "    tnOva += tn\n",
    "    fpOva += fp\n",
    "    fnOva += fn\n",
    "    tpOva += tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( outputDT, open( \"Pickles/OutputRuleSWC.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruleResults = ['Rule', round(testAccOva/nSplits,3), tnOva/nSplits, fpOva/nSplits, round(((tnOva/nSplits)/((tnOva/nSplits)+ (fpOva/nSplits))),3), fnOva/nSplits, tpOva/nSplits,round(((tpOva/nSplits)/((tpOva/nSplits)+ (fnOva/nSplits))),3),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rule', 0.904, 5272.0, 8.2, 0.998, 628.6, 690.4, 0.523]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruleResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovaResults.append(ruleResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "newList = {}\n",
    "newList['entry']=[]\n",
    "newList['tag']=[]\n",
    "\n",
    "\n",
    "for session in allSessionsA:\n",
    "    string = \"\"\n",
    "    for query in session:\n",
    "        string += query + \" \"\n",
    "    newList['entry'].append(string)\n",
    "    #newList['tag'].append('kids')\n",
    "    newList['tag'].append(1)\n",
    "\n",
    "\n",
    "for session in allSessionsNA:\n",
    "    string = \"\"\n",
    "    for query in session:\n",
    "        string += query + \" \"\n",
    "    newList['entry'].append(string)\n",
    "    newList['tag'].append(0)\n",
    "\n",
    "newList = pd.DataFrame(data = newList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rNum = 20210530    \n",
    "rng = np.random.RandomState(rNum)\n",
    "\n",
    "newList2 = {}\n",
    "newList2['entry']=[]\n",
    "newList2['tag']=[]\n",
    "\n",
    "for x in range((len(SQSA))+(len(SQSNA))):\n",
    "    if( x < (len(SQSA))):\n",
    "        newList2['tag'].append(1)\n",
    "    else:\n",
    "        newList2['tag'].append(0)\n",
    "\n",
    "\n",
    "for x in range((len(SQSA))+(len(SQSNA))):\n",
    "    if( x < (len(SQSA))):\n",
    "        newList2['entry'].append(SQSA[x])\n",
    "\n",
    "    else:\n",
    "        newList2['entry'].append(SQSNA[x-(len(SQSA))])\n",
    "\n",
    "data2 = pd.DataFrame(data = newList2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(20210414)\n",
    "tnOva = 0\n",
    "fpOva = 0\n",
    "fnOva = 0\n",
    "tpOva = 0\n",
    "testAccOva = 0\n",
    "outputTam = []\n",
    "randomNumber = 20210530\n",
    "for train_index, test_index in kfold.split(newList):\n",
    "    Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(newList['entry'],newList['tag'],test_size=0.2, random_state=rng)\n",
    "    Train_X, Test_X = newList.iloc[train_index]['entry'], newList.iloc[test_index]['entry']\n",
    "    Train_Y, Test_Y = newList.iloc[train_index]['tag'], newList.iloc[test_index]['tag']\n",
    "    \n",
    "    rng = np.random.RandomState(randomNumber)\n",
    "    test = data2.sample(frac=0.20, random_state=rng)\n",
    "    train_mask = pd.Series(True, index=data2.index)\n",
    "    train_mask[test.index] = False\n",
    "    train = data2[train_mask].copy()\n",
    "\n",
    "    Train_X = pd.concat([Train_X, test['entry']])\n",
    "    Train_Y = pd.concat([Train_Y, test['tag']])\n",
    "    Encoder = LabelEncoder()\n",
    "    Train_Y = Encoder.fit_transform(Train_Y)\n",
    "    Test_Y = Encoder.fit_transform(Test_Y)\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(3, 3))\n",
    "    vectorizer.fit_transform(newList['entry'])\n",
    "    Train_X_Tfidf = vectorizer.transform(Train_X)\n",
    "    Test_X_Tfidf = vectorizer.transform(Test_X)\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "    outputTam.append(predictions_SVM)\n",
    "    testAccOva += accuracy_score(predictions_SVM, Test_Y)\n",
    "    tn, fp, fn, tp = confusion_matrix(Test_Y, predictions_SVM).ravel()\n",
    "    tnOva += tn\n",
    "    fpOva += fp\n",
    "    fnOva += fn\n",
    "    tpOva += tp\n",
    "    randomNumber+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8575888403660793, 5254.0, 26.2, 913.6, 405.4]\n"
     ]
    }
   ],
   "source": [
    "print([testAccOva/nSplits, tnOva/nSplits, fpOva/nSplits, fnOva/nSplits,  tpOva/nSplits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "textBasedResults = ['Text', round(testAccOva/nSplits,3), tnOva/nSplits, fpOva/nSplits, round(((tnOva/nSplits)/((tnOva/nSplits)+ (fpOva/nSplits))),3), fnOva/nSplits, tpOva/nSplits,round(((tpOva/nSplits)/((tpOva/nSplits)+ (fnOva/nSplits))),3),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( outputTam, open( \"Pickles/OutputTextSWC.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovaResults.append(textBasedResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "classList = []\n",
    "numQ = []\n",
    "for session in allSessionsA:\n",
    "    numQueries = 0\n",
    "    doc = \"\"\n",
    "    for query in session:\n",
    "        doc += \" \" + query\n",
    "        numQueries +=1\n",
    "    documents.append(doc)\n",
    "    classList.append(1)\n",
    "    numQ.append(numQueries)\n",
    "    \n",
    "for session in allSessionsNA:\n",
    "    numQueries = 0\n",
    "    doc = \"\"\n",
    "    for query in session:\n",
    "        doc += \" \" + query\n",
    "        numQueries +=1\n",
    "    documents.append(doc)\n",
    "    classList.append(0)\n",
    "    numQ.append(numQueries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(documents, columns = ['text'])\n",
    "posData = []\n",
    "for document in documents:\n",
    "    text = nltk.word_tokenize(document)\n",
    "    tags = np.array(nltk.pos_tag(text)).flatten()\n",
    "    posData.append(tags[1::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pos'] = posData\n",
    "data['numQ'] = numQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docUni = []\n",
    "docBi = []\n",
    "docTri = []\n",
    "\n",
    "for document in documents:\n",
    "    doc = generateNgrams(document,1)\n",
    "    docUni.append(doc)\n",
    "    \n",
    "for document in documents:\n",
    "    doc = generateNgrams(document,2)\n",
    "    docBi.append(doc)\n",
    "    \n",
    "for document in documents:\n",
    "    doc = generateNgrams(document,3)\n",
    "    docTri.append(doc)\n",
    "    \n",
    "data['uniWord'] = docUni\n",
    "data['biWord'] = docBi\n",
    "data['triWord']= docTri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "posMod = []\n",
    "\n",
    "for pos in posData: \n",
    "    string = []\n",
    "    for entry in pos:\n",
    "        string += str(entry) + \" \"\n",
    "    posMod.append(\"\".join(string))\n",
    "\n",
    "    \n",
    "posUni = []\n",
    "posBi = []\n",
    "posTri = []\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generateNgrams(document,1)\n",
    "    posUni.append(doc)\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generateNgrams(document,2)\n",
    "    posBi.append(doc)\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generateNgrams(document,3)\n",
    "    posTri.append(doc)  \n",
    "    \n",
    "data['uniPos'] = posUni\n",
    "data['biPos'] = posBi\n",
    "data['triPos']= posTri\n",
    "data['posMod'] = posMod\n",
    "data['class'] = classList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "text = \"\".join(data['text'].tolist())\n",
    "token = word_tokenize(text)\n",
    "\n",
    "unigrams = list(ngrams(token, 1))\n",
    "uniwords = collections.Counter(unigrams)\n",
    "n = len(uniwords)-40000\n",
    "stopWordsUni = uniwords.most_common()[:-n-1:-1]\n",
    "\n",
    "bigrams = list(ngrams(token, 2))\n",
    "biwords = collections.Counter(bigrams)\n",
    "n = len(biwords)-40000\n",
    "stopWordsBi =  biwords.most_common()[:-n-1:-1]\n",
    "\n",
    "trigrams = list(ngrams(token, 3))\n",
    "triwords = collections.Counter(trigrams)\n",
    "n = len(triwords)-40000\n",
    "stopWordsTri = triwords.most_common()[:-n-1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "classList = []\n",
    "\n",
    "for query in SQSA:\n",
    "    doc = \"\"\n",
    "    doc += \" \" + query\n",
    "    documents.append(doc)\n",
    "    classList.append(1)\n",
    " \n",
    "for query in SQSNA:\n",
    "    doc = \"\"   \n",
    "    doc += \" \" + query\n",
    "    documents.append(doc)\n",
    "    classList.append(0)\n",
    "\n",
    "data2 = pd.DataFrame(documents, columns = ['text'])\n",
    "\n",
    "posData = []\n",
    "for document in documents:\n",
    "    text = nltk.word_tokenize(document)\n",
    "    tags = np.array(nltk.pos_tag(text)).flatten()\n",
    "    posData.append(tags[1::2])\n",
    "    \n",
    "data2['pos'] = posData\n",
    "\n",
    "docUni = []\n",
    "docBi = []\n",
    "docTri = []\n",
    "\n",
    "for document in documents:\n",
    "    doc = generateNgrams(document,1)\n",
    "    docUni.append(doc)\n",
    "    \n",
    "for document in documents:\n",
    "    doc = generateNgrams(document,2)\n",
    "    docBi.append(doc)\n",
    "    \n",
    "for document in documents:\n",
    "    doc = generateNgrams(document,3)\n",
    "    docTri.append(doc)\n",
    "    \n",
    "data2['uniWord'] = docUni\n",
    "data2['biWord'] = docBi\n",
    "data2['triWord']= docTri\n",
    "\n",
    "posMod = []\n",
    "\n",
    "for pos in posData: \n",
    "    string = []\n",
    "    for entry in pos:\n",
    "        string += str(entry) + \" \"\n",
    "    posMod.append(\"\".join(string))\n",
    "\n",
    "    \n",
    "posUni = []\n",
    "posBi = []\n",
    "posTri = []\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generateNgrams(document,1)\n",
    "    posUni.append(doc)\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generateNgrams(document,2)\n",
    "    posBi.append(doc)\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generateNgrams(document,3)\n",
    "    posTri.append(doc)  \n",
    "    \n",
    "data2['uniPos'] = posUni\n",
    "data2['biPos'] = posBi\n",
    "data2['triPos']= posTri\n",
    "data2['posMod'] = posMod\n",
    "data2['class'] = classList\n",
    "data2['numQ'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Part One done\n",
      "Part Two Done\n",
      "start\n",
      "Part One done\n",
      "Part Two Done\n",
      "start\n",
      "Part One done\n",
      "Part Two Done\n",
      "start\n",
      "Part One done\n",
      "Part Two Done\n",
      "start\n",
      "Part One done\n",
      "Part Two Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "FlatSingle = pd.DataFrame(data= data2)\n",
    "rn = 20210530\n",
    "\n",
    "nSplits = 5\n",
    "tnOva = 0\n",
    "fpOva = 0\n",
    "fnOva = 0\n",
    "tpOva = 0\n",
    "testAccOva = 0\n",
    "x= 0\n",
    "accByNum = []\n",
    "kfold = KFold(n_splits=nSplits, random_state=20210530, shuffle=True)\n",
    "outputSan = []\n",
    "accByNum = []\n",
    "outputQ = []\n",
    "for train_index, test_index in kfold.split(data):\n",
    "    \n",
    "    accNum = []\n",
    "    train = data.iloc[train_index]\n",
    "    test = data.iloc[test_index]\n",
    "    \n",
    "    rng = np.random.RandomState(rn)\n",
    "    testSing = data2.sample(frac=0.20, random_state=rng)\n",
    "    train_mask = pd.Series(True, index=data2.index)\n",
    "    train_mask[testSing.index] = False\n",
    "    trainSing = data2[train_mask].copy()\n",
    "\n",
    "\n",
    "#     print(len(trainSing))\n",
    "    train.head()\n",
    "    train = pd.concat([train,testSing])\n",
    "#     print(len(train))\n",
    "#     print(len(test))\n",
    "    subtest = train.sample(frac=0.20, random_state=rng)\n",
    "    subtrain_mask = pd.Series(True, index=train.index)\n",
    "    subtrain_mask[subtest.index] = False\n",
    "    subtrain = train[subtrain_mask].copy()\n",
    "    subtrain.head()\n",
    "\n",
    "\n",
    "    print('start')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    subtest = train.sample(frac=0.20, random_state=rng)\n",
    "    subtrain_mask = pd.Series(True, index=train.index)\n",
    "    subtrain_mask[subtest.index] = False\n",
    "    subtrain = train[subtrain_mask].copy()\n",
    "    subtrain.head()\n",
    "#     print(subtest)\n",
    "    Encoder = LabelEncoder()\n",
    "    Train_Y = Encoder.fit_transform(subtrain['class'])\n",
    "    Test_Y = Encoder.fit_transform(subtest['class'])\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words=stopWordsUni)\n",
    "    vectorizer.fit_transform(train['text'])\n",
    "    Train_X_Tfidf = vectorizer.transform(subtrain['text'])\n",
    "    Test_X_Tfidf = vectorizer.transform(subtest['text'])\n",
    "\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "    predictions_SVM_Uni = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    Test_X_Tfidf = vectorizer.transform(test['text'])\n",
    "    test_predictions_SVM_uni = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    Encoder = LabelEncoder()\n",
    "    Train_Y = Encoder.fit_transform(subtrain['class'])\n",
    "    Test_Y = Encoder.fit_transform(subtest['class'])\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2),stop_words=stopWordsBi)\n",
    "    vectorizer.fit_transform(train['text'])\n",
    "    Train_X_Tfidf = vectorizer.transform(subtrain['text'])\n",
    "    Test_X_Tfidf = vectorizer.transform(subtest['text'])\n",
    "\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "    predictions_SVM_bi = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    Test_X_Tfidf = vectorizer.transform(test['text'])\n",
    "    test_predictions_SVM_bi = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    Encoder = LabelEncoder()\n",
    "    Train_Y = Encoder.fit_transform(subtrain['class'])\n",
    "    Test_Y = Encoder.fit_transform(subtest['class'])\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(3, 3), stop_words=stopWordsTri)\n",
    "    vectorizer.fit_transform(train['text'])\n",
    "    Train_X_Tfidf = vectorizer.transform(subtrain['text'])\n",
    "    Test_X_Tfidf = vectorizer.transform(subtest['text'])\n",
    "\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "    predictions_SVM_tri = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    Test_X_Tfidf = vectorizer.transform(test['text'])\n",
    "    test_predictions_SVM_tri = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    Encoder = LabelEncoder()\n",
    "    Train_Y = Encoder.fit_transform(subtrain['class'])\n",
    "    Test_Y = Encoder.fit_transform(subtest['class'])\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit_transform(train['posMod'])\n",
    "    Train_X_Tfidf = vectorizer.transform(subtrain['posMod'])\n",
    "    Test_X_Tfidf = vectorizer.transform(subtest['posMod'])\n",
    "\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "    predictions_SVM_pos_uni = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    Test_X_Tfidf = vectorizer.transform(test['posMod'])\n",
    "    test_predictions_SVM_pos_uni = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    Encoder = LabelEncoder()\n",
    "    Train_Y = Encoder.fit_transform(subtrain['class'])\n",
    "    Test_Y = Encoder.fit_transform(subtest['class'])\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "    vectorizer.fit_transform(train['posMod'])\n",
    "    Train_X_Tfidf = vectorizer.transform(subtrain['posMod'])\n",
    "    Test_X_Tfidf = vectorizer.transform(subtest['posMod'])\n",
    "\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "    predictions_SVM_pos_bi = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    Test_X_Tfidf = vectorizer.transform(test['posMod'])\n",
    "    test_predictions_SVM_pos_bi = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    Encoder = LabelEncoder()\n",
    "    Train_Y = Encoder.fit_transform(subtrain['class'])\n",
    "    Test_Y = Encoder.fit_transform(subtest['class'])\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(3, 3))\n",
    "    vectorizer.fit_transform(train['posMod'])\n",
    "    Train_X_Tfidf = vectorizer.transform(subtrain['posMod'])\n",
    "    Test_X_Tfidf = vectorizer.transform(subtest['posMod'])\n",
    "\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "    predictions_SVM_pos_tri = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "\n",
    "    Test_X_Tfidf = vectorizer.transform(test['posMod'])\n",
    "    test_predictions_SVM_pos_tri = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    \n",
    "    LDAtest = train.sample(frac=0.4, random_state=rng)\n",
    "    LDAtrain_mask = pd.Series(True, index=train.index)\n",
    "    LDAtrain_mask[LDAtest.index] = False\n",
    "    LDAtrain = train[LDAtrain_mask].copy()\n",
    "\n",
    "\n",
    "    processed_docs = LDAtrain['uniWord']\n",
    "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "    lda_model = models.LdaModel(corpus, num_topics=200, \\\n",
    "                                      id2word=dictionary, \\\n",
    "                                      passes=4, alpha=[0.01]*200, \\\n",
    "                                      eta=[0.01]*len(dictionary.keys()))\n",
    "\n",
    "    subtest['ldaOneOverall'] = subtest['text'].apply(lambda x: rowIndex(x))\n",
    "    test['ldaOneOverall'] = test['text'].apply(lambda x: rowIndex(x))\n",
    "    LDAtest['ldaOneOverall'] = LDAtest['text'].apply(lambda x: rowIndex(x))\n",
    "\n",
    "    ldaOOList = LDAtest['ldaOneOverall'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(200):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "\n",
    "    LDAtest['ldaOneOverallPad'] =  ldaAllPad\n",
    "\n",
    "    ldaOOList = test['ldaOneOverall'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(200):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    test['ldaOneOverallPad'] =  ldaAllPad\n",
    "\n",
    "    ldaOOList = subtest['ldaOneOverall'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(200):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    subtest['ldaOneOverallPad'] =  ldaAllPad\n",
    "\n",
    "    processed_docs = LDAtrain.loc[LDAtrain['class'] ==1]['uniWord']\n",
    "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "    lda_model = models.LdaModel(corpus, num_topics=100, \\\n",
    "                                      id2word=dictionary, \\\n",
    "                                      passes=4, alpha=[0.01]*100, \\\n",
    "                                      eta=[0.01]*len(dictionary.keys()))\n",
    "\n",
    "    subtest['ldaOneTrue'] = subtest['text'].apply(lambda x: rowIndex(x))\n",
    "    test['ldaOneTrue'] = test['text'].apply(lambda x: rowIndex(x))\n",
    "    LDAtest['ldaOneTrue'] = LDAtest['text'].apply(lambda x: rowIndex(x))\n",
    "\n",
    "    ldaOOList = LDAtest['ldaOneTrue'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(100):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    \n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    LDAtest['ldaOneTrue'] =  ldaAllPad\n",
    "\n",
    "    ldaOOList = test['ldaOneTrue'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(100):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    test['ldaOneTrue'] =  ldaAllPad\n",
    "\n",
    "    ldaOOList = subtest['ldaOneTrue'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(100):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    \n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    subtest['ldaOneTrue'] =  ldaAllPad\n",
    "\n",
    "    processed_docs = LDAtrain.loc[LDAtrain['class'] ==0]['uniWord']\n",
    "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "    lda_model = models.LdaModel(corpus, num_topics=100, \\\n",
    "                                      id2word=dictionary, \\\n",
    "                                      passes=4, alpha=[0.01]*100, \\\n",
    "                                      eta=[0.01]*len(dictionary.keys()))\n",
    "\n",
    "    subtest['ldaOneFalse'] = subtest['text'].apply(lambda x: rowIndex(x))\n",
    "    test['ldaOneFalse'] = test['text'].apply(lambda x: rowIndex(x))\n",
    "    LDAtest['ldaOneFalse'] = LDAtest['text'].apply(lambda x: rowIndex(x))\n",
    "\n",
    "    ldaOOList = LDAtest['ldaOneFalse'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(100):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    \n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    LDAtest['ldaOneFalse'] =  ldaAllPad\n",
    "\n",
    "    ldaOOList = test['ldaOneFalse'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(100):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    \n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    test['ldaOneFalse'] =  ldaAllPad\n",
    "\n",
    "    ldaOOList = subtest['ldaOneFalse'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(100):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    \n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    subtest['ldaOneFalse'] =  ldaAllPad\n",
    "\n",
    "    LDAtest['LDAClassOne'] = LDAtest['ldaOneTrue'] + LDAtest['ldaOneFalse']\n",
    "    test['LDAClassOne'] = test['ldaOneTrue'] + test['ldaOneFalse'] \n",
    "    subtest['LDAClassOne'] = subtest['ldaOneTrue'] + subtest['ldaOneFalse'] \n",
    "\n",
    "    LDAtest['LDAallOne'] = LDAtest['ldaOneOverallPad'] + LDAtest['LDAClassOne']\n",
    "    test['LDAallOne'] = test['ldaOneOverallPad'] + test['LDAClassOne'] \n",
    "    subtest['LDAallOne'] = subtest['ldaOneOverallPad'] + subtest['LDAClassOne'] \n",
    "\n",
    "    clf = LogisticRegression(random_state=0, solver = 'liblinear').fit(list(LDAtest['ldaOneOverallPad']),LDAtest['class'])\n",
    "\n",
    "    resultsOne = clf.predict(list(subtest['ldaOneOverallPad']))\n",
    "    testresultsOne = clf.predict(list(test['ldaOneOverallPad']))\n",
    "\n",
    "    clf = LogisticRegression(random_state=0, solver = 'liblinear').fit(list(LDAtest['LDAClassOne']),LDAtest['class'])\n",
    "\n",
    "    resultsTwo = clf.predict(list(subtest['LDAClassOne']))\n",
    "    testresultsTwo = clf.predict(list(test['LDAClassOne']))\n",
    "\n",
    "    clf = LogisticRegression(random_state=0, solver = 'liblinear').fit(list(LDAtest['LDAallOne']),LDAtest['class'])\n",
    "\n",
    "    resultsThree = clf.predict(list(subtest['LDAallOne']))\n",
    "    testresultsThree = clf.predict(list(test['LDAallOne']))\n",
    "    print(\"Part Two Done\")\n",
    "    subtest['SVMUni'] = predictions_SVM_Uni\n",
    "    subtest['SVMBi'] = predictions_SVM_bi\n",
    "    subtest['SVMTri'] = predictions_SVM_tri\n",
    "    subtest['SVMUniPOS'] = predictions_SVM_pos_uni\n",
    "    subtest['SVMBiPOS'] = predictions_SVM_pos_bi\n",
    "    subtest['SVMTriPOS'] = predictions_SVM_pos_tri\n",
    "    subtest['SVMUniLDA'] = resultsOne\n",
    "    subtest['SVMBiLDA'] = resultsTwo\n",
    "    subtest['SVMTriLDA'] = resultsThree\n",
    "\n",
    "    test['SVMUni'] = test_predictions_SVM_uni\n",
    "    test['SVMBi'] = test_predictions_SVM_bi\n",
    "    test['SVMTri'] = test_predictions_SVM_tri\n",
    "    test['SVMUniPOS'] = test_predictions_SVM_pos_uni\n",
    "    test['SVMBiPOS'] = test_predictions_SVM_pos_bi\n",
    "    test['SVMTriPOS'] = test_predictions_SVM_pos_tri\n",
    "    test['SVMUniLDA'] = testresultsOne\n",
    "    test['SVMBiLDA'] = testresultsTwo\n",
    "    test['SVMTriLDA'] = testresultsThree\n",
    "\n",
    "\n",
    "    feat_cols = [\n",
    "        'SVMUni',\n",
    "        'SVMBi',\n",
    "        'SVMTri',\n",
    "        'SVMUniPOS',\n",
    "        'SVMBiPOS',\n",
    "        'SVMTriPOS',\n",
    "        'SVMUniLDA',\n",
    "        'SVMBiLDA',\n",
    "        'SVMTriLDA',\n",
    "    ]\n",
    "\n",
    "    out_col = 'class'\n",
    "\n",
    "    train_x = subtest[feat_cols]\n",
    "    train_y = subtest[out_col]\n",
    "    test_x = test[feat_cols]\n",
    "    test_y = test[out_col]\n",
    "\n",
    "    pure_pipeL2 = Pipeline([\n",
    "        ('standardize', StandardScaler()),\n",
    "        ('classify', DecisionTreeClassifier(criterion='entropy',random_state= 20210518, class_weight = 'balanced'))\n",
    "        ])\n",
    "\n",
    "    pure_pipeL2.fit(train_x, train_y)\n",
    "\n",
    "    prediction = pure_pipeL2.predict(test_x)\n",
    "    testAccOva += accuracy_score(test_y, prediction)\n",
    "    outputSan.append(prediction)\n",
    "    SanAQtn, SanAQfp, SanAQfn, SanAQtp = confusion_matrix(test_y, prediction).ravel()\n",
    "    tnOva += SanAQtn\n",
    "    fpOva += SanAQfp\n",
    "    fnOva += SanAQfn\n",
    "    tpOva += SanAQtp\n",
    "    X_test = test_x\n",
    "    X_test['predict'] = prediction\n",
    "    X_test['numQ'] = test['numQ']\n",
    "    X_test['class'] = test['class']\n",
    "    test = X_test\n",
    "    output = []\n",
    "    \n",
    "    for x in range(5):\n",
    "        testAcc = test[test['numQ'] == (x+1)]\n",
    "        acc = accuracy_score(testAcc['predict'], testAcc['class'])\n",
    "        output.append(testAcc['predict'].tolist())\n",
    "        SELtp = (testAcc[(testAcc['predict']  == 1) & (testAcc['class'] == 1)].index.tolist())\n",
    "        tn, fp, fn, tp = confusion_matrix( testAcc['class'], testAcc['predict']).ravel()\n",
    "        accNum.append([acc, tn, fp, fn, tp])\n",
    "        \n",
    "    testAcc = test[test['numQ'] > (5)]\n",
    "    acc = accuracy_score(testAcc['predict'], testAcc['class'])\n",
    "    output.append(testAcc['predict'].tolist())\n",
    "    SELtp = (testAcc[(testAcc['predict']  == 1) & (testAcc['class'] == 1)].index.tolist())\n",
    "    tn, fp, fn, tp = confusion_matrix( testAcc['class'], testAcc['predict']).ravel()\n",
    "    \n",
    "    accNum.append([acc, tn, fp, fn, tp])\n",
    "    accByNum.append(accNum)\n",
    "    outputQ.append(output)\n",
    "    \n",
    "    rn+=1\n",
    "    x+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1 = []\n",
    "acc2 = []\n",
    "acc3 = []\n",
    "acc4 = []\n",
    "acc5 = []\n",
    "acc6 = []\n",
    "\n",
    "for accBy in accByNum:\n",
    "    x = 0\n",
    "    for acc in accBy:\n",
    "        if(x == 0):\n",
    "            acc1.append(acc)\n",
    "        elif(x == 1):\n",
    "            acc2.append(acc)\n",
    "        elif(x == 2):\n",
    "            acc3.append(acc)\n",
    "        elif(x == 3):\n",
    "            acc4.append(acc)\n",
    "        elif(x == 4):\n",
    "            acc5.append(acc)\n",
    "        elif(x == 5):\n",
    "            acc6.append(acc)\n",
    "        x+=1\n",
    "        np.set_printoptions(suppress=True)\n",
    "\n",
    "acc1 = np.array(acc1)\n",
    "acc1 = acc1.mean(axis = 0)\n",
    "acc2 = np.array(acc2)\n",
    "acc2 = acc2.mean(axis = 0)\n",
    "acc3 = np.array(acc3)\n",
    "acc3 = acc3.mean(axis = 0)\n",
    "acc4 = np.array(acc4)\n",
    "acc4 = acc4.mean(axis = 0)\n",
    "acc5 = np.array(acc5)\n",
    "acc5 = acc5.mean(axis = 0)\n",
    "acc6 = np.array(acc6)\n",
    "acc6 = acc6.mean(axis = 0)\n",
    "accAll = [acc1,acc2,acc3,acc4,acc5,acc6]\n",
    "accAll = pd.DataFrame(data = accAll, columns = ['Acc','TN','FP','FN','TP'])\n",
    "accAll['TNR'] = round((accAll['TN']/(accAll['TN']+accAll['FP'])),3)\n",
    "accAll['TPR'] = round((accAll['TP']/(accAll['TP']+accAll['FN'])),3)\n",
    "accAll = accAll.reset_index()\n",
    "accAll = accAll.rename(columns = {\"index\" :\"numQ\"})\n",
    "accAll['numQ'] = accAll['numQ']+1\n",
    "accAll['numQ'] = accAll['numQ'].astype(str)\n",
    "accAll['numQ'].replace({\"6\": \"6+\"}, inplace=True)\n",
    "accAll = accAll.set_index('numQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>TNR</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numQ</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.939762</td>\n",
       "      <td>1875.8</td>\n",
       "      <td>37.4</td>\n",
       "      <td>118.8</td>\n",
       "      <td>561.2</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.923256</td>\n",
       "      <td>1001.2</td>\n",
       "      <td>46.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.919235</td>\n",
       "      <td>584.0</td>\n",
       "      <td>37.2</td>\n",
       "      <td>23.8</td>\n",
       "      <td>109.6</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.904040</td>\n",
       "      <td>370.0</td>\n",
       "      <td>27.6</td>\n",
       "      <td>16.6</td>\n",
       "      <td>46.8</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.901528</td>\n",
       "      <td>248.0</td>\n",
       "      <td>24.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>26.4</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6+</th>\n",
       "      <td>0.858468</td>\n",
       "      <td>663.2</td>\n",
       "      <td>102.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>28.4</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Acc      TN     FP     FN     TP    TNR    TPR\n",
       "numQ                                                     \n",
       "1     0.939762  1875.8   37.4  118.8  561.2  0.980  0.825\n",
       "2     0.923256  1001.2   46.0   59.0  260.0  0.956  0.815\n",
       "3     0.919235   584.0   37.2   23.8  109.6  0.940  0.822\n",
       "4     0.904040   370.0   27.6   16.6   46.8  0.931  0.738\n",
       "5     0.901528   248.0   24.4    5.6   26.4  0.910  0.825\n",
       "6+    0.858468   663.2  102.6   11.4   28.4  0.866  0.714"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( outputSan, open( \"Pickles/OutputMultiFeatureSWC.p\", \"wb\" ) )\n",
    "pickle.dump( accAll, open( \"Pickles/OutputMultiModelSWCAccByQ.p\", \"wb\" ) )\n",
    "pickle.dump( outputQ, open( \"Pickles/OutputMultiModelSWCResByQ.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiFeatureResults = ['MultiFeature', round(testAccOva/nSplits,3), tnOva/nSplits, fpOva/nSplits, round(((tnOva/nSplits)/((tnOva/nSplits)+ (fpOva/nSplits))),3), fnOva/nSplits, tpOva/nSplits,round(((tpOva/nSplits)/((tpOva/nSplits)+ (fnOva/nSplits))),3),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovaResults.append(multiFeatureResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MultiFeature', 0.919, 4742.2, 275.2, 0.945, 235.2, 1032.4, 0.814]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiFeatureResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nemati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Data/DataSets/SQS/SQSA.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-332188915404>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mkidsAll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mallSessionsK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mTRECS\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mallSessionsA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mkidsSolo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"../Data/DataSets/SQS/SQSA.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mmillQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"../Data/DataSets/SQS/SQSNA.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Data/DataSets/SQS/SQSA.p'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "nSplits = 5\n",
    "kfold = KFold(n_splits=nSplits, random_state=20210530, shuffle=True)\n",
    "\n",
    "allSessions = pickle.load( open( \"Pickles/SWCNoTune.p\", \"rb\" ) )\n",
    "allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "allSessionsK = allSessionsQ.loc[allSessionsQ['class']==1]\n",
    "allSessionsA = allSessionsQ.loc[allSessionsQ['class']==0]\n",
    "\n",
    "kidsAll = allSessionsK.groupby('sID')['query'].apply(pd.Series.tolist).tolist()\n",
    "TRECS =  allSessionsA.groupby('sID')['query'].apply(pd.Series.tolist).tolist()\n",
    "\n",
    "kidsSolo = np.asarray(pickle.load( open( \"../Data/DataSets/SQS/SQSA.p\", \"rb\" ) ))\n",
    "millQ = np.asarray(pickle.load( open( \"../Data/DataSets/SQS/SQSNA.p\", \"rb\" ) ))\n",
    "\n",
    "newList = {}\n",
    "newList['entry']=[]\n",
    "newList['class']=[]\n",
    "newList['numQ']=[]\n",
    "\n",
    "for session in kidsAll:\n",
    "    string = \"\"\n",
    "    numQ = 0\n",
    "    for query in session:\n",
    "        string += query + \" \"\n",
    "        numQ += 1\n",
    "    newList['entry'].append(string)\n",
    "    #newList['tag'].append('kids')\n",
    "    newList['class'].append(1)\n",
    "    newList['numQ'].append(numQ)\n",
    "\n",
    "\n",
    "for session in TRECS:\n",
    "    string = \"\"\n",
    "    numQ = 0\n",
    "    for query in session:\n",
    "        string += query + \" \"\n",
    "        numQ += 1\n",
    "    newList['entry'].append(string)\n",
    "    #newList['tag'].append('kids')\n",
    "    newList['class'].append(0)\n",
    "    newList['numQ'].append(numQ)\n",
    "data = pd.DataFrame(newList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Y1 = []\n",
    "for x in range((301 + 1204)):\n",
    "    if( x < (301)):\n",
    "        Test_Y1.append(1)\n",
    "    else:\n",
    "        Test_Y1.append(0)\n",
    "Test_X2 = []\n",
    "for x in range((301 + 1204)):\n",
    "    if( x < (301)):\n",
    "        Test_X2.append(kidsSolo[x])\n",
    "    else:\n",
    "        Test_X2.append(millQ[x-301])\n",
    "        \n",
    "single = pd.DataFrame(data =[Test_Y1,Test_X2]).T\n",
    "single.rename(columns ={0:'class', 1:'entry'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    ('lr', make_pipeline(CountVectorizer(),\n",
    "                           LogisticRegressionCV(solver = 'liblinear', max_iter=1000))),\n",
    "    ('nb', make_pipeline(TfidfVectorizer(),\n",
    "                          MultinomialNB(alpha = .13))),\n",
    "    ('gb', make_pipeline(CountVectorizer(),\n",
    "                            GradientBoostingClassifier(learning_rate = .2, max_depth = 2))),\n",
    "    ('mlp', make_pipeline(CountVectorizer(),\n",
    "                           MLPClassifier(hidden_layer_sizes= 21, max_iter=1500, shuffle=False, tol=0.012 ))),   \n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf1 = VotingClassifier(estimators=estimators, voting='hard')\n",
    "\n",
    "tnOva = 0\n",
    "fpOva = 0\n",
    "fnOva = 0\n",
    "tpOva = 0\n",
    "testAccOva = 0\n",
    "accByNum = []\n",
    "outputNem = []\n",
    "outputQ = []\n",
    "randomNumber = 20210530\n",
    "for train_index, test_index in kfold.split(data):\n",
    "    accNum = []\n",
    "    X_train, X_test = data.iloc[train_index][['entry','numQ','class']], data.iloc[test_index][['entry','numQ','class']]\n",
    "    y_train, y_test = data.iloc[train_index]['class'], data.iloc[test_index]['class']\n",
    "#     print(X_train)\n",
    "    rng = np.random.RandomState(randomNumber)\n",
    "    test = single.sample(frac=0.20, random_state=rng)\n",
    "    train_mask = pd.Series(True, index=single.index)\n",
    "    train_mask[test.index] = False\n",
    "    train = single[train_mask].copy()\n",
    "    train.head()\n",
    "    \n",
    "    X_train = pd.concat([X_train, test])\n",
    "    y_train = pd.concat([y_train, test['class']])\n",
    "#     print(X_train)\n",
    "    eclf1 = eclf1.fit(X_train['entry'].tolist(), y_train.tolist())\n",
    "    testAccOva +=  accuracy_score(eclf1.predict(X_test['entry']), y_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, eclf1.predict(X_test['entry'])).ravel()\n",
    "    tnOva += tn\n",
    "    fpOva += fp\n",
    "    fnOva += fn\n",
    "    tpOva += tp\n",
    "    X_test['predict'] = eclf1.predict(X_test['entry'])\n",
    "    outputNem.append(X_test['predict'].tolist())\n",
    "#     test = X_test\n",
    "#     output = []\n",
    "#     for x in range(5):\n",
    "#         testAcc = test[test['numQ'] == (x+1)]\n",
    "#         acc = accuracy_score(testAcc['predict'], testAcc['class'])\n",
    "#         output.append(testAcc['predict'].tolist())\n",
    "#         SELtp = (testAcc[(testAcc['predict']  == 1) & (testAcc['class'] == 1)].index.tolist())\n",
    "#         tn, fp, fn, tp = confusion_matrix(testAcc['predict'], testAcc['class']).ravel()\n",
    "#         accNum.append([acc, tn, fp, fn, tp])\n",
    "#     testAcc = test[test['numQ'] > (5)]\n",
    "#     acc = accuracy_score(testAcc['predict'], testAcc['class'])\n",
    "#     output.append(testAcc['predict'].tolist())\n",
    "#     SELtp = (testAcc[(testAcc['predict']  == 1) & (testAcc['class'] == 1)].index.tolist())\n",
    "#     tn, fp, fn, tp = confusion_matrix(testAcc['predict'], testAcc['class']).ravel()\n",
    "#     accNum.append([acc, tn, fp, (tn/(tn+fp)), fn, tp, (tp/(tp+ff))])\n",
    "#     accByNum.append(accNum)\n",
    "#     outputQ.append(output)\n",
    "    randomNumber+=1\n",
    "#     print(randomNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( outputNem, open( \"Pickles/OutputMultiModelSWC.p\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiModelResults = ['MultiModel', round(testAccOva/nSplits,3), tnOva/nSplits, fpOva/nSplits, round(((tnOva/nSplits)/((tnOva/nSplits)+ (fpOva/nSplits))),3), fnOva/nSplits, tpOva/nSplits,round(((tpOva/nSplits)/((tpOva/nSplits)+ (fnOva/nSplits))),3),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multiModelResults)\n",
    "ovaResults.append(multiModelResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allResults = pd.DataFrame(data = ovaResults, columns = [\"Type\",\"Acc\", \"TN\", \"FP\", \"TNR\", \"FN\", \"TP\", \"TPR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( allResults, open( \"Pickles/BaselineSWC.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
