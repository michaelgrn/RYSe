{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we run our baselines on the SQS data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries\n",
    "\n",
    "In the following block of code we import the libraries used in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bl4z3/miniconda3/lib/python3.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "import csv\n",
    "import collections\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from nltk.util import ngrams\n",
    "from gensim import corpora, models\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare Functions\n",
    "\n",
    "In the following block of code we declare the functions we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowIndex(sentence):\n",
    "    # Tokenizes a sentence that is cast to lower\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    \n",
    "    # Retuns the LDA values for those tokens\n",
    "    return ldaModel[dictionary.doc2bow(tokens)] \n",
    "\n",
    "def generateNgrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data Sets and Preprocess\n",
    "\n",
    "In the following block of code loads and preprocesses our data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSessions = pickle.load( open( \"Pickles/SWCNoTune.p\", \"rb\" ) )\n",
    "allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "\n",
    "allSessionsS = allSessionsQ.loc[allSessionsQ['class']==1].groupby('sID')['query'].apply(pd.Series.tolist).tolist()\n",
    "allSessionsNS = allSessionsQ.loc[allSessionsQ['class']==0].groupby('sID')['query'].apply(pd.Series.tolist).tolist()\n",
    "\n",
    "allSessionsSQS = pickle.load( open( \"../Data/DataSets/SQS/SQS.p\", \"rb\" ) )\n",
    "SQSS = allSessionsSQS[allSessionsSQS['class']==1]['query'].tolist()\n",
    "SQSNS = allSessionsSQS[allSessionsSQS['class']==0]['query'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables\n",
    "\n",
    "In the following block of code we declare our global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovaResults = []\n",
    "nSplits = 5\n",
    "kfold = KFold(n_splits=nSplits, random_state=20210530, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority\n",
    "\n",
    "In the following block of code we run our first baseline, the Majority classifier. This classifier classifies all users as the majority class, not our stereotype. We employ this as a baseline due to the difference in number of users who do, and do not; belong to our stereotype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "newList = {}\n",
    "newList['class']=[]\n",
    "\n",
    "for session in SQSS:\n",
    "    newList['class'].append(1)\n",
    "\n",
    "\n",
    "for session in SQSNS:\n",
    "    newList['class'].append(0)\n",
    "\n",
    "data = pd.DataFrame(newList)\n",
    "data['prediction'] = 0\n",
    "\n",
    "tnOva = 0\n",
    "fpOva = 0\n",
    "fnOva = 0\n",
    "tpOva = 0\n",
    "\n",
    "testAccOva = 0\n",
    "\n",
    "outputMaj = []\n",
    "\n",
    "trI = []\n",
    "tI = []\n",
    "\n",
    "for train_index, test_index in kfold.split(data):\n",
    "    \n",
    "    trI.append(train_index)\n",
    "    tI.append(test_index)\n",
    "    \n",
    "    test = data.iloc[test_index]\n",
    "    \n",
    "    outputMaj.append(test['prediction'].tolist())\n",
    "    \n",
    "    testAccOva +=  accuracy_score(test['class'], test['prediction'])\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix( test['class'], test['prediction']).ravel()\n",
    "    \n",
    "    tnOva += tn\n",
    "    fpOva += fp\n",
    "    fnOva += fn\n",
    "    tpOva += tp\n",
    "\n",
    "\n",
    "majorityResults = ['Majority', round(testAccOva/nSplits,3), tnOva/nSplits, fpOva/nSplits, round(((tnOva/nSplits)/((tnOva/nSplits)+ (fpOva/nSplits))),3), fnOva/nSplits, tpOva/nSplits,round(((tpOva/nSplits)/((tpOva/nSplits)+ (fnOva/nSplits))),3),]\n",
    "\n",
    "ovaResults.append(majorityResults)\n",
    "\n",
    "pickle.dump( outputMaj, open( \"Pickles/OutputMajoritySQS.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Based\n",
    "\n",
    "The following block of code is runs our second baseline. Drawing inspiration from \"An analysis of queries intended to search information for children\" (https://dl.acm.org/doi/10.1145/1840784.1840819), we classifiy a session as belonging to our stereotype if it us under a certain length of time and has a click on a site designated for children based on the DMOZ tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ster = SQSS\n",
    "notSter = SQSNS\n",
    "\n",
    "documents = []\n",
    "sessionDuration = []\n",
    "seshClass = []\n",
    "\n",
    "for session in ster:\n",
    "\n",
    "    sites = []\n",
    "    doc = \"\"\n",
    "    documents.append(sites)\n",
    "    length = 0\n",
    "    sessionDuration.append(length)\n",
    "    seshClass.append(1)\n",
    "    \n",
    "for session in notSter:\n",
    "        #print(session)\n",
    "    sites = []\n",
    "    doc = \"\"\n",
    "    documents.append(sites)\n",
    "    length = 0\n",
    "    sessionDuration.append(length)\n",
    "    seshClass.append(0)\n",
    "    \n",
    "data = pd.DataFrame(data=documents)\n",
    "data = data[data.columns[1:]].apply(\n",
    "    lambda x: ' '.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "data = pd.DataFrame(data, columns=['sites'])\n",
    "\n",
    "data['duration'] = sessionDuration\n",
    "data['class'] = seshClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1505/1505 [00:11<00:00, 135.52it/s]\n"
     ]
    }
   ],
   "source": [
    "sites = data['sites'].tolist()\n",
    "duration = data['duration'].tolist()\n",
    "\n",
    "durationClassification = []\n",
    "\n",
    "for sD in duration:\n",
    "    try:\n",
    "        if float(sD) <= (60*30):\n",
    "            durationClassification.append(1)\n",
    "        else:\n",
    "            durationClassification.append(0)    \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "kidsSites = []\n",
    "\n",
    "with open('../Data/DataSources/DMOZ/URL Classification.csv') as csv_file:\n",
    "    csvReader = csv.reader(csv_file)\n",
    "    lineCount = -1\n",
    "    for row in csvReader:\n",
    "\n",
    "        if( 'Kids' in row[2]):\n",
    "\n",
    "            kidsSites.append(row[1])\n",
    "            \n",
    "siteClassification = []\n",
    "\n",
    "with tqdm(total=len(sites)) as pbar:\n",
    "    for visitedSites in sites:\n",
    "        found = 0\n",
    "        for kidsSite in kidsSites:\n",
    "            if kidsSite in visitedSites:\n",
    "                siteClassification.append(1)\n",
    "                found = 1\n",
    "                break\n",
    "        if found == 0:\n",
    "            siteClassification.append(0)\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['durationClassification'] = durationClassification\n",
    "data['siteClassification'] = siteClassification\n",
    "data['prediction'] = data['durationClassification'] & data['siteClassification']\n",
    "\n",
    "data = data[data.index.isin(allSessions.sID)]\n",
    "\n",
    "tnOva = 0\n",
    "fpOva = 0\n",
    "fnOva = 0\n",
    "tpOva = 0\n",
    "\n",
    "testAccOva = 0\n",
    "\n",
    "outputDT = []\n",
    "\n",
    "for train_index, test_index in kfold.split(data):\n",
    "    test = data.iloc[test_index]\n",
    "    outputDT.append( test['prediction'].tolist())\n",
    "    testAccOva +=  accuracy_score(test['class'], test['prediction'])\n",
    "    tn, fp, fn, tp = confusion_matrix(test['class'], test['prediction']).ravel()\n",
    "    tnOva += tn\n",
    "    fpOva += fp\n",
    "    fnOva += fn\n",
    "    tpOva += tp\n",
    "    \n",
    "ruleResults = ['Rule', round(testAccOva/nSplits,3), tnOva/nSplits, fpOva/nSplits, round(((tnOva/nSplits)/((tnOva/nSplits)+ (fpOva/nSplits))),3), fnOva/nSplits, tpOva/nSplits,round(((tpOva/nSplits)/((tpOva/nSplits)+ (fnOva/nSplits))),3),]\n",
    "\n",
    "ovaResults.append(ruleResults)\n",
    "\n",
    "pickle.dump( outputDT, open( \"Pickles/OutputRuleSQS.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Based\n",
    "\n",
    "In the following blocks of code we implement the text based classifier proposed in \"Age detection in chat\" (https://ieeexplore.ieee.org/document/5298540), concatenating the queries of all our sessions and performing text based classification to recognize whether or not a session was generated by a user who belongs to our stereotype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "newList = {}\n",
    "newList['entry']=[]\n",
    "newList['tag']=[]\n",
    "\n",
    "\n",
    "for session in allSessionsS:\n",
    "    string = \"\"\n",
    "    for query in session:\n",
    "        string += query + \" \"\n",
    "    newList['entry'].append(string)\n",
    "    newList['tag'].append(1)\n",
    "\n",
    "\n",
    "for session in allSessionsNS:\n",
    "    string = \"\"\n",
    "    for query in session:\n",
    "        string += query + \" \"\n",
    "    newList['entry'].append(string)\n",
    "    newList['tag'].append(0)\n",
    "\n",
    "newList = pd.DataFrame(data = newList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "newList2 = {}\n",
    "newList2['entry']=[]\n",
    "newList2['tag']=[]\n",
    "\n",
    "for x in range((len(SQSS))+(len(SQSNS))):\n",
    "    if( x < (len(SQSS))):\n",
    "        newList2['tag'].append(1)\n",
    "    else:\n",
    "        newList2['tag'].append(0)\n",
    "\n",
    "\n",
    "for x in range((len(SQSS))+(len(SQSNS))):\n",
    "    if( x < (len(SQSS))):\n",
    "        newList2['entry'].append(SQSS[x])\n",
    "\n",
    "    else:\n",
    "        newList2['entry'].append(SQSNS[x-(len(SQSS))])\n",
    "\n",
    "data2 = pd.DataFrame(data = newList2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(20210414)\n",
    "\n",
    "tnOva = 0\n",
    "fpOva = 0\n",
    "fnOva = 0\n",
    "tpOva = 0\n",
    "\n",
    "testAccOva = 0\n",
    "\n",
    "outputTam = []\n",
    "\n",
    "randomNumber = 20210530\n",
    "\n",
    "for trainIndex, testIndex in kfold.split(data2):\n",
    "    \n",
    "    trainX, testX = data2.iloc[train_index]['entry'], data2.iloc[test_index]['entry']\n",
    "    trainY, testY = data2.iloc[train_index]['tag'], data2.iloc[test_index]['tag']\n",
    "    \n",
    "    test = newList.sample(frac=0.20, random_state=rng)\n",
    "    trainMask = pd.Series(True, index=newList.index)\n",
    "    trainMask[test.index] = False\n",
    "    train = newList[trainMask].copy()\n",
    "\n",
    "    trainX = pd.concat([trainX, test['entry']])\n",
    "    trainY = pd.concat([trainY, test['tag']])\n",
    "    Encoder = LabelEncoder()\n",
    "    \n",
    "    trainY = Encoder.fit_transform(trainY)\n",
    "    testY = Encoder.fit_transform(testY)\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(3, 3))\n",
    "    vectorizer.fit_transform(newList['entry'])\n",
    "    \n",
    "    trainXTfidf = vectorizer.transform(trainX)\n",
    "    testXTfidf = vectorizer.transform(testX)\n",
    "    \n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', gamma='auto')\n",
    "    \n",
    "    SVM.fit(trainXTfidf,trainY)# predict the labels on validation dataset\n",
    "    \n",
    "    predictionsSVM = SVM.predict(testXTfidf)\n",
    "    \n",
    "    outputTam.append(predictionsSVM)\n",
    "    \n",
    "    testAccOva += accuracy_score(predictionsSVM, testY)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(testY, predictionsSVM).ravel()\n",
    "    \n",
    "    tnOva += tn\n",
    "    fpOva += fp\n",
    "    fnOva += fn\n",
    "    tpOva += tp\n",
    "    \n",
    "    randomNumber+=1\n",
    "    \n",
    "textBasedResults = ['Text', round(testAccOva/nSplits,3), tnOva/nSplits, fpOva/nSplits, round(((tnOva/nSplits)/((tnOva/nSplits)+ (fpOva/nSplits))),3), fnOva/nSplits, tpOva/nSplits,round(((tpOva/nSplits)/((tpOva/nSplits)+ (fnOva/nSplits))),3),]\n",
    "\n",
    "ovaResults.append(textBasedResults)\n",
    "\n",
    "pickle.dump( outputTam, open( \"Pickles/OutputTextSQS.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiFeature\n",
    "\n",
    "In the following blocks of code we re-implement the strategy proposed in \"Author profiling: Predicting age and gender from blogs\" (https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.667.4496&rep=rep1&type=pdf). This process involves extracting a variety of features from the queries found in sessions, training on some of those features and using the rest for classification. We don't break up the large block of code that performs the classification due to the nature of kfold split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "classList = []\n",
    "numQ = []\n",
    "\n",
    "for session in allSessionsS:\n",
    "    numQueries = 0\n",
    "    doc = \"\"\n",
    "    for query in session:\n",
    "        doc += \" \" + query\n",
    "        numQueries +=1\n",
    "    documents.append(doc)\n",
    "    classList.append(1)\n",
    "    numQ.append(numQueries)\n",
    "    \n",
    "for session in allSessionsNS:\n",
    "    numQueries = 0\n",
    "    doc = \"\"\n",
    "    for query in session:\n",
    "        doc += \" \" + query\n",
    "        numQueries +=1\n",
    "    documents.append(doc)\n",
    "    classList.append(0)\n",
    "    numQ.append(numQueries)\n",
    "    \n",
    "data = pd.DataFrame(documents, columns = ['text'])\n",
    "posData = []\n",
    "for document in documents:\n",
    "    text = nltk.word_tokenize(document)\n",
    "    tags = np.array(nltk.pos_tag(text)).flatten()\n",
    "    posData.append(tags[1::2])\n",
    "    \n",
    "data['pos'] = posData\n",
    "data['numQ'] = numQ  \n",
    "\n",
    "docUni = []\n",
    "docBi = []\n",
    "docTri = []\n",
    "\n",
    "for document in documents:\n",
    "    doc = generateNgrams(document,1)\n",
    "    docUni.append(doc)\n",
    "    \n",
    "for document in documents:\n",
    "    doc = generateNgrams(document,2)\n",
    "    docBi.append(doc)\n",
    "    \n",
    "for document in documents:\n",
    "    doc = generateNgrams(document,3)\n",
    "    docTri.append(doc)\n",
    "    \n",
    "data['uniWord'] = docUni\n",
    "data['biWord'] = docBi\n",
    "data['triWord']= docTri\n",
    "\n",
    "posMod = []\n",
    "\n",
    "for pos in posData: \n",
    "    string = []\n",
    "    for entry in pos:\n",
    "        string += str(entry) + \" \"\n",
    "    posMod.append(\"\".join(string))\n",
    "\n",
    "    \n",
    "posUni = []\n",
    "posBi = []\n",
    "posTri = []\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generateNgrams(document,1)\n",
    "    posUni.append(doc)\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generateNgrams(document,2)\n",
    "    posBi.append(doc)\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generateNgrams(document,3)\n",
    "    posTri.append(doc)  \n",
    "    \n",
    "data['uniPos'] = posUni\n",
    "data['biPos'] = posBi\n",
    "data['triPos']= posTri\n",
    "data['posMod'] = posMod\n",
    "data['class'] = classList\n",
    "\n",
    "text = \"\".join(data['text'].tolist())\n",
    "token = word_tokenize(text)\n",
    "\n",
    "unigrams = list(ngrams(token, 1))\n",
    "uniwords = collections.Counter(unigrams)\n",
    "n = len(uniwords)-40000\n",
    "stopWordsUni = uniwords.most_common()[:-n-1:-1]\n",
    "\n",
    "bigrams = list(ngrams(token, 2))\n",
    "biwords = collections.Counter(bigrams)\n",
    "n = len(biwords)-40000\n",
    "stopWordsBi =  biwords.most_common()[:-n-1:-1]\n",
    "\n",
    "trigrams = list(ngrams(token, 3))\n",
    "triwords = collections.Counter(trigrams)\n",
    "n = len(triwords)-40000\n",
    "stopWordsTri = triwords.most_common()[:-n-1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "classList = []\n",
    "\n",
    "for query in SQSS:\n",
    "    doc = \"\"\n",
    "    doc += \" \" + query\n",
    "    documents.append(doc)\n",
    "    classList.append(1)\n",
    " \n",
    "for query in SQSNS:\n",
    "    doc = \"\"   \n",
    "    doc += \" \" + query\n",
    "    documents.append(doc)\n",
    "    classList.append(0)\n",
    "\n",
    "data2 = pd.DataFrame(documents, columns = ['text'])\n",
    "\n",
    "posData = []\n",
    "for document in documents:\n",
    "    text = nltk.word_tokenize(document)\n",
    "    tags = np.array(nltk.pos_tag(text)).flatten()\n",
    "    posData.append(tags[1::2])\n",
    "    \n",
    "data2['pos'] = posData\n",
    "\n",
    "docUni = []\n",
    "docBi = []\n",
    "docTri = []\n",
    "\n",
    "for document in documents:\n",
    "    doc = generateNgrams(document,1)\n",
    "    docUni.append(doc)\n",
    "    \n",
    "for document in documents:\n",
    "    doc = generateNgrams(document,2)\n",
    "    docBi.append(doc)\n",
    "    \n",
    "for document in documents:\n",
    "    doc = generateNgrams(document,3)\n",
    "    docTri.append(doc)\n",
    "    \n",
    "data2['uniWord'] = docUni\n",
    "data2['biWord'] = docBi\n",
    "data2['triWord']= docTri\n",
    "\n",
    "posMod = []\n",
    "\n",
    "for pos in posData: \n",
    "    string = []\n",
    "    for entry in pos:\n",
    "        string += str(entry) + \" \"\n",
    "    posMod.append(\"\".join(string))\n",
    "\n",
    "    \n",
    "posUni = []\n",
    "posBi = []\n",
    "posTri = []\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generateNgrams(document,1)\n",
    "    posUni.append(doc)\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generateNgrams(document,2)\n",
    "    posBi.append(doc)\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generateNgrams(document,3)\n",
    "    posTri.append(doc)  \n",
    "    \n",
    "data2['uniPos'] = posUni\n",
    "data2['biPos'] = posBi\n",
    "data2['triPos']= posTri\n",
    "data2['posMod'] = posMod\n",
    "data2['class'] = classList\n",
    "data2['numQ'] = 1\n",
    "\n",
    "FlatSingle = pd.DataFrame(data= data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "start word-Gram training\n",
      "start PoS-Gram training\n"
     ]
    }
   ],
   "source": [
    "rn = 20210530\n",
    "nSplits = 5\n",
    "tnOva = 0\n",
    "fpOva = 0\n",
    "fnOva = 0\n",
    "tpOva = 0\n",
    "\n",
    "testAccOva = 0\n",
    "x= 0\n",
    "accByNum = []\n",
    "\n",
    "outputSan = []\n",
    "accByNum = []\n",
    "outputQ = []\n",
    "\n",
    "for trainIndex, testIndex in kfold.split(data2):\n",
    "    \n",
    "    \n",
    "    print('start')\n",
    "    \n",
    "    accNum = []\n",
    "    train = data2.iloc[testIndex]\n",
    "    test = data2.iloc[trainIndex]\n",
    "    \n",
    "    rng = np.random.RandomState(rn)\n",
    "    testSing = data.sample(frac=0.20, random_state=rng)\n",
    "    trainMask = pd.Series(True, index=data.index)\n",
    "    trainMask[testSing.index] = False\n",
    "    trainSing = data[trainMask].copy()\n",
    "\n",
    "    train = pd.concat([train,trainSing])\n",
    "    subtest = train.sample(frac=0.20, random_state=rng)\n",
    "    subtrainMask = pd.Series(True, index=train.index)\n",
    "    subtrainMask[subtest.index] = False\n",
    "    subtrain = train[subtrainMask].copy()\n",
    "\n",
    "    ## word-Gram Model Training\n",
    "    print('start word-Gram training')\n",
    "    \n",
    "    Encoder = LabelEncoder()\n",
    "    \n",
    "    trainY = Encoder.fit_transform(subtrain['class'])\n",
    "    testY = Encoder.fit_transform(subtest['class'])\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words=stopWordsUni)\n",
    "    vectorizer.fit_transform(train['text'])\n",
    "    trainXTfidf = vectorizer.transform(subtrain['text'])\n",
    "    testXTfidf = vectorizer.transform(subtest['text'])\n",
    "\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(trainXTfidf,trainY)\n",
    "    predictionsSVMUni = SVM.predict(testXTfidf)\n",
    "\n",
    "    testXTfidf = vectorizer.transform(test['text'])\n",
    "    testPredictionsSVMUni = SVM.predict(testXTfidf)\n",
    "\n",
    "  \n",
    "    trainY = Encoder.fit_transform(subtrain['class'])\n",
    "    testY = Encoder.fit_transform(subtest['class'])\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2),stop_words=stopWordsBi)\n",
    "    vectorizer.fit_transform(train['text'])\n",
    "    trainXTfidf = vectorizer.transform(subtrain['text'])\n",
    "    testXTfidf = vectorizer.transform(subtest['text'])\n",
    "\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(trainXTfidf,trainY)# predict the labels on validation dataset\n",
    "    predictionsSVMBi = SVM.predict(testXTfidf)\n",
    "\n",
    "    testXTfidf = vectorizer.transform(test['text'])\n",
    "    testPredictionsSVMBi = SVM.predict(testXTfidf)\n",
    "\n",
    "    trainY = Encoder.fit_transform(subtrain['class'])\n",
    "    testY = Encoder.fit_transform(subtest['class'])\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(3, 3), stop_words=stopWordsTri)\n",
    "    vectorizer.fit_transform(train['text'])\n",
    "    trainXTfidf = vectorizer.transform(subtrain['text'])\n",
    "    testXTfidf = vectorizer.transform(subtest['text'])\n",
    "\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(trainXTfidf,trainY)# predict the labels on validation dataset\n",
    "    predictionsSVMTri = SVM.predict(testXTfidf)\n",
    "\n",
    "    testXTfidf = vectorizer.transform(test['text'])\n",
    "    testPredictionsSVMTri = SVM.predict(testXTfidf)\n",
    "\n",
    "\n",
    "    ## PoS Gram Model Training\n",
    "    \n",
    "    print('start PoS-Gram training')\n",
    "    \n",
    "    trainY = Encoder.fit_transform(subtrain['class'])\n",
    "    testY = Encoder.fit_transform(subtest['class'])\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit_transform(train['posMod'])\n",
    "    trainXTfidf = vectorizer.transform(subtrain['posMod'])\n",
    "    testXTfidf = vectorizer.transform(subtest['posMod'])\n",
    "\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(trainXTfidf,trainY)# predict the labels on validation dataset\n",
    "    predictionsSVMPosUni = SVM.predict(testXTfidf)\n",
    "\n",
    "    testXTfidf = vectorizer.transform(test['posMod'])\n",
    "    testPredictionsSVMPosUni = SVM.predict(testXTfidf)\n",
    "\n",
    "    trainY = Encoder.fit_transform(subtrain['class'])\n",
    "    testY = Encoder.fit_transform(subtest['class'])\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "    vectorizer.fit_transform(train['posMod'])\n",
    "    trainXTfidf = vectorizer.transform(subtrain['posMod'])\n",
    "    testXTfidf = vectorizer.transform(subtest['posMod'])\n",
    "\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(trainXTfidf,trainY)# predict the labels on validation dataset\n",
    "    predictionsSVMPosBi = SVM.predict(testXTfidf)\n",
    "\n",
    "    testXTfidf = vectorizer.transform(test['posMod'])\n",
    "    testPredictionsSVMPosBi = SVM.predict(testXTfidf)\n",
    "\n",
    "    trainY = Encoder.fit_transform(subtrain['class'])\n",
    "    testY = Encoder.fit_transform(subtest['class'])\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(3, 3))\n",
    "    vectorizer.fit_transform(train['posMod'])\n",
    "    trainXTfidf = vectorizer.transform(subtrain['posMod'])\n",
    "    testXTfidf = vectorizer.transform(subtest['posMod'])\n",
    "\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(trainXTfidf,trainY)# predict the labels on validation dataset\n",
    "    predictionsSVMPosTri = SVM.predict(testXTfidf)\n",
    "\n",
    "\n",
    "    testXTfidf = vectorizer.transform(test['posMod'])\n",
    "    testPredictionsSVMPosTri = SVM.predict(testXTfidf)\n",
    "\n",
    "    \n",
    "    ## LDA \n",
    "    \n",
    "    print('start LDA training')\n",
    "    \n",
    "    LDAtest = train.sample(frac=0.4, random_state=rng)\n",
    "    LDAtrain_mask = pd.Series(True, index=train.index)\n",
    "    LDAtrain_mask[LDAtest.index] = False\n",
    "    LDAtrain = train[LDAtrain_mask].copy()\n",
    "\n",
    "\n",
    "    processedDocs = LDAtrain['uniWord']\n",
    "    dictionary = gensim.corpora.Dictionary(processedDocs)\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processedDocs]\n",
    "    ldaModel = models.LdaModel(corpus, num_topics=200, \\\n",
    "                                      id2word=dictionary, \\\n",
    "                                      passes=4, alpha=[0.01]*200, \\\n",
    "                                      eta=[0.01]*len(dictionary.keys()))\n",
    "\n",
    "    subtest['ldaOneOverall'] = subtest['text'].apply(lambda x: rowIndex(x))\n",
    "    test['ldaOneOverall'] = test['text'].apply(lambda x: rowIndex(x))\n",
    "    LDAtest['ldaOneOverall'] = LDAtest['text'].apply(lambda x: rowIndex(x))\n",
    "\n",
    "    ldaOOList = LDAtest['ldaOneOverall'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(200):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "\n",
    "    LDAtest['ldaOneOverallPad'] =  ldaAllPad\n",
    "\n",
    "    ldaOOList = test['ldaOneOverall'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(200):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    test['ldaOneOverallPad'] =  ldaAllPad\n",
    "\n",
    "    ldaOOList = subtest['ldaOneOverall'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(200):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    subtest['ldaOneOverallPad'] =  ldaAllPad\n",
    "\n",
    "    processedDocs = LDAtrain.loc[LDAtrain['class'] ==1]['uniWord']\n",
    "    dictionary = gensim.corpora.Dictionary(processedDocs)\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processedDocs]\n",
    "    ldaModel = models.LdaModel(corpus, num_topics=100, \\\n",
    "                                      id2word=dictionary, \\\n",
    "                                      passes=4, alpha=[0.01]*100, \\\n",
    "                                      eta=[0.01]*len(dictionary.keys()))\n",
    "\n",
    "    subtest['ldaOneTrue'] = subtest['text'].apply(lambda x: rowIndex(x))\n",
    "    test['ldaOneTrue'] = test['text'].apply(lambda x: rowIndex(x))\n",
    "    LDAtest['ldaOneTrue'] = LDAtest['text'].apply(lambda x: rowIndex(x))\n",
    "\n",
    "    ldaOOList = LDAtest['ldaOneTrue'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(100):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    \n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    LDAtest['ldaOneTrue'] =  ldaAllPad\n",
    "\n",
    "    ldaOOList = test['ldaOneTrue'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(100):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    test['ldaOneTrue'] =  ldaAllPad\n",
    "\n",
    "    ldaOOList = subtest['ldaOneTrue'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(100):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    \n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    subtest['ldaOneTrue'] =  ldaAllPad\n",
    "\n",
    "    processedDocs = LDAtrain.loc[LDAtrain['class'] ==0]['uniWord']\n",
    "    dictionary = gensim.corpora.Dictionary(processedDocs)\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processedDocs]\n",
    "    ldaModel = models.LdaModel(corpus, num_topics=100, \\\n",
    "                                      id2word=dictionary, \\\n",
    "                                      passes=4, alpha=[0.01]*100, \\\n",
    "                                      eta=[0.01]*len(dictionary.keys()))\n",
    "\n",
    "    subtest['ldaOneFalse'] = subtest['text'].apply(lambda x: rowIndex(x))\n",
    "    test['ldaOneFalse'] = test['text'].apply(lambda x: rowIndex(x))\n",
    "    LDAtest['ldaOneFalse'] = LDAtest['text'].apply(lambda x: rowIndex(x))\n",
    "\n",
    "    ldaOOList = LDAtest['ldaOneFalse'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(100):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    \n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    LDAtest['ldaOneFalse'] =  ldaAllPad\n",
    "\n",
    "    ldaOOList = test['ldaOneFalse'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(100):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    \n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    test['ldaOneFalse'] =  ldaAllPad\n",
    "\n",
    "    ldaOOList = subtest['ldaOneFalse'].tolist()\n",
    "\n",
    "    emptyList200 = []\n",
    "\n",
    "    for x in range(100):\n",
    "        emptyList200.append((0))\n",
    "\n",
    "    ldaAllPad = []\n",
    "    \n",
    "    for entry in ldaOOList:\n",
    "        if entry:\n",
    "            newList = emptyList200.copy()\n",
    "            for tupleItem in entry:\n",
    "                newList[tupleItem[0]] = tupleItem[1]\n",
    "            ldaAllPad.append(newList)\n",
    "        else:\n",
    "            ldaAllPad.append(emptyList200)\n",
    "\n",
    "    subtest['ldaOneFalse'] =  ldaAllPad\n",
    "\n",
    "    LDAtest['LDAClassOne'] = LDAtest['ldaOneTrue'] + LDAtest['ldaOneFalse']\n",
    "    test['LDAClassOne'] = test['ldaOneTrue'] + test['ldaOneFalse'] \n",
    "    subtest['LDAClassOne'] = subtest['ldaOneTrue'] + subtest['ldaOneFalse'] \n",
    "\n",
    "    LDAtest['LDAallOne'] = LDAtest['ldaOneOverallPad'] + LDAtest['LDAClassOne']\n",
    "    test['LDAallOne'] = test['ldaOneOverallPad'] + test['LDAClassOne'] \n",
    "    subtest['LDAallOne'] = subtest['ldaOneOverallPad'] + subtest['LDAClassOne'] \n",
    "\n",
    "    clf = LogisticRegression(random_state=0, solver = 'liblinear').fit(list(LDAtest['ldaOneOverallPad']),LDAtest['class'])\n",
    "\n",
    "    resultsOne = clf.predict(list(subtest['ldaOneOverallPad']))\n",
    "    testresultsOne = clf.predict(list(test['ldaOneOverallPad']))\n",
    "\n",
    "    clf = LogisticRegression(random_state=0, solver = 'liblinear').fit(list(LDAtest['LDAClassOne']),LDAtest['class'])\n",
    "\n",
    "    resultsTwo = clf.predict(list(subtest['LDAClassOne']))\n",
    "    testresultsTwo = clf.predict(list(test['LDAClassOne']))\n",
    "\n",
    "    clf = LogisticRegression(random_state=0, solver = 'liblinear').fit(list(LDAtest['LDAallOne']),LDAtest['class'])\n",
    "\n",
    "    resultsThree = clf.predict(list(subtest['LDAallOne']))\n",
    "    testresultsThree = clf.predict(list(test['LDAallOne']))\n",
    "\n",
    "    print('start model testing')\n",
    "    \n",
    "    subtest['SVMUni'] = predictionsSVMUni\n",
    "    subtest['SVMBi'] = predictionsSVMBi\n",
    "    subtest['SVMTri'] = predictionsSVMTri\n",
    "    subtest['SVMUniPOS'] = predictionsSVMPosUni\n",
    "    subtest['SVMBiPOS'] = predictionsSVMPosBi\n",
    "    subtest['SVMTriPOS'] = predictionsSVMPosTri\n",
    "    subtest['SVMUniLDA'] = resultsOne\n",
    "    subtest['SVMBiLDA'] = resultsTwo\n",
    "    subtest['SVMTriLDA'] = resultsThree\n",
    "\n",
    "    test['SVMUni'] = testPredictionsSVMUni\n",
    "    test['SVMBi'] = testPredictionsSVMBi\n",
    "    test['SVMTri'] = testPredictionsSVMTri\n",
    "    test['SVMUniPOS'] = testPredictionsSVMPosUni\n",
    "    test['SVMBiPOS'] = testPredictionsSVMPosBi\n",
    "    test['SVMTriPOS'] = testPredictionsSVMPosTri\n",
    "    test['SVMUniLDA'] = testresultsOne\n",
    "    test['SVMBiLDA'] = testresultsTwo\n",
    "    test['SVMTriLDA'] = testresultsThree\n",
    "\n",
    "\n",
    "    feat_cols = [\n",
    "        'SVMUni',\n",
    "        'SVMBi',\n",
    "        'SVMTri',\n",
    "        'SVMUniPOS',\n",
    "        'SVMBiPOS',\n",
    "        'SVMTriPOS',\n",
    "        'SVMUniLDA',\n",
    "        'SVMBiLDA',\n",
    "        'SVMTriLDA',\n",
    "    ]\n",
    "\n",
    "    out_col = 'class'\n",
    "\n",
    "    trainX = subtest[feat_cols]\n",
    "    trainY = subtest[out_col]\n",
    "    testX = test[feat_cols]\n",
    "    testY = test[out_col]\n",
    "\n",
    "    pipeLine = Pipeline([\n",
    "        ('standardize', StandardScaler()),\n",
    "        ('classify', DecisionTreeClassifier(criterion='entropy',random_state= 20210518, class_weight = 'balanced'))\n",
    "        ])\n",
    "\n",
    "    pipeLine.fit(trainX, trainY)\n",
    "\n",
    "    prediction = pipeLine.predict(testX)\n",
    "    testAccOva += acc\n",
    "    outputSan.append(prediction)\n",
    "    SanAQtn, SanAQfp, SanAQfn, SanAQtp = confusion_matrix(testY, prediction).ravel()\n",
    "    tnOva += SanAQtn\n",
    "    fpOva += SanAQfp\n",
    "    fnOva += SanAQfn\n",
    "    tpOva += SanAQtp\n",
    "\n",
    "    rn+=1\n",
    "    x+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiFeatureResults = ['MultiFeature', round(testAccOva/nSplits,3), tnOva/nSplits, fpOva/nSplits, round(((tnOva/nSplits)/((tnOva/nSplits)+ (fpOva/nSplits))),3), fnOva/nSplits, tpOva/nSplits,round(((tpOva/nSplits)/((tpOva/nSplits)+ (fnOva/nSplits))),3),]\n",
    "\n",
    "ovaResults.append(multiFeatureResults)\n",
    "\n",
    "pickle.dump( outputSan, open( \"Pickles/OutputMultiFeatureSQS.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiModel\n",
    "\n",
    "In the following block of code we implement the classification strategy found in \"Gender and Age Prediction Multilingual Author Profiles Based on Comments\" (http://ceur-ws.org/Vol-2266/T4-4.pdf). This strategy relies on an ensemble classifier to recognize users based on the text found in their sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newList = {}\n",
    "newList['entry']=[]\n",
    "newList['class']=[]\n",
    "\n",
    "for session in allSessionsS:\n",
    "    string = \"\"\n",
    "    numQ = 0\n",
    "    for query in session:\n",
    "        string += query + \" \"\n",
    "        numQ += 1\n",
    "    newList['entry'].append(string)\n",
    "    newList['class'].append(1)\n",
    "\n",
    "\n",
    "\n",
    "for session in allSessionsNS:\n",
    "    string = \"\"\n",
    "    numQ = 0\n",
    "    for query in session:\n",
    "        string += query + \" \"\n",
    "        numQ += 1\n",
    "    newList['entry'].append(string)\n",
    "    newList['class'].append(0)\n",
    "\n",
    "data = pd.DataFrame(newList)\n",
    "\n",
    "SQSY = []\n",
    "\n",
    "for x in range((len(SQSS) +len(SQSNS))):\n",
    "    if( x < (len(SQSS))):\n",
    "        SQSY.append(1)\n",
    "    else:\n",
    "        SQSY.append(0)\n",
    "\n",
    "SQSX = []\n",
    "\n",
    "for x in range((len(SQSS) +len(SQSNS))):\n",
    "    if( x < (len(SQSS))):\n",
    "        SQSX.append(SQSS[x])\n",
    "    else:\n",
    "        SQSX.append(SQSNS[x-301])\n",
    "        \n",
    "single = pd.DataFrame(data =[SQSY,SQSX]).T\n",
    "single.rename(columns ={0:'class', 1:'entry'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "estimators = [\n",
    "    ('lr', make_pipeline(CountVectorizer(),\n",
    "                           LogisticRegressionCV(solver = 'liblinear', max_iter=1000))),\n",
    "    ('nb', make_pipeline(TfidfVectorizer(),\n",
    "                          MultinomialNB(alpha = .13))),\n",
    "    ('gb', make_pipeline(CountVectorizer(),\n",
    "                            GradientBoostingClassifier(learning_rate = .2, max_depth = 2))),\n",
    "    ('mlp', make_pipeline(CountVectorizer(),\n",
    "                           MLPClassifier(hidden_layer_sizes= 21, max_iter=1500, shuffle=False, tol=0.012 ))),   \n",
    " ]\n",
    "\n",
    "eclf1 = VotingClassifier(estimators=estimators, voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnOva = 0\n",
    "fpOva = 0\n",
    "fnOva = 0\n",
    "tpOva = 0\n",
    "\n",
    "testAccOva = 0\n",
    "\n",
    "accByNum = []\n",
    "outputNem = []\n",
    "outputQ = []\n",
    "\n",
    "\n",
    "\n",
    "for trainIndex, testIndex in kfold.split(single):\n",
    "\n",
    "    rng = np.random.RandomState(rn)\n",
    "    test = data.sample(frac=0.20, random_state=rng)\n",
    "    trainMask = pd.Series(True, index=data.index)\n",
    "    trainMask[test.index] = False\n",
    "    train = data[trainMask].copy()\n",
    "    train.head()\n",
    "\n",
    "    testX, trainX = single.iloc[train_index]['entry'], single.iloc[test_index]['entry']\n",
    "    testY, trainY = single.iloc[train_index]['class'], single.iloc[test_index]['class']\n",
    "\n",
    "    trainX = pd.concat([trainX, train['entry']])\n",
    "    trainY = pd.concat([trainY, train['class']])\n",
    "\n",
    "    eclf1 = eclf1.fit(trainX.tolist(), trainY.tolist())\n",
    "    \n",
    "    prediction = eclf1.predict(testX)\n",
    "   \n",
    "    outputNem.append(prediction)\n",
    "    \n",
    "    testAccOva +=  accuracy_score(prediction, testY.tolist())\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix( testY.tolist(), prediction).ravel()\n",
    "    \n",
    "    tnOva += tn\n",
    "    fpOva += fp\n",
    "    fnOva += fn\n",
    "    tpOva += tp\n",
    "    rn+=1\n",
    "    x+=1\n",
    "    \n",
    "multiModelResults = ['MultiModel', round(testAccOva/nSplits,3), tnOva/nSplits, fpOva/nSplits, round(((tnOva/nSplits)/((tnOva/nSplits)+ (fpOva/nSplits))),3), fnOva/nSplits, tpOva/nSplits,round(((tpOva/nSplits)/((tpOva/nSplits)+ (fnOva/nSplits))),3),]\n",
    "\n",
    "ovaResults.append(multiModelResults)\n",
    "\n",
    "pickle.dump( outputNem, open( \"Pickles/OutputMultiModelSQS.p\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "In the following block of code we aggregate and then save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allResults = pd.DataFrame(data = ovaResults, columns = [\"Type\",\"Acc\", \"TN\", \"FP\", \"TNR\", \"FN\", \"TP\", \"TPR\"])\n",
    "\n",
    "pickle.dump( allResults, open( \"Pickles/BaselineSQS.p\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
