{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognizeSearcher(params, feats, featType, numQueryAnalysis):\n",
    "    \n",
    "    tnOva = 0\n",
    "    fpOva = 0\n",
    "    fnOva = 0\n",
    "    tpOva = 0\n",
    "    testAccOva = 0\n",
    "    nSplits = 5\n",
    "    x = 0\n",
    "    z = 1\n",
    "    accByNum = []\n",
    "    outputAll = []\n",
    "    outputQ = []\n",
    "\n",
    "    kfold = KFold(n_splits=nSplits, random_state=20210530, shuffle=True)\n",
    "    \n",
    "    randomNumber = 20210530 #(open random number)\n",
    "    \n",
    "    X = SWC[feats]\n",
    "    y = SWC['class']\n",
    "   \n",
    "    if params['scaler']:\n",
    "        if params['classWeight']:\n",
    "            tunePipe = Pipeline([\n",
    "            ('standardize', params['scaler']),\n",
    "            ('classify', RandomForestClassifier(n_estimators=params['numEstimators'], bootstrap = params['bootStrap'],\n",
    "                                                criterion= params['criterion'], class_weight = params['classWeight'], \n",
    "                                                random_state= randomNumber, n_jobs = -1))\n",
    "            ])\n",
    "        else:\n",
    "            tunePipe = Pipeline([\n",
    "            ('standardize', params['scaler']),\n",
    "            ('classify', RandomForestClassifier(n_estimators=params['numEstimators'], bootstrap = params['bootStrap'],\n",
    "                                                criterion= params['criterion'], random_state= randomNumber, n_jobs = -1))\n",
    "            ])\n",
    "    else:\n",
    "        if params['classWeight']:\n",
    "            tunePipe = Pipeline([\n",
    "            ('classify', RandomForestClassifier(n_estimators=params['numEstimators'], bootstrap = params['bootStrap'],\n",
    "                                                criterion= params['criterion'], class_weight = params['classWeight'], \n",
    "                                                random_state= randomNumber, n_jobs = -1))\n",
    "            ])\n",
    "        else:\n",
    "            tunePipe = Pipeline([\n",
    "            ('classify', RandomForestClassifier(n_estimators=params['numEstimators'], bootstrap = params['bootStrap'],\n",
    "                                                criterion= params['criterion'], random_state= randomNumber, n_jobs = -1))\n",
    "            ])\n",
    "                \n",
    "\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "\n",
    "        rng = np.random.RandomState(randomNumber)\n",
    "    \n",
    "        trainSQS = SQS.sample(frac=0.20, random_state=rng)\n",
    "        \n",
    "        subTrainX = trainSQS[feats]\n",
    "        subTrainY = trainSQS['class']\n",
    "        \n",
    "        trainX, testX = X.iloc[train_index], X.iloc[test_index]\n",
    "        trainY, testY = y.iloc[train_index], y.iloc[test_index]\n",
    "        trainX = pd.concat([trainX,subTrainX])\n",
    "        trainY = pd.concat([trainY,subTrainY])\n",
    "        trainX = trainX.fillna(0)\n",
    "        tunePipe.fit(trainX, trainY)\n",
    "        testX['predict'] =  tunePipe.predict(testX)\n",
    "        testX['class'] = testY\n",
    "        testAccOva +=  accuracy_score(testX['class'], testX['predict'])\n",
    "        tn, fp, fn, tp = confusion_matrix(testX['class'], testX['predict']).ravel()\n",
    "        tnOva += tn\n",
    "        fpOva += fp\n",
    "        fnOva += fn\n",
    "        tpOva += tp\n",
    "        outputAll.append(testX['predict'])\n",
    "        if(numQueryAnalysis):\n",
    "\n",
    "            accNum = []\n",
    "            output = []\n",
    "            ova = 0\n",
    "            for x in range(5):\n",
    "                testAcc = testX[testX['numQueries'] == (x+1)]\n",
    "                acc = accuracy_score(testAcc['predict'], testAcc['class'])\n",
    "                output.append(testAcc['predict'].tolist())\n",
    "                SELtp = (testAcc[(testAcc['predict']  == 1) & (testAcc['class'] == 1)].index.tolist())\n",
    "                tn, fp, fn, tp = confusion_matrix( testAcc['class'], testAcc['predict']).ravel()\n",
    "                accNum.append([acc, tn, fp, fn, tp])\n",
    "            testAcc = testX[testX['numQueries'] > (5)]\n",
    "            acc = accuracy_score(testAcc['predict'], testAcc['class'])\n",
    "            output.append(testAcc['predict'].tolist())\n",
    "            SELtp = (testAcc[(testAcc['predict']  == 1) & (testAcc['class'] == 1)].index.tolist())\n",
    "            tn, fp, fn, tp = confusion_matrix( testAcc['class'], testAcc['predict']).ravel()\n",
    "            ova += ((tp +fn+fp+tn))\n",
    "            accNum.append([acc, tn, fp, fn, tp])\n",
    "            accByNum.append(accNum)\n",
    "            outputQ.append(output)\n",
    "\n",
    "\n",
    "        randomNumber +=1\n",
    "        \n",
    "    pickle.dump( outputAll, open( \"Pickles/OutputAll\" + str(featType) + \"SWC.p\", \"wb\" ) )\n",
    "    results = [featType, round(testAccOva/5,3), tnOva/5, fpOva/5, round(((tnOva/5)/((tnOva/5)+ (fpOva/5))),3), fnOva/5, tpOva/5,round(((tpOva/5)/((tpOva/5)+ (fnOva/5))),3),]\n",
    "    if(numQueryAnalysis):\n",
    "        np.set_printoptions(suppress=True)\n",
    "        acc1 = []\n",
    "        acc2 = []\n",
    "        acc3 = []\n",
    "        acc4 = []\n",
    "        acc5 = []\n",
    "        acc6 = []\n",
    "\n",
    "        for accBy in accByNum:\n",
    "            x = 0\n",
    "            for acc in accBy:\n",
    "                if(x == 0):\n",
    "                    acc1.append(acc)\n",
    "                elif(x == 1):\n",
    "                    acc2.append(acc)\n",
    "                elif(x == 2):\n",
    "                    acc3.append(acc)\n",
    "                elif(x == 3):\n",
    "                    acc4.append(acc)\n",
    "                elif(x == 4):\n",
    "                    acc5.append(acc)\n",
    "                elif(x == 5):\n",
    "                    acc6.append(acc)\n",
    "                x+=1\n",
    "\n",
    "                \n",
    "        acc1 = np.array(acc1)\n",
    "        acc1 = acc1.mean(axis = 0)\n",
    "        acc2 = np.array(acc2)\n",
    "        acc2 = acc2.mean(axis = 0)\n",
    "        acc3 = np.array(acc3)\n",
    "        acc3 = acc3.mean(axis = 0)\n",
    "        acc4 = np.array(acc4)\n",
    "        acc4 = acc4.mean(axis = 0)\n",
    "        acc5 = np.array(acc5)\n",
    "        acc5 = acc5.mean(axis = 0)\n",
    "        acc6 = np.array(acc6)\n",
    "        acc6 = acc6.mean(axis = 0)\n",
    "        accAll = [acc1,acc2,acc3,acc4,acc5,acc6]\n",
    "        accAll = pd.DataFrame(data = accAll, columns = ['Acc','TN','FP','FN','TP'])\n",
    "        accAll['TNR'] = round((accAll['TN']/(accAll['TN']+accAll['FP'])),3)\n",
    "        accAll['TPR'] = round((accAll['TP']/(accAll['TP']+accAll['FN'])),3)\n",
    "        accAll = accAll.reset_index()\n",
    "        accAll = accAll.rename(columns = {\"index\" :\"numQ\"})\n",
    "        accAll['numQ'] = accAll['numQ']+1\n",
    "        accAll['numQ'] = accAll['numQ'].astype(str)\n",
    "        accAll['numQ'].replace({\"6\": \"6+\"}, inplace=True)\n",
    "        accAll = accAll.set_index('numQ')\n",
    "        pickle.dump(outputQ, open(\"Pickles/OutputByQueryCount.p\", \"wb\"))\n",
    "        pickle.dump(accAll, open(\"Pickles/AccByQueryCount.p\", \"wb\"))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWC = pickle.load( open( \"Pickles/SWCFeatNoTune.p\", \"rb\" ) )\n",
    "SQS =  pickle.load( open( \"../FeatureExtraction/DataSets/SQSFeatures/SQSFeat.p\", \"rb\" ) )\n",
    "bestParameters = pickle.load( open( \"Pickles/BestParam.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(SQS.columns)\n",
    "features.remove('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "P3Feats = ['ld',\n",
    " 'ls1',\n",
    " 'ls2',\n",
    " 'vs1',\n",
    " 'vs2',\n",
    " 'cvs1',\n",
    " 'ndw',\n",
    " 'ttr',\n",
    " 'cttr',\n",
    " 'rttr',\n",
    " 'logttr',\n",
    " 'lv',\n",
    " 'vv1',\n",
    " 'svv1',\n",
    " 'cvv1',\n",
    " 'vv2',\n",
    " 'nv',\n",
    " 'adjv',\n",
    " 'numSpellingErrors',\n",
    " 'offByOne',\n",
    " 'kidsError',\n",
    " 'coreVocab',\n",
    " 'nonCoreVocab',\n",
    " 'minAoA',\n",
    " 'maxAoA',\n",
    " 'ratioAoA',\n",
    " 'queryComplexity',\n",
    " 'SVEN',\n",
    " 'top250SterCount',\n",
    " 'top250SterRatAnt',\n",
    " 'top250SterRatCon',\n",
    " 'top250NonSterCount',\n",
    " 'top250NonSterRatAnt',\n",
    " 'top250NonSterRatCon',\n",
    " 'top50SterCount',\n",
    " 'top50SterRatAnt',\n",
    " 'top50SterAntCon',\n",
    " 'top50NonSterCount',\n",
    " 'top50NonSterRatAnt',\n",
    " 'top50NonSterAntCon',\n",
    " 'tfidfAll',\n",
    " 'tfidfS',\n",
    " 'tfidfNS'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "DC1Feats = ['ndw',\n",
    " 'ttr',\n",
    " 'cttr',\n",
    " 'rttr',\n",
    " 'logttr',\n",
    " 'lv',\n",
    " 'vv1',\n",
    " 'svv1',\n",
    " 'cvv1',\n",
    " 'vv2',\n",
    " 'nv',\n",
    " 'adjv',\n",
    " 'totalSyl',\n",
    " 'avgSyl',\n",
    " 'simWords',\n",
    " 'comWords',\n",
    " 'greatestSyl',\n",
    " 'leastSyl',\n",
    " 'numChars',\n",
    " 'numWords',\n",
    " 'avgLenWord',\n",
    " 'minAoA',\n",
    " 'maxAoA',\n",
    " 'queryComplexity',\n",
    " 'stopCount',\n",
    " 'com',\n",
    " 'net',\n",
    " 'org',\n",
    " 'edu',\n",
    " 'gov',\n",
    " 'http',\n",
    " 'AND',\n",
    " 'OR',\n",
    " 'quotes',\n",
    " 'inter',\n",
    " 'numSpellingErrors',\n",
    " 'offByOne',\n",
    " 'kidsError',\n",
    " 'punct',\n",
    " 'casing',\n",
    " ' Level0',\n",
    " ' Level1',\n",
    " ' Level2',\n",
    " ' Level3',\n",
    " ' Level4',\n",
    " ' Level5',\n",
    " ' Level6',\n",
    " ' Level7',\n",
    " ' MeanLevel',\n",
    " 'cc',\n",
    " 'cd',\n",
    " 'dt',\n",
    " 'ex',\n",
    " 'fw',\n",
    " 'in',\n",
    " 'jj',\n",
    " 'jjr',\n",
    " 'jjs',\n",
    " 'md',\n",
    " 'nn',\n",
    " 'nnp',\n",
    " 'nnps',\n",
    " 'nns',\n",
    " 'pdt',\n",
    " 'pos',\n",
    " 'prp',\n",
    " 'rb',\n",
    " 'rbr',\n",
    " 'rbs',\n",
    " 'rp',\n",
    " 'sym',\n",
    " 'to',\n",
    " 'uh',\n",
    " 'vb',\n",
    " 'vbd',\n",
    " 'vbg',\n",
    " 'vbn',\n",
    " 'vbp',\n",
    " 'vbz',\n",
    " 'wdt',\n",
    " 'wp',\n",
    " 'wrb',\n",
    " 'nn nn',\n",
    " 'jj nn',\n",
    " 'nn nns',\n",
    " 'to vb',\n",
    " 'jj nns',\n",
    " 'jj to',\n",
    " 'nn in',\n",
    " 'nns in',\n",
    " 'in nn',\n",
    " 'dt nn',\n",
    " 'jj nn nn',\n",
    " 'nn nn nn',\n",
    " 'jj to vb',\n",
    " 'nn nn nns',\n",
    " 'to vb nn',\n",
    " 'repeatClicks',\n",
    " 'clickDistance',\n",
    " 'meanClickPosition',\n",
    " 'numClicks',\n",
    " 'numClicksPerQuery',\n",
    " 'numQueries',\n",
    " 'timeClicks',\n",
    " 'uniqueQueries',\n",
    " 'allSameClicks',\n",
    " 'uniqueClicks',\n",
    " 'allSameQueries',\n",
    " 'queryDistance',\n",
    " 'timeQueries',\n",
    " 'repeatQueries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextFeat = ['cc',\n",
    " 'cd',\n",
    " 'dt',\n",
    " 'ex',\n",
    " 'fw',\n",
    " 'in',\n",
    " 'jj',\n",
    " 'jjr',\n",
    " 'jjs',\n",
    " 'md',\n",
    " 'nn',\n",
    " 'nnp',\n",
    " 'nnps',\n",
    " 'nns',\n",
    " 'pdt',\n",
    " 'pos',\n",
    " 'prp',\n",
    " 'rb',\n",
    " 'rbr',\n",
    " 'rbs',\n",
    " 'rp',\n",
    " 'sym',\n",
    " 'to',\n",
    " 'uh',\n",
    " 'vb',\n",
    " 'vbd',\n",
    " 'vbg',\n",
    " 'vbn',\n",
    " 'vbp',\n",
    " 'vbz',\n",
    " 'wdt',\n",
    " 'wp',\n",
    " 'wrb',\n",
    " 'nn nn',\n",
    " 'jj nn',\n",
    " 'nn nns',\n",
    " 'to vb',\n",
    " 'jj nns',\n",
    " 'jj to',\n",
    " 'nn in',\n",
    " 'nns in',\n",
    " 'in nn',\n",
    " 'dt nn',\n",
    " 'jj nn nn',\n",
    " 'nn nn nn',\n",
    " 'jj to vb',\n",
    " 'nn nn nns',\n",
    " 'to vb nn',\n",
    " ' Level0',\n",
    " ' Level1',\n",
    " ' Level2',\n",
    " ' Level3',\n",
    " ' Level4',\n",
    " ' Level5',\n",
    " ' Level6',\n",
    " ' Level7',\n",
    " ' MeanLevel',\n",
    " 'totalSyl',\n",
    " 'avgSyl',\n",
    " 'simWords',\n",
    " 'comWords',\n",
    " 'greatestSyl',\n",
    " 'leastSyl',\n",
    " 'numChars',\n",
    " 'numWords',\n",
    " 'avgLenWord',\n",
    " 'ld',\n",
    " 'ls1',\n",
    " 'ls2',\n",
    " 'vs1',\n",
    " 'vs2',\n",
    " 'cvs1',\n",
    " 'ndw',\n",
    " 'ttr',\n",
    " 'cttr',\n",
    " 'rttr',\n",
    " 'logttr',\n",
    " 'lv',\n",
    " 'vv1',\n",
    " 'svv1',\n",
    " 'cvv1',\n",
    " 'vv2',\n",
    " 'nv',\n",
    " 'adjv',\n",
    " 'numSpellingErrors',\n",
    " 'offByOne',\n",
    " 'kidsError',\n",
    " 'punct',\n",
    " 'casing',\n",
    " 'coreVocab',\n",
    " 'nonCoreVocab',\n",
    " 'minAoA',\n",
    " 'maxAoA',\n",
    " 'ratioAoA',\n",
    " 'queryComplexity',\n",
    " 'SVEN',\n",
    " 'top250SterCount',\n",
    " 'top250SterRatAnt',\n",
    " 'top250SterRatCon',\n",
    " 'top250NonSterCount',\n",
    " 'top250NonSterRatAnt',\n",
    " 'top250NonSterRatCon',\n",
    " 'top50SterCount',\n",
    " 'top50SterRatAnt',\n",
    " 'top50SterAntCon',\n",
    " 'top50NonSterCount',\n",
    " 'top50NonSterRatAnt',\n",
    " 'top50NonSterAntCon',\n",
    " 'tfidfAll',\n",
    " 'tfidfS',\n",
    " 'tfidfNS',\n",
    " 'stopCount',\n",
    " 'com',\n",
    " 'net',\n",
    " 'org',\n",
    " 'edu',\n",
    " 'gov',\n",
    " 'http',\n",
    " 'AND',\n",
    " 'OR',\n",
    " 'quotes',\n",
    " 'inter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "SessionFeats = ['repeatClicks',\n",
    " 'clickDistance',\n",
    " 'meanClickPosition',\n",
    " 'numClicks',\n",
    " 'numClicksPerQuery',\n",
    " 'numQueries',\n",
    " 'timeClicks',\n",
    " 'uniqueQueries',\n",
    " 'allSameClicks',\n",
    " 'uniqueClicks',\n",
    " 'allSameQueries',\n",
    " 'queryDistance',\n",
    " 'timeQueries',\n",
    " 'repeatQueries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RYSe = recognizeSearcher(bestParameters, features, 'RYSe', True)\n",
    "P3Results = recognizeSearcher(bestParameters, P3Feats, 'P3', False)\n",
    "DC1Results = recognizeSearcher(bestParameters, DC1Feats, 'DC1', False)\n",
    "textResults = recognizeSearcher(bestParameters, TextFeat, 'TextBased', False)\n",
    "sessionResults = recognizeSearcher(bestParameters, SessionFeats, 'SessionBased', False)\n",
    "\n",
    "allResults = pd.DataFrame(data = [RYSe,P3Results,DC1Results,textResults,sessionResults  ], columns = [\"Type\",\"Acc\", \"TN\", \"FP\", \"TNR\", \"FN\", \"TP\", \"TPR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( allResults, open( \"Pickles/AblationSWC.p\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
