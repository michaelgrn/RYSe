{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognizeSearcher(params, feats, featType, numQueryAnalysis):\n",
    "    \n",
    "    tnOva = 0\n",
    "    fpOva = 0\n",
    "    fnOva = 0\n",
    "    tpOva = 0\n",
    "    testAccOva = 0\n",
    "    nSplits = 5\n",
    "    x = 0\n",
    "    z = 1\n",
    "    accByNum = []\n",
    "    outputAll = []\n",
    "    outputQ = []\n",
    "\n",
    "    kfold = KFold(n_splits=nSplits, random_state=20210530, shuffle=True)\n",
    "    \n",
    "    randomNumber = 20210530 #(open random number)\n",
    "    \n",
    "    X = SWC[feats]\n",
    "    y = SWC['class']\n",
    "   \n",
    "    if params['scaler']:\n",
    "        if params['classWeight']:\n",
    "            tune_pipe = Pipeline([\n",
    "            ('standardize', params['scaler']),\n",
    "            ('classify', RandomForestClassifier(n_estimators=params['numEstimators'], bootstrap = params['bootStrap'],\n",
    "                                                criterion= params['criterion'], class_weight = params['classWeight'], \n",
    "                                                random_state= randomNumber, n_jobs = -1))\n",
    "            ])\n",
    "        else:\n",
    "                        tune_pipe = Pipeline([\n",
    "            ('standardize', params['scaler']),\n",
    "            ('classify', RandomForestClassifier(n_estimators=params['numEstimators'], bootstrap = params['bootStrap'],\n",
    "                                                criterion= params['criterion'], random_state= randomNumber, n_jobs = -1))\n",
    "            ])\n",
    "    else:\n",
    "        if params['classWeight']:\n",
    "            tune_pipe = Pipeline([\n",
    "            ('classify', RandomForestClassifier(n_estimators=params['numEstimators'], bootstrap = params['bootStrap'],\n",
    "                                                criterion= params['criterion'], class_weight = params['classWeight'], \n",
    "                                                random_state= randomNumber, n_jobs = -1))\n",
    "            ])\n",
    "        else:\n",
    "                        tune_pipe = Pipeline([\n",
    "            ('classify', RandomForestClassifier(n_estimators=params['numEstimators'], bootstrap = params['bootStrap'],\n",
    "                                                criterion= params['criterion'], random_state= randomNumber, n_jobs = -1))\n",
    "            ])\n",
    "                \n",
    "\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "\n",
    "        rng = np.random.RandomState(randomNumber)\n",
    "    \n",
    "        trainSQS = SQS.sample(frac=0.20, random_state=rng)\n",
    "#         train_mask = pd.Series(True, index=SQS.index)\n",
    "#         train_mask[test.index] = False\n",
    "#         train = SQS[train_mask].copy()\n",
    "#         train.head()\n",
    "        \n",
    "        subTrain_x = trainSQS[feats]\n",
    "        subTrain_y = trainSQS['class']\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        X_train = pd.concat([X_train,subTrain_x])\n",
    "        y_train = pd.concat([y_train,subTrain_y])\n",
    "        X_train = X_train.fillna(0)\n",
    "        tune_pipe.fit(X_train, y_train)\n",
    "        X_test['predict'] =  tune_pipe.predict(X_test)\n",
    "        X_test['class'] = y_test\n",
    "        testAccOva +=  accuracy_score(X_test['class'], X_test['predict'])\n",
    "        tn, fp, fn, tp = confusion_matrix(X_test['class'], X_test['predict']).ravel()\n",
    "        tnOva += tn\n",
    "        fpOva += fp\n",
    "        fnOva += fn\n",
    "        tpOva += tp\n",
    "        outputAll.append(X_test['predict'])\n",
    "        if(numQueryAnalysis):\n",
    "\n",
    "            accNum = []\n",
    "            output = []\n",
    "            ova = 0\n",
    "            for x in range(5):\n",
    "                testAcc = X_test[X_test['numQueries'] == (x+1)]\n",
    "                acc = accuracy_score(testAcc['predict'], testAcc['class'])\n",
    "                output.append(testAcc['predict'].tolist())\n",
    "                SELtp = (testAcc[(testAcc['predict']  == 1) & (testAcc['class'] == 1)].index.tolist())\n",
    "                tn, fp, fn, tp = confusion_matrix( testAcc['class'], testAcc['predict']).ravel()\n",
    "                accNum.append([acc, tn, fp, fn, tp])\n",
    "            testAcc = X_test[X_test['numQueries'] > (5)]\n",
    "            acc = accuracy_score(testAcc['predict'], testAcc['class'])\n",
    "            output.append(testAcc['predict'].tolist())\n",
    "            SELtp = (testAcc[(testAcc['predict']  == 1) & (testAcc['class'] == 1)].index.tolist())\n",
    "            tn, fp, fn, tp = confusion_matrix( testAcc['class'], testAcc['predict']).ravel()\n",
    "            ova += ((tp +fn+fp+tn))\n",
    "            accNum.append([acc, tn, fp, fn, tp])\n",
    "            accByNum.append(accNum)\n",
    "            outputQ.append(output)\n",
    "\n",
    "\n",
    "        randomNumber +=1\n",
    "        \n",
    "    pickle.dump( outputAll, open( \"Pickles/OutputAll\" + str(featType) + \"SWC.p\", \"wb\" ) )\n",
    "    results = [featType, round(testAccOva/5,3), tnOva/5, fpOva/5, round(((tnOva/5)/((tnOva/5)+ (fpOva/5))),3), fnOva/5, tpOva/5,round(((tpOva/5)/((tpOva/5)+ (fnOva/5))),3),]\n",
    "    if(numQueryAnalysis):\n",
    "        np.set_printoptions(suppress=True)\n",
    "        acc1 = []\n",
    "        acc2 = []\n",
    "        acc3 = []\n",
    "        acc4 = []\n",
    "        acc5 = []\n",
    "        acc6 = []\n",
    "\n",
    "        for accBy in accByNum:\n",
    "            x = 0\n",
    "            for acc in accBy:\n",
    "                if(x == 0):\n",
    "                    acc1.append(acc)\n",
    "                elif(x == 1):\n",
    "                    acc2.append(acc)\n",
    "                elif(x == 2):\n",
    "                    acc3.append(acc)\n",
    "                elif(x == 3):\n",
    "                    acc4.append(acc)\n",
    "                elif(x == 4):\n",
    "                    acc5.append(acc)\n",
    "                elif(x == 5):\n",
    "                    acc6.append(acc)\n",
    "                x+=1\n",
    "\n",
    "                \n",
    "        acc1 = np.array(acc1)\n",
    "        acc1 = acc1.mean(axis = 0)\n",
    "        acc2 = np.array(acc2)\n",
    "        acc2 = acc2.mean(axis = 0)\n",
    "        acc3 = np.array(acc3)\n",
    "        acc3 = acc3.mean(axis = 0)\n",
    "        acc4 = np.array(acc4)\n",
    "        acc4 = acc4.mean(axis = 0)\n",
    "        acc5 = np.array(acc5)\n",
    "        acc5 = acc5.mean(axis = 0)\n",
    "        acc6 = np.array(acc6)\n",
    "        acc6 = acc6.mean(axis = 0)\n",
    "        accAll = [acc1,acc2,acc3,acc4,acc5,acc6]\n",
    "        accAll = pd.DataFrame(data = accAll, columns = ['Acc','TN','FP','FN','TP'])\n",
    "        accAll['TNR'] = round((accAll['TN']/(accAll['TN']+accAll['FP'])),3)\n",
    "        accAll['TPR'] = round((accAll['TP']/(accAll['TP']+accAll['FN'])),3)\n",
    "        accAll = accAll.reset_index()\n",
    "        accAll = accAll.rename(columns = {\"index\" :\"numQ\"})\n",
    "        accAll['numQ'] = accAll['numQ']+1\n",
    "        accAll['numQ'] = accAll['numQ'].astype(str)\n",
    "        accAll['numQ'].replace({\"6\": \"6+\"}, inplace=True)\n",
    "        accAll = accAll.set_index('numQ')\n",
    "        pickle.dump(outputQ, open(\"Pickles/OutputByQueryCount.p\", \"wb\"))\n",
    "        pickle.dump(accAll, open(\"Pickles/AccByQueryCount.p\", \"wb\"))\n",
    "#         print(accAll)\n",
    "#         print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWC = pickle.load( open( \"Pickles/SWCFeatNoTune.p\", \"rb\" ) )\n",
    "SQS =  pickle.load( open( \"../FeatureExtraction/DataSets/SQSFeatures/SQSFeat.p\", \"rb\" ) )\n",
    "bestParameters = pickle.load( open( \"Pickles/BestParam.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(SQS.columns)\n",
    "features.remove('class')\n",
    "features.remove('Filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['queryDistance',\n",
       " 'timeQueries',\n",
       " 'repeatQueries',\n",
       " 'repeatClicks',\n",
       " 'clickDistance',\n",
       " 'meanClickPosition',\n",
       " 'numClicks',\n",
       " 'numClicksPerQuery',\n",
       " 'numQueries',\n",
       " 'timeClicks',\n",
       " 'uniqueQueries',\n",
       " 'allSameClicks',\n",
       " 'uniqueClicks',\n",
       " 'allSameQueries',\n",
       " 'cc',\n",
       " 'cd',\n",
       " 'dt',\n",
       " 'ex',\n",
       " 'fw',\n",
       " 'in',\n",
       " 'jj',\n",
       " 'jjr',\n",
       " 'jjs',\n",
       " 'ls',\n",
       " 'md',\n",
       " 'nn',\n",
       " 'nnp',\n",
       " 'nnps',\n",
       " 'nns',\n",
       " 'pdt',\n",
       " 'pos',\n",
       " 'prp',\n",
       " 'rb',\n",
       " 'rbr',\n",
       " 'rbs',\n",
       " 'rp',\n",
       " 'sym',\n",
       " 'to',\n",
       " 'uh',\n",
       " 'vb',\n",
       " 'vbd',\n",
       " 'vbg',\n",
       " 'vbn',\n",
       " 'vbp',\n",
       " 'vbz',\n",
       " 'wdt',\n",
       " 'wp',\n",
       " 'wrb',\n",
       " 'nn nn',\n",
       " 'jj nn',\n",
       " 'nn nns',\n",
       " 'to vb',\n",
       " 'jj nns',\n",
       " 'jj to',\n",
       " 'nn in',\n",
       " 'nns in',\n",
       " 'in nn',\n",
       " 'dt nn',\n",
       " 'jj nn nn',\n",
       " 'nn nn nn',\n",
       " 'jj to vb',\n",
       " 'nn nn nns',\n",
       " 'to vb nn',\n",
       " ' Level0',\n",
       " ' Level1',\n",
       " ' Level2',\n",
       " ' Level3',\n",
       " ' Level4',\n",
       " ' Level5',\n",
       " ' Level6',\n",
       " ' Level7',\n",
       " ' MeanLevel',\n",
       " 'totalSyl',\n",
       " 'avgSyl',\n",
       " 'simWords',\n",
       " 'comWords',\n",
       " 'greatestSyl',\n",
       " 'leastSyl',\n",
       " 'numChars',\n",
       " 'numWords',\n",
       " 'avgLenWord',\n",
       " 'ld',\n",
       " 'ls1',\n",
       " 'ls2',\n",
       " 'vs1',\n",
       " 'vs2',\n",
       " 'cvs1',\n",
       " 'ndw',\n",
       " 'ttr',\n",
       " 'cttr',\n",
       " 'rttr',\n",
       " 'logttr',\n",
       " 'lv',\n",
       " 'vv1',\n",
       " 'svv1',\n",
       " 'cvv1',\n",
       " 'vv2',\n",
       " 'nv',\n",
       " 'adjv',\n",
       " 'numSpellingErrors',\n",
       " 'offByOne',\n",
       " 'kidsError',\n",
       " 'punct',\n",
       " 'casing',\n",
       " 'coreVocab',\n",
       " 'nonCoreVocab',\n",
       " 'vocabMin',\n",
       " 'vocabMax',\n",
       " 'queryComplexity',\n",
       " 'addVocab',\n",
       " 'SVEN',\n",
       " 'top250All',\n",
       " 'top250avgAll',\n",
       " 'top250avgNotAll',\n",
       " 'top250AllNA',\n",
       " 'top250avgAllNA',\n",
       " 'top250avgNotAllNA',\n",
       " 'top50bi',\n",
       " 'top50biAvg',\n",
       " 'top50biNot',\n",
       " 'top50biNA',\n",
       " 'top50biavgNA',\n",
       " 'top50biNotNA',\n",
       " 'tfidfAll',\n",
       " 'tfidf',\n",
       " 'tfidfNA',\n",
       " 'stopCount',\n",
       " 'com',\n",
       " 'net',\n",
       " 'org',\n",
       " 'edu',\n",
       " 'gov',\n",
       " 'http',\n",
       " 'AND',\n",
       " 'OR',\n",
       " 'quotes',\n",
       " 'inter']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "P3Feats = ['ld',\n",
    " 'ls1',\n",
    " 'ls2',\n",
    " 'vs1',\n",
    " 'vs2',\n",
    " 'cvs1',\n",
    " 'ndw',\n",
    " 'ttr',\n",
    " 'cttr',\n",
    " 'rttr',\n",
    " 'logttr',\n",
    " 'lv',\n",
    " 'vv1',\n",
    " 'svv1',\n",
    " 'cvv1',\n",
    " 'vv2',\n",
    " 'nv',\n",
    " 'adjv',\n",
    " 'numSpellingErrors',\n",
    " 'offByOne',\n",
    " 'kidsError',\n",
    " 'coreVocab',\n",
    " 'nonCoreVocab',\n",
    " 'vocabMin',\n",
    " 'vocabMax',\n",
    " 'queryComplexity',\n",
    " 'addVocab',\n",
    " 'SVEN',\n",
    " 'top250All',\n",
    " 'top250avgAll',\n",
    " 'top250avgNotAll',\n",
    " 'top250AllNA',\n",
    " 'top250avgAllNA',\n",
    " 'top250avgNotAllNA',\n",
    " 'top50bi',\n",
    " 'top50biAvg',\n",
    " 'top50biNot',\n",
    " 'top50biNA',\n",
    " 'top50biavgNA',\n",
    " 'top50biNotNA',\n",
    " 'tfidfAll',\n",
    " 'tfidf',\n",
    " 'tfidfNA',\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DC1Feats = ['ndw',\n",
    " 'ttr',\n",
    " 'cttr',\n",
    " 'rttr',\n",
    " 'logttr',\n",
    " 'lv',\n",
    " 'vv1',\n",
    " 'svv1',\n",
    " 'cvv1',\n",
    " 'vv2',\n",
    " 'nv',\n",
    " 'adjv',\n",
    " 'totalSyl',\n",
    " 'avgSyl',\n",
    " 'simWords',\n",
    " 'comWords',\n",
    " 'greatestSyl',\n",
    " 'leastSyl',\n",
    " 'numChars',\n",
    " 'numWords',\n",
    " 'avgLenWord',\n",
    " 'vocabMin',\n",
    " 'vocabMax',\n",
    " 'queryComplexity',\n",
    " 'stopCount',\n",
    " 'com',\n",
    " 'net',\n",
    " 'org',\n",
    " 'edu',\n",
    " 'gov',\n",
    " 'http',\n",
    " 'AND',\n",
    " 'OR',\n",
    " 'quotes',\n",
    " 'inter',\n",
    " 'numSpellingErrors',\n",
    " 'offByOne',\n",
    " 'kidsError',\n",
    " 'punct',\n",
    " 'casing',\n",
    " 'cc',\n",
    " 'cd',\n",
    " 'dt',\n",
    " 'ex',\n",
    " 'fw',\n",
    " 'in',\n",
    " 'jj',\n",
    " 'jjr',\n",
    " 'jjs',\n",
    " 'ls',\n",
    " 'md',\n",
    " 'nn',\n",
    " 'nnp',\n",
    " 'nnps',\n",
    " 'nns',\n",
    " 'pdt',\n",
    " 'pos',\n",
    " 'prp',\n",
    " 'rb',\n",
    " 'rbr',\n",
    " 'rbs',\n",
    " 'rp',\n",
    " 'sym',\n",
    " 'to',\n",
    " 'uh',\n",
    " 'vb',\n",
    " 'vbd',\n",
    " 'vbg',\n",
    " 'vbn',\n",
    " 'vbp',\n",
    " 'vbz',\n",
    " 'wdt',\n",
    " 'wp',\n",
    " 'wrb',\n",
    " 'nn nn',\n",
    " 'jj nn',\n",
    " 'nn nns',\n",
    " 'to vb',\n",
    " 'jj nns',\n",
    " 'jj to',\n",
    " 'nn in',\n",
    " 'nns in',\n",
    " 'in nn',\n",
    " 'dt nn',\n",
    " 'jj nn nn',\n",
    " 'nn nn nn',\n",
    " 'jj to vb',\n",
    " 'nn nn nns',\n",
    " 'to vb nn',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextFeat = ['cc',\n",
    " 'cd',\n",
    " 'dt',\n",
    " 'ex',\n",
    " 'fw',\n",
    " 'in',\n",
    " 'jj',\n",
    " 'jjr',\n",
    " 'jjs',\n",
    " 'ls',\n",
    " 'md',\n",
    " 'nn',\n",
    " 'nnp',\n",
    " 'nnps',\n",
    " 'nns',\n",
    " 'pdt',\n",
    " 'pos',\n",
    " 'prp',\n",
    " 'rb',\n",
    " 'rbr',\n",
    " 'rbs',\n",
    " 'rp',\n",
    " 'sym',\n",
    " 'to',\n",
    " 'uh',\n",
    " 'vb',\n",
    " 'vbd',\n",
    " 'vbg',\n",
    " 'vbn',\n",
    " 'vbp',\n",
    " 'vbz',\n",
    " 'wdt',\n",
    " 'wp',\n",
    " 'wrb',\n",
    " 'nn nn',\n",
    " 'jj nn',\n",
    " 'nn nns',\n",
    " 'to vb',\n",
    " 'jj nns',\n",
    " 'jj to',\n",
    " 'nn in',\n",
    " 'nns in',\n",
    " 'in nn',\n",
    " 'dt nn',\n",
    " 'jj nn nn',\n",
    " 'nn nn nn',\n",
    " 'jj to vb',\n",
    " 'nn nn nns',\n",
    " 'to vb nn',\n",
    " ' Level0',\n",
    " ' Level1',\n",
    " ' Level2',\n",
    " ' Level3',\n",
    " ' Level4',\n",
    " ' Level5',\n",
    " ' Level6',\n",
    " ' Level7',\n",
    " ' MeanLevel',\n",
    " 'totalSyl',\n",
    " 'avgSyl',\n",
    " 'simWords',\n",
    " 'comWords',\n",
    " 'greatestSyl',\n",
    " 'leastSyl',\n",
    " 'numChars',\n",
    " 'numWords',\n",
    " 'avgLenWord',\n",
    " 'ld',\n",
    " 'ls1',\n",
    " 'ls2',\n",
    " 'vs1',\n",
    " 'vs2',\n",
    " 'cvs1',\n",
    " 'ndw',\n",
    " 'ttr',\n",
    " 'cttr',\n",
    " 'rttr',\n",
    " 'logttr',\n",
    " 'lv',\n",
    " 'vv1',\n",
    " 'svv1',\n",
    " 'cvv1',\n",
    " 'vv2',\n",
    " 'nv',\n",
    " 'adjv',\n",
    " 'numSpellingErrors',\n",
    " 'offByOne',\n",
    " 'kidsError',\n",
    " 'punct',\n",
    " 'casing',\n",
    " 'coreVocab',\n",
    " 'nonCoreVocab',\n",
    " 'vocabMin',\n",
    " 'queryComplexity',\n",
    " 'addVocab',\n",
    " 'vocabMax',\n",
    " 'SVEN',\n",
    " 'top250All',\n",
    " 'top250avgAll',\n",
    " 'top250avgNotAll',\n",
    " 'top250AllNA',\n",
    " 'top250avgAllNA',\n",
    " 'top250avgNotAllNA',\n",
    " 'top50bi',\n",
    " 'top50biAvg',\n",
    " 'top50biNot',\n",
    " 'top50biNA',\n",
    " 'top50biavgNA',\n",
    " 'top50biNotNA',\n",
    " 'tfidfAll',\n",
    " 'tfidf',\n",
    " 'tfidfNA',\n",
    " 'stopCount',\n",
    " 'com',\n",
    " 'net',\n",
    " 'org',\n",
    " 'edu',\n",
    " 'gov',\n",
    " 'http',\n",
    " 'AND',\n",
    " 'OR',\n",
    " 'quotes',\n",
    " 'inter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SessionFeats = ['repeatClicks',\n",
    " 'clickDistance',\n",
    " 'meanClickPosition',\n",
    " 'numClicks',\n",
    " 'numClicksPerQuery',\n",
    " 'numQueries',\n",
    " 'timeClicks',\n",
    " 'uniqueQueries',\n",
    " 'allSameClicks',\n",
    " 'uniqueClicks',\n",
    " 'allSameQueries',\n",
    " 'queryDistance',\n",
    " 'timeQueries',\n",
    " 'repeatQueries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>cc</th>\n",
       "      <th>cd</th>\n",
       "      <th>dt</th>\n",
       "      <th>ex</th>\n",
       "      <th>fw</th>\n",
       "      <th>in</th>\n",
       "      <th>jj</th>\n",
       "      <th>jjr</th>\n",
       "      <th>jjs</th>\n",
       "      <th>...</th>\n",
       "      <th>numClicksPerQuery</th>\n",
       "      <th>numQueries</th>\n",
       "      <th>timeClicks</th>\n",
       "      <th>uniqueQueries</th>\n",
       "      <th>allSameClicks</th>\n",
       "      <th>uniqueClicks</th>\n",
       "      <th>allSameQueries</th>\n",
       "      <th>queryDistance</th>\n",
       "      <th>timeQueries</th>\n",
       "      <th>repeatQueries</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [class, cc, cd, dt, ex, fw, in, jj, jjr, jjs, ls, md, nn, nnp, nnps, nns, pdt, pos, prp, rb, rbr, rbs, rp, sym, to, uh, vb, vbd, vbg, vbn, vbp, vbz, wdt, wp, wrb, nn nn, jj nn, nn nns, to vb, jj nns, jj to, nn in, nns in, in nn, dt nn, jj nn nn, nn nn nn, jj to vb, nn nn nns, to vb nn,  Level0,  Level1,  Level2,  Level3,  Level4,  Level5,  Level6,  Level7,  MeanLevel, totalSyl, avgSyl, simWords, comWords, greatestSyl, leastSyl, numChars, numWords, avgLenWord, ld, ls1, ls2, vs1, vs2, cvs1, ndw, ttr, cttr, rttr, logttr, lv, vv1, svv1, cvv1, vv2, nv, adjv, numSpellingErrors, offByOne, kidsError, punct, casing, coreVocab, nonCoreVocab, vocabMin, vocabMax, queryComplexity, addVocab, SVEN, top250All, top250avgAll, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 138 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SWC[SWC['numQueries']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RYSe = recognizeSearcher(bestParameters, features, 'RYSe', True)\n",
    "P3Results = recognizeSearcher(bestParameters, P3Feats, 'P3', False)\n",
    "DC1Results = recognizeSearcher(bestParameters, DC1Feats, 'DC1', False)\n",
    "textResults = recognizeSearcher(bestParameters, TextFeat, 'TextBased', False)\n",
    "sessionResults = recognizeSearcher(bestParameters, SessionFeats, 'SessionBased', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RYSe', 0.952, 5219.6, 56.2, 0.989, 258.0, 1065.8, 0.805]\n",
      "['P3', 0.94, 5182.8, 93.0, 0.982, 303.6, 1020.2, 0.771]\n",
      "['DC1', 0.888, 5203.6, 72.2, 0.986, 668.4, 655.4, 0.495]\n",
      "['TextBased', 0.937, 5193.4, 82.4, 0.984, 334.2, 989.6, 0.748]\n",
      "['SessionBased', 0.857, 4893.4, 382.4, 0.928, 562.6, 761.2, 0.575]\n"
     ]
    }
   ],
   "source": [
    "print(RYSe)\n",
    "print(P3Results)\n",
    "print(DC1Results)\n",
    "print(textResults)\n",
    "print(sessionResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "allResults = pd.DataFrame(data = [RYSe,P3Results,DC1Results,textResults,sessionResults  ], columns = [\"Type\",\"Acc\", \"TN\", \"FP\", \"TNR\", \"FN\", \"TP\", \"TPR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( allResults, open( \"Pickles/AblationSWC.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
